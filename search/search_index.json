{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bioinformatics Collection This site is a collection of lectures, programs, tools, tutorials etc. for myself. It serves as a help but does not claim to be complete, accurate or reliable for others. Commands are written as \"one liner\" use the \"triple\" mouse click to copy the complete line, because it gets the complete command even if its on 2 or more lines due to website formatting Written/Collected by Christian Brandt. Researchgate Profile Google Scholar Profile Git Profile Dockerhub Content cloud Courses Big thanks to the SLU bioinformatic course at Uppsala Course site Big thanks to the CeBiTec course at Bielefeld License GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.","title":"Bioinformatics Collection"},{"location":"#bioinformatics-collection","text":"This site is a collection of lectures, programs, tools, tutorials etc. for myself. It serves as a help but does not claim to be complete, accurate or reliable for others. Commands are written as \"one liner\" use the \"triple\" mouse click to copy the complete line, because it gets the complete command even if its on 2 or more lines due to website formatting Written/Collected by Christian Brandt. Researchgate Profile Google Scholar Profile Git Profile Dockerhub","title":"Bioinformatics Collection"},{"location":"#content-cloud","text":"","title":"Content cloud"},{"location":"#courses","text":"Big thanks to the SLU bioinformatic course at Uppsala Course site Big thanks to the CeBiTec course at Bielefeld","title":"Courses"},{"location":"#license","text":"GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.","title":"License"},{"location":"R/R/","text":"Overview of R Installing r-base (language) and R-Studio (environment) for Linux Mint (Xenial) # Installing R language sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 sudo add-apt-repository 'deb [arch=amd64,i386] https://cran.rstudio.com/bin/linux/ubuntu xenial/' sudo apt-get update sudo apt-get install r-base Get the R-Studio from here . If you want to code in terminal type R . Now you are in a R environment (\"console\"). q() to exit R in terminal. But use R Studio for scripting not the terminal. Installing packages in R Packages are organized in repositories: CRAN , Bioconductor , R-forge , Github or * Googlecode Installing a package in R and first time source location # 1 establish bioconductor as a source, first time only source(\"https://bioconductor.org/biocLite.R\") # 2 install package methylKit from biocLite biocLite(\"methylKit\") # CRAN # install from CRAN install.packages(\"fortunes\") Basics \"file\" -> \"new project\" -> \"new directory\" and save it to a location \"file\" -> \"new file\" -> \"R Script\" The output of R is given as [1] in the console Help functions Get help in R here or you use help commands: help(read.table) # help on a function ?read.table # is the same help.search(\"deviation\") ??deviation # is the same Variables Variables: any letter or word incl. numbers, _ or . Variables are case sensitive and don't have to be declared first x <- 4+3 # adds sum of 4+3 to the variable x y <- 9 # also shown in the environment window or with ls() x+y [1] 16 # result of x+y Operators More tutorials about operators # Arithmetic Operators +, -, *, /, %%, %/%, ^ # Relational Operators, TRUE or FALSE as a result >, <, >=, <=, ==, != # Logical Operators, & = AND, | = OR &, | , !, &&, || #&& = searches if the value is somewhere in the vectors 4<3 | 4>3 #4<3 = FALSE, 4>3 = TRUE [1] TRUE # Assignment Operators <-, =, <<-, ->, ->> # Miscellaneous Operators %in%, %*% # check help for more Value/data types # Numeric \u2013 A floating point number (default data type of R): 2.4, 9, 888 # Integer \u2013 A number with L suffix: 2L, 25L, 0L # Character - Any amount of text: \"a\", \"school is a house\", \"66\" # Logical values: TRUE, FALSE # Complex \u2013 defined via imaginary value i: 2+3i, 5+8i class() and typeof() functions # class( ) to determine the class of the R object # typeof( ) to determine the internal type (storage mode) of the object typeof(5.67) [1] \"double\" class(5) [1] \"numeric\" # To create integer variable in R, you invoke as.integer() function. x <- as.integer(3) x [1] 3 class(var_name) [1] \"integer\" print() and paste() functions print(\"I am at home\") #prints its argument in R console [1] \"I am at home\" x <- 6 print(x) #prints also variables [1] 6 #paste() combines arguments and variables print(paste(\"I went there\", x, \"times\")) [1] \"I went there 6 times\" Data storage (Vector, Matrix, Data frame, List) Vectors are one row data storages all of the same class (e.g. numeric , characters like words or logicals like TRUE, FALSE) allows arithmantics access with x[1] or x[c(1,2)] remove data with x[-1] or x[c(-1,-2)] Matrices data table A table with values, all of the same class access with x[1,2] [,2] or x[1,c(1,2)] Dataframes data table like a matrix but can store serveral classes into it access with x[1,2] [,2] or x[1,c(1,2)] Lists object list can combine objects like vectors, matrices, dataframes and more in a single variable each objects gets an identifier [[1]] , [[2]] and so on access with x[[2]][1] : the first [[ ]] is for the list ID, the second [ ] is for the data in the list, its [ ] or [,] depending of stored vector or dataframe/matrix (see above) Vectors values are included with combine c(<value1>, <value2>, <etc.>) x <- c(2,3,5) #Vector containing three numeric values x [1] 2 3 5 class(x) [1] \"numeric\" #numeric vector storage Vector commands length() function gives length of vector class() function gives value type typeof() also does this somehow Combining vectors * again with the c() command. x <- c(1,2,3) y <- c(\"a\",\"b\") #character vector z <- c(x,y) #new vector will contain elements of both vectors z [1] \"1\" \"2\" \"3\" \"a\" \"b\" class(z) [1] \"character\" #numeric values will be converted into character string. As a vector contains same data type Vector arithmetic's #each member of vector will be multiplied by 5 a <- c(1,3,5) 5*a [1] 5 15 25 #Addition or multiplication will be performed element by element b <- c(2,4,6) a + b [1] 3 7 11 a * b [1] 2 12 30 #multiplication as \"power to\" will be performed element by element a^2 [1]1 9 25 # power 2 for each member #if different length of vectors: then shorter will recycle - Recycling Rule. x <- c(2,4) y <- c(3,5,7,9) x + y [1] 5 9 Accessing Vector data The values of a vector can be retrieved <variable>[<position>] or removed <variable>[-<position>] Use c() function to retrieve more then one value * Use <- to redirect the values in new variables x <- c(\"a\",\"b\",\"c\",\"d\") x[3] #retrive command [1] c x[-3] # remove command [1] \"a\" \"b\" \"d\" # retrieving more values frome one vector x[c(2,3)] #use the combine argument [1] \"b\" \"c\" x[c(2,3,3)] [1] \"b\" \"c\" \"c\" #Duplicate indexes x[c(2,1,3)] [1] \"b\" \"a\" \"c\" #out of order indexes x[2:4] [1] \"b\" \"c\" \"d\" #Range index with colon \":\" operator Name Vector Members * Names can be assigned to vector members by using names( ) x <- c(\"Christian\", \"Brandt\") names(x)<- c(\"first_name\", \"last_name\") #naming each value entry of vector x x first_name last_name \"Christian\" \"Brandt\" # data retrival works also with names x[c(\"last_name\",\"first_name\")] last_name first_name \"Christian\" \"Brandt\" Other commands * such commands can all be combined like excel functions #Create a sequential vector from 10 to 20 steps by 2 seq(from=10,to=20,by=2) [1] 10 12 14 16 18 20 #Sequential vector of vector_length 5 which starts from 1 and incremented by 3 seq(from=1,by=3,length.out=5) [1] 1 4 7 10 13 #replication command rep(\"hello\",5) [1] \"hello\" \"hello\" \"hello\" \"hello\" \"hello\" #combine rep( ) functions c(rep(1,2),rep(2,3),rep(3,4)) [1] 1 1 2 2 2 3 3 3 3 #advanced examples of vector h x<- 1:10 x [1] 1 2 3 4 5 6 7 8 9 10 y <- x[x<8 & x>3] y [1] 4 5 6 7 Matrices Creating a Matrix using matrix e. g.: matrix(c(<values>),nrow= ,ncol=, byrow=TRUE/FALSE) leave nrow or ncol blank e.g. only ncol=2, byrow=TRUE to fill the table by creating 2 columns and as many rows as needed for the data combining matrices with the cbind() e. g. cbind(x,y) #Create a matrix of 2 rows and 3 columns; byrow=FALSE fills columns first z <- matrix ( c(1,2,3,4,5,6), nrow=2, ncol=3, byrow= TRUE) z [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 #Accessing the data x[2,3] [1] 6 # value of 2nd row and 3th column x[2,] [1] 2 4 6 # values of 2nd row x[,c(1,3)] [,1] [,3] [1,] 1 5 [2,] 2 6 Naming a matrix using dimnames then also retrivable by name e.g. x[\"row1\", c(\"col1\",\"col2\")] dimnames(x)<- list( c(\"row1\", \"row2\"), c(\"col1\", \"col2\", \"col3\") ) x col1 col2 col3 row1 1 2 3 row2 4 5 6 Show properties of matrix class(x) typeof(x) nrow(x) ncol(x) length(x) Dataframe - This is the Excel of R you want to use this each column is a class like in SPSS or other databases) create a dataframe using data.frame each column is a vector datafile a<- c(1,2,3) # numeric vector b<- c(\"x\",\"y\",\"z\") # character vector c<- c(TRUE,FALSE,TRUE) # logical vector # creating data frame df<- data.frame (a,b,c) # accessing data frame df a b c # header contains vector names 1 1 x TRUE 2 2 y FALSE 3 3 z TRUE #extracting data df[\"1\",\"b\"] [1] x Adding and removing data * using rbind for rows and cbind for columns #creating a dataframe df <- data.frame(first = c('Christian'), last = c('Brandt'), lucky_number = c(5), stringsAsFactors = FALSE) df first last lucky_number Christian Brandt 5 #adding a row df <- rbind(df, list('Erik', 'Bongcam', 10 )) df first last lucky_number 1 Christian Brandt 5 2 Erik Bongcam 10 #adding a column df <-cbind(df, coffeetime =c(TRUE,TRUE)) # removal of data with df1<- df[-2,] # remove 2nd row df2<- df[,-3] # remove 3rd column Assigning names # You can assign names to rows and columns as: dimnames(df)<- list ( c(\"row1\",\"row2\",\"row3\"), c(\"col1\",\"col2\",\"col3\") ) df col1 col2 col3 row1 1 TRUE row2 2 FALSE row3 3 TRUE # possible ways to get data (all with the same results) df[[3]] df[[\"col3\"]] df$col3 df[,\"col3\"] [1] TRUE FALSE TRUE #result for all 4 commands #retrieve multiple columns df[c(\"col2\",\"col3\")] List use the list() command to store objects in a data list Each object (vector, matrix etc.) gets a own ID which is [[1]] , [[2]] and so on. my_list <- list(a,b,33) #a and b is a vector, 33 is only a value my_list [[1]] [1] 1 2 3 [[2]] [1] \"SLU\" \"Uppsala\" [[3]] [1] 33 #Accessing data my_list[2] [[2]] [1] \"SLU\" \"Uppsala\" my_list[[2]][1] [1] \"SLU\" #Modify the contents of list members directly with [[ ]] my_list[[2]][1] = \"new_SLU\" Advanced data manipulation if -else, for, while function IF ELSE functions * example: x <- 9 if (x >= 10) { print(\"x is greater than or equal to 10\") } else { print(\"x is less than 10\") } Loop functions * for(i in <variable>) . Thats the counter. A interactive tutorial on loops can be found here (includes coding, while functions). for(i in 1:10) {print(i)} #output goes from 1 to 10 # or do stuff like this: m<- c(1:10) #a vector from 1 to 10 for(i in m){print(paste(\"5*\",i,\"=\",5*i))} #results is 5*3=15 for one of the 10 rows Working dir getwd() to get your current working dir setwd(\"/Users/naat0001/Desktop/\") to specify were your workdir should be now You can do this also in rStudio using the GUI Advanced folder creation with IF loop: mainDir <- \"/Users/naat0001/Desktop\" subDir <- \"outputDirectory\" if (file.exists(subDir)) #TRUE or FALSE { setwd(file.path(mainDir, subDir)) } else { dir.create(file.path(mainDir, subDir)) #creates dir setwd(file.path(mainDir, subDir)) } # file.path(mainDir, subDir) says basically # /Users/naat0001/Desktop/outputDirectory File import and download Use delimiter files with tab ; , like *.csv Use read.table with sep=\"\" (enter seperator here) and header=TRUE x <- read.table(\"path_to_file/file_name.csv\", sep=\",\", header = TRUE) # read.table needs sep=\"\" and header =TRUE Download and read a file download.file does store the file in your working dir with destfile= download.file(\"https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/gh-pages/_episodes_rmd/data/gapminder-FiveYearData.csv\", destfile = \"gapminder-FiveYearData.csv\") # read the file to a varible x <- read.csv(\"gapminder-FiveYearData.csv\") # you can also directly read the online file to a variable x <- read.table(\"https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/gh-pages/_episodes_rmd/data/gapminder-FiveYearData.csv\", sep=\",\", header = TRUE) Useful commands #x is a dataframe str(x) # tells you what x is, e.g. a dataframe and what class in each column is stored (e.g. numerical) head(x) #To view the first few rows of the data tail(x) #To view the last few rows of the data summary(x) #shows for each coloum various stuff like minimum, lower quartile, median, mean, upper quartile... nrow(x) #number of rows #nrow example with build in functions: nrow(x[x$BPressure==\"high\" | x$BPressure==\"pre-high\",]) #counts rows in colum BPressure with the value high oder pre-high #like SVERWEIS if value in BCHolesterol >190 show value of column Name in the same row x[x$BCholesterol>190,\"Name\"] Export data frames into a file You do this with the write.table() function If you don\u2019t have any row names set up for your data frame use this: write.table(data,file=\"out.txt\",sep=\"\\t\",quote=FALSE,row.names=FALSE) If you do have row names then use the command below which will keep the row names and will move the column names so they line up correctly with the data. write.table(data,file=\"out.txt\",sep=\"\\t\",quote=FALSE,col.names=NA)` Merging two files using the merge command you tell them where the ID is (like Excel SVERWEIS) # lets say i have 2 dataframes: df1 <- data.frame(Chr=paste0('chr',1:9), Gene=paste0('gene',19:11)) df2 <- data.frame(Chr=paste0('chr',2:10),Position=paste0('pos',22:30)) # they look like this, shortend it with (...) > df1 Chr Gene 1 chr1 gene19 2 chr2 gene18 3 chr3 gene17 4 chr4 gene16 (...) > df2 Chr Position 1 chr2 pos22 2 chr3 pos23 3 chr4 pos24 4 chr5 pos25 (...) # you need two uniq column names, one in each dataframe # we merge them based on the uniq \"Chr\" column (our \"ID\") df3 <- merge(df1, df2, by=\"Chr\") # it only works if the column names are the same # so use this to name them (changes the name of df2 column 1 to \"Chr_ID\": colnames(df2)[1] <- \"Chr_ID\" > df3 Chr Gene Position 1 chr2 gene18 pos22 2 chr3 gene17 pos23 3 chr4 gene16 pos24 4 chr5 gene15 pos25 (...) # alternatively say which columns should be used for the merging: df3 <- merge(df1, df2, by.x=\"Chr\", by.y=\"Chr_ID\") Graphs in R Overview of some basic plots can be found here . Advanced graphs can be found here . This example is based on the hist plot: x <- rnorm(50) # 50 random values from normal distribution. hist(x) # plot histogram of values of x hist(x,main=\"title of histogram\") # change title hist(x, main = \"Title of histogram\", col = \"orange\") # with color bars # How to save / export graphs from console in pdf format. pdf(file=\"my_histogram.pdf\") hist(x, main = \"Title of histogram\", col = \"orange\") dev.off() An introduction to R R for data science","title":"Basics"},{"location":"R/R/#overview-of-r","text":"","title":"Overview of R"},{"location":"R/R/#installing-r-base-language-and-r-studio-environment","text":"for Linux Mint (Xenial) # Installing R language sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 sudo add-apt-repository 'deb [arch=amd64,i386] https://cran.rstudio.com/bin/linux/ubuntu xenial/' sudo apt-get update sudo apt-get install r-base Get the R-Studio from here . If you want to code in terminal type R . Now you are in a R environment (\"console\"). q() to exit R in terminal. But use R Studio for scripting not the terminal. Installing packages in R Packages are organized in repositories: CRAN , Bioconductor , R-forge , Github or * Googlecode Installing a package in R and first time source location # 1 establish bioconductor as a source, first time only source(\"https://bioconductor.org/biocLite.R\") # 2 install package methylKit from biocLite biocLite(\"methylKit\") # CRAN # install from CRAN install.packages(\"fortunes\")","title":"Installing r-base (language) and R-Studio (environment)"},{"location":"R/R/#basics","text":"\"file\" -> \"new project\" -> \"new directory\" and save it to a location \"file\" -> \"new file\" -> \"R Script\" The output of R is given as [1] in the console Help functions Get help in R here or you use help commands: help(read.table) # help on a function ?read.table # is the same help.search(\"deviation\") ??deviation # is the same","title":"Basics"},{"location":"R/R/#variables","text":"Variables: any letter or word incl. numbers, _ or . Variables are case sensitive and don't have to be declared first x <- 4+3 # adds sum of 4+3 to the variable x y <- 9 # also shown in the environment window or with ls() x+y [1] 16 # result of x+y","title":"Variables"},{"location":"R/R/#operators","text":"More tutorials about operators # Arithmetic Operators +, -, *, /, %%, %/%, ^ # Relational Operators, TRUE or FALSE as a result >, <, >=, <=, ==, != # Logical Operators, & = AND, | = OR &, | , !, &&, || #&& = searches if the value is somewhere in the vectors 4<3 | 4>3 #4<3 = FALSE, 4>3 = TRUE [1] TRUE # Assignment Operators <-, =, <<-, ->, ->> # Miscellaneous Operators %in%, %*% # check help for more","title":"Operators"},{"location":"R/R/#valuedata-types","text":"# Numeric \u2013 A floating point number (default data type of R): 2.4, 9, 888 # Integer \u2013 A number with L suffix: 2L, 25L, 0L # Character - Any amount of text: \"a\", \"school is a house\", \"66\" # Logical values: TRUE, FALSE # Complex \u2013 defined via imaginary value i: 2+3i, 5+8i","title":"Value/data types"},{"location":"R/R/#class-and-typeof-functions","text":"# class( ) to determine the class of the R object # typeof( ) to determine the internal type (storage mode) of the object typeof(5.67) [1] \"double\" class(5) [1] \"numeric\" # To create integer variable in R, you invoke as.integer() function. x <- as.integer(3) x [1] 3 class(var_name) [1] \"integer\"","title":"class() and typeof() functions"},{"location":"R/R/#print-and-paste-functions","text":"print(\"I am at home\") #prints its argument in R console [1] \"I am at home\" x <- 6 print(x) #prints also variables [1] 6 #paste() combines arguments and variables print(paste(\"I went there\", x, \"times\")) [1] \"I went there 6 times\"","title":"print() and paste() functions"},{"location":"R/R/#data-storage-vector-matrix-data-frame-list","text":"Vectors are one row data storages all of the same class (e.g. numeric , characters like words or logicals like TRUE, FALSE) allows arithmantics access with x[1] or x[c(1,2)] remove data with x[-1] or x[c(-1,-2)] Matrices data table A table with values, all of the same class access with x[1,2] [,2] or x[1,c(1,2)] Dataframes data table like a matrix but can store serveral classes into it access with x[1,2] [,2] or x[1,c(1,2)] Lists object list can combine objects like vectors, matrices, dataframes and more in a single variable each objects gets an identifier [[1]] , [[2]] and so on access with x[[2]][1] : the first [[ ]] is for the list ID, the second [ ] is for the data in the list, its [ ] or [,] depending of stored vector or dataframe/matrix (see above)","title":"Data storage (Vector, Matrix, Data frame, List)"},{"location":"R/R/#vectors","text":"values are included with combine c(<value1>, <value2>, <etc.>) x <- c(2,3,5) #Vector containing three numeric values x [1] 2 3 5 class(x) [1] \"numeric\" #numeric vector storage Vector commands length() function gives length of vector class() function gives value type typeof() also does this somehow Combining vectors * again with the c() command. x <- c(1,2,3) y <- c(\"a\",\"b\") #character vector z <- c(x,y) #new vector will contain elements of both vectors z [1] \"1\" \"2\" \"3\" \"a\" \"b\" class(z) [1] \"character\" #numeric values will be converted into character string. As a vector contains same data type Vector arithmetic's #each member of vector will be multiplied by 5 a <- c(1,3,5) 5*a [1] 5 15 25 #Addition or multiplication will be performed element by element b <- c(2,4,6) a + b [1] 3 7 11 a * b [1] 2 12 30 #multiplication as \"power to\" will be performed element by element a^2 [1]1 9 25 # power 2 for each member #if different length of vectors: then shorter will recycle - Recycling Rule. x <- c(2,4) y <- c(3,5,7,9) x + y [1] 5 9 Accessing Vector data The values of a vector can be retrieved <variable>[<position>] or removed <variable>[-<position>] Use c() function to retrieve more then one value * Use <- to redirect the values in new variables x <- c(\"a\",\"b\",\"c\",\"d\") x[3] #retrive command [1] c x[-3] # remove command [1] \"a\" \"b\" \"d\" # retrieving more values frome one vector x[c(2,3)] #use the combine argument [1] \"b\" \"c\" x[c(2,3,3)] [1] \"b\" \"c\" \"c\" #Duplicate indexes x[c(2,1,3)] [1] \"b\" \"a\" \"c\" #out of order indexes x[2:4] [1] \"b\" \"c\" \"d\" #Range index with colon \":\" operator Name Vector Members * Names can be assigned to vector members by using names( ) x <- c(\"Christian\", \"Brandt\") names(x)<- c(\"first_name\", \"last_name\") #naming each value entry of vector x x first_name last_name \"Christian\" \"Brandt\" # data retrival works also with names x[c(\"last_name\",\"first_name\")] last_name first_name \"Christian\" \"Brandt\" Other commands * such commands can all be combined like excel functions #Create a sequential vector from 10 to 20 steps by 2 seq(from=10,to=20,by=2) [1] 10 12 14 16 18 20 #Sequential vector of vector_length 5 which starts from 1 and incremented by 3 seq(from=1,by=3,length.out=5) [1] 1 4 7 10 13 #replication command rep(\"hello\",5) [1] \"hello\" \"hello\" \"hello\" \"hello\" \"hello\" #combine rep( ) functions c(rep(1,2),rep(2,3),rep(3,4)) [1] 1 1 2 2 2 3 3 3 3 #advanced examples of vector h x<- 1:10 x [1] 1 2 3 4 5 6 7 8 9 10 y <- x[x<8 & x>3] y [1] 4 5 6 7","title":"Vectors"},{"location":"R/R/#matrices","text":"Creating a Matrix using matrix e. g.: matrix(c(<values>),nrow= ,ncol=, byrow=TRUE/FALSE) leave nrow or ncol blank e.g. only ncol=2, byrow=TRUE to fill the table by creating 2 columns and as many rows as needed for the data combining matrices with the cbind() e. g. cbind(x,y) #Create a matrix of 2 rows and 3 columns; byrow=FALSE fills columns first z <- matrix ( c(1,2,3,4,5,6), nrow=2, ncol=3, byrow= TRUE) z [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 #Accessing the data x[2,3] [1] 6 # value of 2nd row and 3th column x[2,] [1] 2 4 6 # values of 2nd row x[,c(1,3)] [,1] [,3] [1,] 1 5 [2,] 2 6 Naming a matrix using dimnames then also retrivable by name e.g. x[\"row1\", c(\"col1\",\"col2\")] dimnames(x)<- list( c(\"row1\", \"row2\"), c(\"col1\", \"col2\", \"col3\") ) x col1 col2 col3 row1 1 2 3 row2 4 5 6 Show properties of matrix class(x) typeof(x) nrow(x) ncol(x) length(x)","title":"Matrices"},{"location":"R/R/#dataframe-this-is-the-excel-of-r","text":"you want to use this each column is a class like in SPSS or other databases) create a dataframe using data.frame each column is a vector datafile a<- c(1,2,3) # numeric vector b<- c(\"x\",\"y\",\"z\") # character vector c<- c(TRUE,FALSE,TRUE) # logical vector # creating data frame df<- data.frame (a,b,c) # accessing data frame df a b c # header contains vector names 1 1 x TRUE 2 2 y FALSE 3 3 z TRUE #extracting data df[\"1\",\"b\"] [1] x Adding and removing data * using rbind for rows and cbind for columns #creating a dataframe df <- data.frame(first = c('Christian'), last = c('Brandt'), lucky_number = c(5), stringsAsFactors = FALSE) df first last lucky_number Christian Brandt 5 #adding a row df <- rbind(df, list('Erik', 'Bongcam', 10 )) df first last lucky_number 1 Christian Brandt 5 2 Erik Bongcam 10 #adding a column df <-cbind(df, coffeetime =c(TRUE,TRUE)) # removal of data with df1<- df[-2,] # remove 2nd row df2<- df[,-3] # remove 3rd column Assigning names # You can assign names to rows and columns as: dimnames(df)<- list ( c(\"row1\",\"row2\",\"row3\"), c(\"col1\",\"col2\",\"col3\") ) df col1 col2 col3 row1 1 TRUE row2 2 FALSE row3 3 TRUE # possible ways to get data (all with the same results) df[[3]] df[[\"col3\"]] df$col3 df[,\"col3\"] [1] TRUE FALSE TRUE #result for all 4 commands #retrieve multiple columns df[c(\"col2\",\"col3\")]","title":"Dataframe - This is the Excel of R"},{"location":"R/R/#list","text":"use the list() command to store objects in a data list Each object (vector, matrix etc.) gets a own ID which is [[1]] , [[2]] and so on. my_list <- list(a,b,33) #a and b is a vector, 33 is only a value my_list [[1]] [1] 1 2 3 [[2]] [1] \"SLU\" \"Uppsala\" [[3]] [1] 33 #Accessing data my_list[2] [[2]] [1] \"SLU\" \"Uppsala\" my_list[[2]][1] [1] \"SLU\" #Modify the contents of list members directly with [[ ]] my_list[[2]][1] = \"new_SLU\"","title":"List"},{"location":"R/R/#advanced-data-manipulation","text":"","title":"Advanced data manipulation"},{"location":"R/R/#if-else-for-while-function","text":"IF ELSE functions * example: x <- 9 if (x >= 10) { print(\"x is greater than or equal to 10\") } else { print(\"x is less than 10\") } Loop functions * for(i in <variable>) . Thats the counter. A interactive tutorial on loops can be found here (includes coding, while functions). for(i in 1:10) {print(i)} #output goes from 1 to 10 # or do stuff like this: m<- c(1:10) #a vector from 1 to 10 for(i in m){print(paste(\"5*\",i,\"=\",5*i))} #results is 5*3=15 for one of the 10 rows","title":"if -else, for, while function"},{"location":"R/R/#working-dir","text":"getwd() to get your current working dir setwd(\"/Users/naat0001/Desktop/\") to specify were your workdir should be now You can do this also in rStudio using the GUI Advanced folder creation with IF loop: mainDir <- \"/Users/naat0001/Desktop\" subDir <- \"outputDirectory\" if (file.exists(subDir)) #TRUE or FALSE { setwd(file.path(mainDir, subDir)) } else { dir.create(file.path(mainDir, subDir)) #creates dir setwd(file.path(mainDir, subDir)) } # file.path(mainDir, subDir) says basically # /Users/naat0001/Desktop/outputDirectory","title":"Working dir"},{"location":"R/R/#file-import-and-download","text":"Use delimiter files with tab ; , like *.csv Use read.table with sep=\"\" (enter seperator here) and header=TRUE x <- read.table(\"path_to_file/file_name.csv\", sep=\",\", header = TRUE) # read.table needs sep=\"\" and header =TRUE Download and read a file download.file does store the file in your working dir with destfile= download.file(\"https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/gh-pages/_episodes_rmd/data/gapminder-FiveYearData.csv\", destfile = \"gapminder-FiveYearData.csv\") # read the file to a varible x <- read.csv(\"gapminder-FiveYearData.csv\") # you can also directly read the online file to a variable x <- read.table(\"https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/gh-pages/_episodes_rmd/data/gapminder-FiveYearData.csv\", sep=\",\", header = TRUE) Useful commands #x is a dataframe str(x) # tells you what x is, e.g. a dataframe and what class in each column is stored (e.g. numerical) head(x) #To view the first few rows of the data tail(x) #To view the last few rows of the data summary(x) #shows for each coloum various stuff like minimum, lower quartile, median, mean, upper quartile... nrow(x) #number of rows #nrow example with build in functions: nrow(x[x$BPressure==\"high\" | x$BPressure==\"pre-high\",]) #counts rows in colum BPressure with the value high oder pre-high #like SVERWEIS if value in BCHolesterol >190 show value of column Name in the same row x[x$BCholesterol>190,\"Name\"] Export data frames into a file You do this with the write.table() function If you don\u2019t have any row names set up for your data frame use this: write.table(data,file=\"out.txt\",sep=\"\\t\",quote=FALSE,row.names=FALSE) If you do have row names then use the command below which will keep the row names and will move the column names so they line up correctly with the data. write.table(data,file=\"out.txt\",sep=\"\\t\",quote=FALSE,col.names=NA)`","title":"File import and download"},{"location":"R/R/#merging-two-files","text":"using the merge command you tell them where the ID is (like Excel SVERWEIS) # lets say i have 2 dataframes: df1 <- data.frame(Chr=paste0('chr',1:9), Gene=paste0('gene',19:11)) df2 <- data.frame(Chr=paste0('chr',2:10),Position=paste0('pos',22:30)) # they look like this, shortend it with (...) > df1 Chr Gene 1 chr1 gene19 2 chr2 gene18 3 chr3 gene17 4 chr4 gene16 (...) > df2 Chr Position 1 chr2 pos22 2 chr3 pos23 3 chr4 pos24 4 chr5 pos25 (...) # you need two uniq column names, one in each dataframe # we merge them based on the uniq \"Chr\" column (our \"ID\") df3 <- merge(df1, df2, by=\"Chr\") # it only works if the column names are the same # so use this to name them (changes the name of df2 column 1 to \"Chr_ID\": colnames(df2)[1] <- \"Chr_ID\" > df3 Chr Gene Position 1 chr2 gene18 pos22 2 chr3 gene17 pos23 3 chr4 gene16 pos24 4 chr5 gene15 pos25 (...) # alternatively say which columns should be used for the merging: df3 <- merge(df1, df2, by.x=\"Chr\", by.y=\"Chr_ID\")","title":"Merging two files"},{"location":"R/R/#graphs-in-r","text":"Overview of some basic plots can be found here . Advanced graphs can be found here . This example is based on the hist plot: x <- rnorm(50) # 50 random values from normal distribution. hist(x) # plot histogram of values of x hist(x,main=\"title of histogram\") # change title hist(x, main = \"Title of histogram\", col = \"orange\") # with color bars # How to save / export graphs from console in pdf format. pdf(file=\"my_histogram.pdf\") hist(x, main = \"Title of histogram\", col = \"orange\") dev.off() An introduction to R R for data science","title":"Graphs in R"},{"location":"R/distanceheatmap/","text":"Distance based heatmap.2 tutorial to create a distance based heatmap fast distance estimation via mash plotting via R Another good guide about heatmap.2 can be found here Distance via mash see github here see example below with a folder full of fasta files # better/faster as a blast all versus all mash sketch -o reference *.fasta mash info reference.msh mash dist reference.msh *.fasta > results.tsv this creates a results.tsv for R this file was modified afterwards the accession numbers were renamed and metadata was added to each name, e.g. binA_11_IncA important if you want to add colors to each branch (see picture below) Plotting in R step by step in R example picture: Dependencies installing and loading dependencies source(\"https://bioconductor.org/biocLite.R\") biocLite(\"gplots\") library(reshape2);packageVersion(\"reshape2\") library(RColorBrewer);packageVersion(\"RColorBrewer\") library(\"gplots\");packageVersion(\"gplots\") load data and create matrix setwd(\"<path to file>\") df.results <- read.table(\"results.tsv\", header = T, sep = \"\\t\") matrix.results <- acast(df.results, binA~binB, value.var = \"distance\") Optional meta data optional step to add colors to branches greps certain values in the name to give it a color meta data has to be added to the name (e.g. accessionnumber_metadata) inc_colour <- unlist(lapply(rownames(matrix.results), function(x){ if (grepl(\"no\", x)) \"#FFFFFF\" # white else if (grepl(\"IncFIB\", x))\"#FFA500\" # orange else if (grepl(\"IncA/C2\", x))\"#BDB76B\" # dark khaki else if (grepl(\"IncA/C\", x))\"#808000\" # olive else if (grepl(\"IncN\", x))\"#FFD700\" # gold else if (grepl(\"IncQ1\", x))\"#00FF00\" # lime else if (grepl(\"IncFIA\", x))\"#00FFFF\" # cyan else if (grepl(\"IncX3\", x))\"#D2691E\" # chocolate else if (grepl(\"IncFII\", x))\"#40E0D0\" # turquoise else if (grepl(\"IncL/M\", x))\"#8B008B\" # dark magenta else if (grepl(\"IncI2\", x))\"#8A2BE2\" # blue violet else if (grepl(\"IncU\", x))\"#800000\" # maroon else if (grepl(\"IncP\", x))\"#FF1493\" # deep pink else if (grepl(\"IncQ2\", x))\"#6495ED\" # corn flower blue })) # important length has to be similar to matrix columns and rows length(inc_colour) Plotting data create plot via heatmap2 # same color gradient as the image from above Colors <- rev(brewer.pal(11, \"Spectral\")) # use spectral color.spek <- colorRampPalette(Colors)(30) # use this if you want 0 set to white (change the heatmap2 below) color.zerow <- c(\"#F8F8F8\", colorRampPalette(Colors)(n=900)) # plotting data heatmap.2(matrix.plasmids, trace = \"none\", tracecol = \"#000000\", col = color.spek, margins = c(11, 7.5), keysize = 0.8, labRow = FALSE, labCol = FALSE, na.rm = T, key.title = NA, key.xlab = \"Distance\", ColSideColors = inc_colour, RowSideColors = inc_colour) add legend to picture # add name and add colors leg.name <- c(\"None\", \"IncFIB\", \"IncA/C2\", \"IncA/C\") leg.color <- c(\"#FFFFFF\", \"#FFA500\", \"#BDB76B\", \"#808000\") # add legend to heatmap legend(\"bottom\", legend = paste(INC_names), fill = INC_names_legend, cex = 0.65, ncol = 7, horiz = F) save as vector graphic # starts saving everything to svg svg(\"Heatmap_spectral.svg\", height = 9, width = 9) # add here the ploting functions e.g. heatmap.2(.......) # see above legend(....) # stops saving everything to svg dev.off()","title":"heatmap"},{"location":"R/distanceheatmap/#distance-based-heatmap2","text":"tutorial to create a distance based heatmap fast distance estimation via mash plotting via R Another good guide about heatmap.2 can be found here","title":"Distance based heatmap.2"},{"location":"R/distanceheatmap/#distance-via-mash","text":"see github here see example below with a folder full of fasta files # better/faster as a blast all versus all mash sketch -o reference *.fasta mash info reference.msh mash dist reference.msh *.fasta > results.tsv this creates a results.tsv for R this file was modified afterwards the accession numbers were renamed and metadata was added to each name, e.g. binA_11_IncA important if you want to add colors to each branch (see picture below)","title":"Distance via mash"},{"location":"R/distanceheatmap/#plotting-in-r","text":"step by step in R example picture:","title":"Plotting in R"},{"location":"R/distanceheatmap/#dependencies","text":"installing and loading dependencies source(\"https://bioconductor.org/biocLite.R\") biocLite(\"gplots\") library(reshape2);packageVersion(\"reshape2\") library(RColorBrewer);packageVersion(\"RColorBrewer\") library(\"gplots\");packageVersion(\"gplots\") load data and create matrix setwd(\"<path to file>\") df.results <- read.table(\"results.tsv\", header = T, sep = \"\\t\") matrix.results <- acast(df.results, binA~binB, value.var = \"distance\")","title":"Dependencies"},{"location":"R/distanceheatmap/#optional-meta-data","text":"optional step to add colors to branches greps certain values in the name to give it a color meta data has to be added to the name (e.g. accessionnumber_metadata) inc_colour <- unlist(lapply(rownames(matrix.results), function(x){ if (grepl(\"no\", x)) \"#FFFFFF\" # white else if (grepl(\"IncFIB\", x))\"#FFA500\" # orange else if (grepl(\"IncA/C2\", x))\"#BDB76B\" # dark khaki else if (grepl(\"IncA/C\", x))\"#808000\" # olive else if (grepl(\"IncN\", x))\"#FFD700\" # gold else if (grepl(\"IncQ1\", x))\"#00FF00\" # lime else if (grepl(\"IncFIA\", x))\"#00FFFF\" # cyan else if (grepl(\"IncX3\", x))\"#D2691E\" # chocolate else if (grepl(\"IncFII\", x))\"#40E0D0\" # turquoise else if (grepl(\"IncL/M\", x))\"#8B008B\" # dark magenta else if (grepl(\"IncI2\", x))\"#8A2BE2\" # blue violet else if (grepl(\"IncU\", x))\"#800000\" # maroon else if (grepl(\"IncP\", x))\"#FF1493\" # deep pink else if (grepl(\"IncQ2\", x))\"#6495ED\" # corn flower blue })) # important length has to be similar to matrix columns and rows length(inc_colour)","title":"Optional meta data"},{"location":"R/distanceheatmap/#plotting-data","text":"create plot via heatmap2 # same color gradient as the image from above Colors <- rev(brewer.pal(11, \"Spectral\")) # use spectral color.spek <- colorRampPalette(Colors)(30) # use this if you want 0 set to white (change the heatmap2 below) color.zerow <- c(\"#F8F8F8\", colorRampPalette(Colors)(n=900)) # plotting data heatmap.2(matrix.plasmids, trace = \"none\", tracecol = \"#000000\", col = color.spek, margins = c(11, 7.5), keysize = 0.8, labRow = FALSE, labCol = FALSE, na.rm = T, key.title = NA, key.xlab = \"Distance\", ColSideColors = inc_colour, RowSideColors = inc_colour) add legend to picture # add name and add colors leg.name <- c(\"None\", \"IncFIB\", \"IncA/C2\", \"IncA/C\") leg.color <- c(\"#FFFFFF\", \"#FFA500\", \"#BDB76B\", \"#808000\") # add legend to heatmap legend(\"bottom\", legend = paste(INC_names), fill = INC_names_legend, cex = 0.65, ncol = 7, horiz = F) save as vector graphic # starts saving everything to svg svg(\"Heatmap_spectral.svg\", height = 9, width = 9) # add here the ploting functions e.g. heatmap.2(.......) # see above legend(....) # stops saving everything to svg dev.off()","title":"Plotting data"},{"location":"R/ggplot/","text":"ggplot2 Basic heatmap source(\"https://bioconductor.org/biocLite.R\") biocLite(\"ggplot2\") # install ggplot2 library(ggplot2) # load library # get data into R setwd(\"D:/UKJ_Project_Folder/KPC_Letter/mesh\") df.datafile <- read.table(\"results_all.tsv\", sep=\"\", header = FALSE) # get you column names with head(df.datafile) # say which column for what (V1 to V3 here) gg <- ggplot(data = df.datafile, aes(x = V2, y = V1)) + geom_tile(aes(fill = V3)) # to add more heatmap modification to it do \"gg <- gg + (...)\" # plotting the output gg","title":"ggplot"},{"location":"R/ggplot/#ggplot2","text":"","title":"ggplot2"},{"location":"R/ggplot/#basic-heatmap","text":"source(\"https://bioconductor.org/biocLite.R\") biocLite(\"ggplot2\") # install ggplot2 library(ggplot2) # load library # get data into R setwd(\"D:/UKJ_Project_Folder/KPC_Letter/mesh\") df.datafile <- read.table(\"results_all.tsv\", sep=\"\", header = FALSE) # get you column names with head(df.datafile) # say which column for what (V1 to V3 here) gg <- ggplot(data = df.datafile, aes(x = V2, y = V1)) + geom_tile(aes(fill = V3)) # to add more heatmap modification to it do \"gg <- gg + (...)\" # plotting the output gg","title":"Basic heatmap"},{"location":"docker/docker/","text":"Docker (hub.docker.com) docker can't be installed on Win10 Home edition Created a VM with ubuntu (ubuntu flavor also) activate in the BIOS \"VT-d\" and install a \"ubuntu VM\" via VMWare Getting started Installing docker: install guide for docker here update the apt to find the docker repositories: 1. install dependencies for docker sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 2. get docker key and verify it: sudo apt-key fingerprint 0EBFCD88 # results: pub 4096R/0EBFCD88 2017-02-22 Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 uid Docker Release (CE deb) <docker@docker.com> sub 4096R/F273FCD8 2017-02-22 3. add the docker repository sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" 4. Install docker sudo apt-get update sudo apt-get install docker-ce sudo docker run hello-world add \"docker\" to your group so you don't have to type sudo every time sudo usermod -a -G docker $USER sudo reboot Important commands docker run --rm <imagename> <command> # runs image as a container and removes it afterwards docker pull <repositoryname/dockername> # basically git clone of a docker image docker build -t <image_name> . # build a image from Docker file in . docker images # shows all images docker rmi <name> # removes a image docker ps -a # shows all current containers (active and exited) docker rm <name> # removes docker container (IMPORTANT if you want to clean up) Creating docker images via automated builds on hub.docker.com this means I create Dockerfiles in ubuntu and check if they can be created, than I only push the Dockerfiles to github create a new automated rep on dockerhub name it like the image (e.g. debian_basic) point under options to the correct directory in github dockerhub needs to be linked to your github ssh into docker containers this makes creating Dockerfiles much more easier since you can check if something is wrong or not to get into a docker container you have to attach a images (then called container) # get into the docker bash docker run --name docker_running -d replikation/debian_basic /bin/bash # this can be done to just activate a container: docker run --name docker_running -d replikation/debian_basic sleep 8h # afterwards it will exits and detach the container # check all docker containers with docker ps -a More information here: create auto build for dockers - LINK Run dockers examples example docker run --rm -it -v /path/to/example_data:/example_data andrewjpage/tiptoft tiptoft /example_data/ERS654932_plasmids.fastq.gz example porechop docker run --rm -it -v /home/christian/Desktop/Docker_experimental_area:/Docker_experimental_area porechop porechop -i /Docker_experimental_area/all2.fastq -b /Docker_experimental_area/demultiplexed --rm removes docker after it exits, so you dont \"fill up\" on containers Docker pipelines they need to run with -i and without -t docker run --rm -i replikation/nanopolish echo $((3+4888)) | \\ docker run --rm -i replikation/nanopolish tr -d 1 ``` ## Docker Wildcards * WILDCARDS in docker via run are only interpreted via shell * so run a command like this with `sh -c`: docker run --rm -i replikation/nanopolish sh -c 'ls /folder/*.fasta' ``` Clean up for dockers update all docker images change the REPOSITORY to your repository name were all the images are stored docker images |grep -v REPOSITORY|awk '{print $1}'|xargs -L1 docker pull stop all containers docker stop $(docker ps -a -q) remove all container docker rm $(docker ps -a -q) remove all untagged images docker rmi $(docker images -f \"dangling=true\" -q)","title":"Docker-Basics"},{"location":"docker/docker/#docker-hubdockercom","text":"docker can't be installed on Win10 Home edition Created a VM with ubuntu (ubuntu flavor also) activate in the BIOS \"VT-d\" and install a \"ubuntu VM\" via VMWare","title":"Docker (hub.docker.com)"},{"location":"docker/docker/#getting-started","text":"","title":"Getting started"},{"location":"docker/docker/#installing-docker","text":"install guide for docker here update the apt to find the docker repositories: 1. install dependencies for docker sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 2. get docker key and verify it: sudo apt-key fingerprint 0EBFCD88 # results: pub 4096R/0EBFCD88 2017-02-22 Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 uid Docker Release (CE deb) <docker@docker.com> sub 4096R/F273FCD8 2017-02-22 3. add the docker repository sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" 4. Install docker sudo apt-get update sudo apt-get install docker-ce sudo docker run hello-world add \"docker\" to your group so you don't have to type sudo every time sudo usermod -a -G docker $USER sudo reboot","title":"Installing docker:"},{"location":"docker/docker/#important-commands","text":"docker run --rm <imagename> <command> # runs image as a container and removes it afterwards docker pull <repositoryname/dockername> # basically git clone of a docker image docker build -t <image_name> . # build a image from Docker file in . docker images # shows all images docker rmi <name> # removes a image docker ps -a # shows all current containers (active and exited) docker rm <name> # removes docker container (IMPORTANT if you want to clean up)","title":"Important commands"},{"location":"docker/docker/#creating-docker-images","text":"via automated builds on hub.docker.com this means I create Dockerfiles in ubuntu and check if they can be created, than I only push the Dockerfiles to github create a new automated rep on dockerhub name it like the image (e.g. debian_basic) point under options to the correct directory in github dockerhub needs to be linked to your github","title":"Creating docker images"},{"location":"docker/docker/#ssh-into-docker-containers","text":"this makes creating Dockerfiles much more easier since you can check if something is wrong or not to get into a docker container you have to attach a images (then called container) # get into the docker bash docker run --name docker_running -d replikation/debian_basic /bin/bash # this can be done to just activate a container: docker run --name docker_running -d replikation/debian_basic sleep 8h # afterwards it will exits and detach the container # check all docker containers with docker ps -a More information here: create auto build for dockers - LINK","title":"ssh into docker containers"},{"location":"docker/docker/#run-dockers-examples","text":"example docker run --rm -it -v /path/to/example_data:/example_data andrewjpage/tiptoft tiptoft /example_data/ERS654932_plasmids.fastq.gz example porechop docker run --rm -it -v /home/christian/Desktop/Docker_experimental_area:/Docker_experimental_area porechop porechop -i /Docker_experimental_area/all2.fastq -b /Docker_experimental_area/demultiplexed --rm removes docker after it exits, so you dont \"fill up\" on containers","title":"Run dockers examples"},{"location":"docker/docker/#docker-pipelines","text":"they need to run with -i and without -t docker run --rm -i replikation/nanopolish echo $((3+4888)) | \\ docker run --rm -i replikation/nanopolish tr -d 1 ``` ## Docker Wildcards * WILDCARDS in docker via run are only interpreted via shell * so run a command like this with `sh -c`: docker run --rm -i replikation/nanopolish sh -c 'ls /folder/*.fasta' ```","title":"Docker pipelines"},{"location":"docker/docker/#clean-up-for-dockers","text":"update all docker images change the REPOSITORY to your repository name were all the images are stored docker images |grep -v REPOSITORY|awk '{print $1}'|xargs -L1 docker pull stop all containers docker stop $(docker ps -a -q) remove all container docker rm $(docker ps -a -q) remove all untagged images docker rmi $(docker images -f \"dangling=true\" -q)","title":"Clean up for dockers"},{"location":"metagenome/metabarcoding/","text":"Metabarcoding Overview very biased part since we only look at one very small gene a rapid method of high-throughput, DNA-based identification of multiple species from a complex and possibly degraded sample of DNA or from mass collection of specimens 16S rRNA regions for genomic classification not ideal, 23S or more gene would be useful, but most reference only exist on 16S platform dependent sequence information (short reads is less information) usually illumina is used because its cheap, but nanopore data or pacbio will be the future standard since the results are way better Preparation steps for metabarcoding using Illumina. Programs to use: + QIIME but its buggy + MOTHUR also buggy + WIMP - what's in my pot nanopore approach, its really good Biodiversity \u03b1 = represents a local habitat/environment/sample can be plotted using the plot_richness function of the phylosec package this is a wrapper featuring 7 plots like \"Shannon\" \u03b2 = is the differences between two \u03b1 (samples) can be plotted using the plot_ordination function of the phylosec package declare the method first ( ord <- ordinate(physeq, 'NMDS', 'bray' ) see below for the different \u03b2 diversity methods \u03b3 = describes the total diversity of an ecosystem or of all gathered samples Illustration of \u03b1, \u03b2, and \u03b3 diversity: Figure: Circles represent \u03b1 diversity as richness of species (symbols) in a sample (local). Dashed box represents \u03b3 diversity (diversity within a set of samples or within a larger region). \u03b2 diversity describes the differences between samples (\u03b2~D~ = similarity; \u03b2~P~ = species richness comparision). This is a more complex description and explained in detail in this paper . \u03b2 diversity analysis Plotting your data: PCA versus MDA Principal Component Analysis (PCA) or Multiple Discriminant Analysis (MDA) both eliminate an axis (e.g. from 3D to 2D) while maximizing the variance on the x-axis, see example MDA also maximizes the spread of the 2D data more information here log-transform is used to filter off trivial effects, which could dominate our PCA without log normalization with log normalization Which distance method to choose for \u03b2 diversity? A overview can be found in this publication . identities only = binary; Abundance included = quantitative All methods destinquish whether they exclude or include joint absences: Metabarcoding - practical Another detailed tutorial about DADA2 it can be found here we use the DADA2 apporach (very extensive manual and very memory efficient while being fast) is a relatively new method to analyse amplicon data which uses exact variants instead of OTUs 1. Install and Load Packages install DADA2 and other necessary packages source('https://bioconductor.org/biocLite.R') biocLite('dada2') biocLite('phyloseq') biocLite('DECIPHER') install.packages('ggplot2') install.packages('phangorn') load the packages and verify you have the correct DADA2 version library(dada2) library(ggplot2) library(phyloseq) library(phangorn) library(DECIPHER) packageVersion('dada2') downloaded our data in shell wget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip unzip MiSeqSOPData.zip cd MiSeq_SOP wget https://zenodo.org/record/824551/files/silva_nr_v128_train_set.fa.gz wget https://zenodo.org/record/824551/files/silva_species_assignment_v128.fa.gz Assign in R the path to our data to a variable and check it path <- '~/MiSeq_SOP' list.files(path) 2. Filtering and Trimming the Reads in R using DADA2 create two lists with the sorted name of the reads: one for forward reads, one for reverse reads raw_forward <- sort(list.files(path, pattern=\"_R1_001.fastq\", full.names=TRUE)) raw_reverse <- sort(list.files(path, pattern=\"_R2_001.fastq\", full.names=TRUE)) # we also need the sample names sample_names <- sapply(strsplit(basename(raw_forward), \"_\"), `[`, 1) visualizing the quality of our reads plotQualityProfile(raw_forward[1:2]) plotQualityProfile(raw_reverse[1:2]) The quality plots (for reverse) are looking like this: really bad quality starting at around 150 bp Dada2 requires us to define the name of our output files first before trimming # place filtered files in filtered/ subdirectory filtered_path <- file.path(path, \"filtered\") filtered_forward <- file.path(filtered_path, paste0(sample_names, \"_R1_trimmed.fastq.gz\")) filtered_reverse <- file.path(filtered_path, paste0(sample_names, \"_R2_trimmed.fastq.gz\")) for filtering parameters: maxN=0 (DADA2 requires no Ns), truncQ=2 , rm.phix=TRUE and maxEE=2 maxEE parameter sets the maximum number of \u201cexpected errors allowed in a read, which according to the USEARCH authors is a better filter than simply averaging quality scores out <- filterAndTrim(raw_forward, filtered_forward, raw_reverse, filtered_reverse, truncLen=c(240,160), maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE) # to see the read output head(out) DADA2 algorithm depends on a parametric error model and every amplicon dataset has a slightly different error rate learnErrors of DADA2 learns the error model from the data and will help DADA2 to fits its method to your data errors_forward <- learnErrors(filtered_forward, multithread=TRUE) errors_reverse <- learnErrors(filtered_reverse, multithread=TRUE) visualize the estimated error rates plotErrors(errors_forward, nominalQ=TRUE) + theme_minimal() The error rates for each possible transition (eg. A to C, A to G (A2G), \u2026) are shown: 3. Dereplication Dereplication combines identical reads into \"unique sequences\" with a corresponding \"abundance\" (number of reads with that unique sequence) highly reduces computation time by eliminating redundant comparisons derep_forward <- derepFastq(filtered_forward, verbose=TRUE) derep_reverse <- derepFastq(filtered_reverse, verbose=TRUE) # name the derep-class objects by the sample names names(derep_forward) <- sample_names names(derep_reverse) <- sample_names 4. Sample inference applying the core sequence-variant inference algorithm to the dereplicated data dada_forward <- dada(derep_forward, err=errors_forward, multithread=TRUE) dada_reverse <- dada(derep_reverse, err=errors_reverse, multithread=TRUE) # inspect the dada-class object dada_forward[[1]] 5. Merge Paired-end Reads reads are now trimmed, dereplicated and error-corrected, so we merge R1 & R2 together merged_reads <- mergePairs(dada_forward, derep_forward, dada_reverse, derep_reverse, verbose=TRUE) # inspect the merger data.frame from the first sample head(merged_reads[[1]]) 6. Construct Sequence Table constructing a sequence table of our samples a higher-resolution version of the OTU table produced by traditional methods seq_table <- makeSequenceTable(merged_reads) dim(seq_table) # inspect distribution of sequence lengths table(nchar(getSequences(seq_table))) 7. Remove Chimeras this are reads who map to more than one region or contig (so called split-reads) dada removes substitutions and indel errors but chimeras remain We remove the chimeras with: seq_table_nochim <- removeBimeraDenovo(seq_table, method='consensus', multithread=TRUE, verbose=TRUE) dim(seq_table_nochim) # which percentage of our reads did we keep? sum(seq_table_nochim) / sum(seq_table) as a final check of our progress, we\u2019ll look at the number of reads that made it through each step in the pipeline: get_n <- function(x) sum(getUniques(x)) track <- cbind(out, sapply(dada_forward, get_n), sapply(merged_reads, get_n), rowSums(seq_table), rowSums(seq_table_nochim)) colnames(track) <- c('input', 'filtered', 'denoised', 'merged', 'tabled', 'nonchim') rownames(track) <- sample_names head(track) #checking the table we build ##Looks like this: ## input filtered denoised merged tabled nonchim ## F3D0 7793 7113 7113 6600 6600 6588 ## F3D1 5869 5299 5299 5078 5078 5067 8. Assign Taxonomy now we assign taxonomy to our sequences using the SILVA database taxa <- assignTaxonomy(seq_table_nochim, '~/MiSeq_SOP/silva_nr_v128_train_set.fa.gz', multithread=TRUE) taxa <- addSpecies(taxa, '~/MiSeq_SOP/silva_species_assignment_v128.fa.gz') inspecting the classification with: taxa_print <- taxa # removing sequence rownames for display only rownames(taxa_print) <- NULL head(taxa_print) 9. Phylogenetic Tree creating a multiple alignment sequences <- getSequences(seq_table) names(sequences) <- sequences # this propagates to the tip labels of the tree alignment <- AlignSeqs(DNAStringSet(sequences), anchor=NA) build a neighbour-joining tree then fit a maximum likelihood tree using the neighbour-joining tree as a starting point phang_align <- phyDat(as(alignment, 'matrix'), type='DNA') dm <- dist.ml(phang_align) treeNJ <- NJ(dm) # note, tip order != sequence order fit = pml(treeNJ, data=phang_align) #changed negative lengths to 0 fitGTR <- update(fit, k=4, inv=0.2) fitGTR <- optim.pml(fitGTR, model='GTR', optInv=TRUE, optGamma=TRUE, rearrangement = 'stochastic', control = pml.control(trace = 0)) detach('package:phangorn', unload=TRUE) 10. Phyloseq we load now pre-prepared meta data for this tutorial its a normal tab table or data.frame starting with the sequence name sample_data <- read.table('https://hadrieng.github.io/tutorials/data/16S_metadata.txt', header=TRUE, row.names=\"sample_name\") constructing the phyloseq object using our output and the downloaded metadata we remove the mock sample, which is some kind of quality control of the whole process (it consists of 20 samples of known connetions) for more details about the mock sample look at the extensive tutorial physeq <- phyloseq(otu_table(seq_table_nochim, taxa_are_rows=FALSE), sample_data(sample_data), tax_table(taxa), phy_tree(fitGTR$tree)) # remove mock sample physeq <- prune_samples(sample_names(physeq) != 'Mock', physeq) physeq 11. Diversity analysis graphs \u03b1 diversity plot_richness is from phyloseq package . its just a \"wrapper\" so no calculations. We use the Shannon and Fisher wraper. plot_richness(physeq, x='day', measures=c('Shannon', 'Fisher'), color='when') + theme_minimal() \u03b2 diversity 1. Performed a MDS with euclidean distance (mathematically equivalent to a PCA) ord <- ordinate(physeq, 'MDS', 'euclidean') plot_ordination(physeq, ord, type='samples', color='when', title='PCA of the samples from the MiSeq SOP') + theme_minimal() 2. Performed with Bray-Curtis distance ord <- ordinate(physeq, 'NMDS', 'bray') plot_ordination(physeq, ord, type='samples', color='when', title='PCA of the samples from the MiSeq SOP') + theme_minimal() \u03b2 diversity (Bray-Courtis distance) looks like this: Distribution of the most abundant families top20 <- names(sort(taxa_sums(physeq), decreasing=TRUE))[1:20] physeq_top20 <- transform_sample_counts(physeq, function(OTU) OTU/sum(OTU)) physeq_top20 <- prune_taxa(top20, physeq_top20) plot_bar(physeq_top20, x='day', fill='Family') + facet_wrap(~when, scales='free_x') + theme_minimal() looks like this We can place them in a tree bacteroidetes <- subset_taxa(physeq, Phylum %in% c('Bacteroidetes')) plot_tree(bacteroidetes, ladderize='left', size='abundance', color='when', label.tips='Family') Looks like this:","title":"Metabarcoding"},{"location":"metagenome/metabarcoding/#metabarcoding","text":"","title":"Metabarcoding"},{"location":"metagenome/metabarcoding/#overview","text":"very biased part since we only look at one very small gene a rapid method of high-throughput, DNA-based identification of multiple species from a complex and possibly degraded sample of DNA or from mass collection of specimens 16S rRNA regions for genomic classification not ideal, 23S or more gene would be useful, but most reference only exist on 16S platform dependent sequence information (short reads is less information) usually illumina is used because its cheap, but nanopore data or pacbio will be the future standard since the results are way better Preparation steps for metabarcoding using Illumina. Programs to use: + QIIME but its buggy + MOTHUR also buggy + WIMP - what's in my pot nanopore approach, its really good","title":"Overview"},{"location":"metagenome/metabarcoding/#biodiversity","text":"\u03b1 = represents a local habitat/environment/sample can be plotted using the plot_richness function of the phylosec package this is a wrapper featuring 7 plots like \"Shannon\" \u03b2 = is the differences between two \u03b1 (samples) can be plotted using the plot_ordination function of the phylosec package declare the method first ( ord <- ordinate(physeq, 'NMDS', 'bray' ) see below for the different \u03b2 diversity methods \u03b3 = describes the total diversity of an ecosystem or of all gathered samples Illustration of \u03b1, \u03b2, and \u03b3 diversity: Figure: Circles represent \u03b1 diversity as richness of species (symbols) in a sample (local). Dashed box represents \u03b3 diversity (diversity within a set of samples or within a larger region). \u03b2 diversity describes the differences between samples (\u03b2~D~ = similarity; \u03b2~P~ = species richness comparision). This is a more complex description and explained in detail in this paper .","title":"Biodiversity"},{"location":"metagenome/metabarcoding/#diversity-analysis","text":"","title":"\u03b2 diversity analysis"},{"location":"metagenome/metabarcoding/#plotting-your-data-pca-versus-mda","text":"Principal Component Analysis (PCA) or Multiple Discriminant Analysis (MDA) both eliminate an axis (e.g. from 3D to 2D) while maximizing the variance on the x-axis, see example MDA also maximizes the spread of the 2D data more information here log-transform is used to filter off trivial effects, which could dominate our PCA without log normalization with log normalization","title":"Plotting your data: PCA versus MDA"},{"location":"metagenome/metabarcoding/#which-distance-method-to-choose-for-diversity","text":"A overview can be found in this publication . identities only = binary; Abundance included = quantitative All methods destinquish whether they exclude or include joint absences:","title":"Which distance method to choose for \u03b2 diversity?"},{"location":"metagenome/metabarcoding/#metabarcoding-practical","text":"Another detailed tutorial about DADA2 it can be found here we use the DADA2 apporach (very extensive manual and very memory efficient while being fast) is a relatively new method to analyse amplicon data which uses exact variants instead of OTUs","title":"Metabarcoding - practical"},{"location":"metagenome/metabarcoding/#1-install-and-load-packages","text":"install DADA2 and other necessary packages source('https://bioconductor.org/biocLite.R') biocLite('dada2') biocLite('phyloseq') biocLite('DECIPHER') install.packages('ggplot2') install.packages('phangorn') load the packages and verify you have the correct DADA2 version library(dada2) library(ggplot2) library(phyloseq) library(phangorn) library(DECIPHER) packageVersion('dada2') downloaded our data in shell wget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip unzip MiSeqSOPData.zip cd MiSeq_SOP wget https://zenodo.org/record/824551/files/silva_nr_v128_train_set.fa.gz wget https://zenodo.org/record/824551/files/silva_species_assignment_v128.fa.gz Assign in R the path to our data to a variable and check it path <- '~/MiSeq_SOP' list.files(path)","title":"1. Install and Load Packages"},{"location":"metagenome/metabarcoding/#2-filtering-and-trimming-the-reads-in-r-using-dada2","text":"create two lists with the sorted name of the reads: one for forward reads, one for reverse reads raw_forward <- sort(list.files(path, pattern=\"_R1_001.fastq\", full.names=TRUE)) raw_reverse <- sort(list.files(path, pattern=\"_R2_001.fastq\", full.names=TRUE)) # we also need the sample names sample_names <- sapply(strsplit(basename(raw_forward), \"_\"), `[`, 1) visualizing the quality of our reads plotQualityProfile(raw_forward[1:2]) plotQualityProfile(raw_reverse[1:2]) The quality plots (for reverse) are looking like this: really bad quality starting at around 150 bp Dada2 requires us to define the name of our output files first before trimming # place filtered files in filtered/ subdirectory filtered_path <- file.path(path, \"filtered\") filtered_forward <- file.path(filtered_path, paste0(sample_names, \"_R1_trimmed.fastq.gz\")) filtered_reverse <- file.path(filtered_path, paste0(sample_names, \"_R2_trimmed.fastq.gz\")) for filtering parameters: maxN=0 (DADA2 requires no Ns), truncQ=2 , rm.phix=TRUE and maxEE=2 maxEE parameter sets the maximum number of \u201cexpected errors allowed in a read, which according to the USEARCH authors is a better filter than simply averaging quality scores out <- filterAndTrim(raw_forward, filtered_forward, raw_reverse, filtered_reverse, truncLen=c(240,160), maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE) # to see the read output head(out) DADA2 algorithm depends on a parametric error model and every amplicon dataset has a slightly different error rate learnErrors of DADA2 learns the error model from the data and will help DADA2 to fits its method to your data errors_forward <- learnErrors(filtered_forward, multithread=TRUE) errors_reverse <- learnErrors(filtered_reverse, multithread=TRUE) visualize the estimated error rates plotErrors(errors_forward, nominalQ=TRUE) + theme_minimal() The error rates for each possible transition (eg. A to C, A to G (A2G), \u2026) are shown:","title":"2. Filtering and Trimming the Reads in R using DADA2"},{"location":"metagenome/metabarcoding/#3-dereplication","text":"Dereplication combines identical reads into \"unique sequences\" with a corresponding \"abundance\" (number of reads with that unique sequence) highly reduces computation time by eliminating redundant comparisons derep_forward <- derepFastq(filtered_forward, verbose=TRUE) derep_reverse <- derepFastq(filtered_reverse, verbose=TRUE) # name the derep-class objects by the sample names names(derep_forward) <- sample_names names(derep_reverse) <- sample_names","title":"3. Dereplication"},{"location":"metagenome/metabarcoding/#4-sample-inference","text":"applying the core sequence-variant inference algorithm to the dereplicated data dada_forward <- dada(derep_forward, err=errors_forward, multithread=TRUE) dada_reverse <- dada(derep_reverse, err=errors_reverse, multithread=TRUE) # inspect the dada-class object dada_forward[[1]]","title":"4. Sample inference"},{"location":"metagenome/metabarcoding/#5-merge-paired-end-reads","text":"reads are now trimmed, dereplicated and error-corrected, so we merge R1 & R2 together merged_reads <- mergePairs(dada_forward, derep_forward, dada_reverse, derep_reverse, verbose=TRUE) # inspect the merger data.frame from the first sample head(merged_reads[[1]])","title":"5. Merge Paired-end Reads"},{"location":"metagenome/metabarcoding/#6-construct-sequence-table","text":"constructing a sequence table of our samples a higher-resolution version of the OTU table produced by traditional methods seq_table <- makeSequenceTable(merged_reads) dim(seq_table) # inspect distribution of sequence lengths table(nchar(getSequences(seq_table)))","title":"6. Construct Sequence Table"},{"location":"metagenome/metabarcoding/#7-remove-chimeras","text":"this are reads who map to more than one region or contig (so called split-reads) dada removes substitutions and indel errors but chimeras remain We remove the chimeras with: seq_table_nochim <- removeBimeraDenovo(seq_table, method='consensus', multithread=TRUE, verbose=TRUE) dim(seq_table_nochim) # which percentage of our reads did we keep? sum(seq_table_nochim) / sum(seq_table) as a final check of our progress, we\u2019ll look at the number of reads that made it through each step in the pipeline: get_n <- function(x) sum(getUniques(x)) track <- cbind(out, sapply(dada_forward, get_n), sapply(merged_reads, get_n), rowSums(seq_table), rowSums(seq_table_nochim)) colnames(track) <- c('input', 'filtered', 'denoised', 'merged', 'tabled', 'nonchim') rownames(track) <- sample_names head(track) #checking the table we build ##Looks like this: ## input filtered denoised merged tabled nonchim ## F3D0 7793 7113 7113 6600 6600 6588 ## F3D1 5869 5299 5299 5078 5078 5067","title":"7. Remove Chimeras"},{"location":"metagenome/metabarcoding/#8-assign-taxonomy","text":"now we assign taxonomy to our sequences using the SILVA database taxa <- assignTaxonomy(seq_table_nochim, '~/MiSeq_SOP/silva_nr_v128_train_set.fa.gz', multithread=TRUE) taxa <- addSpecies(taxa, '~/MiSeq_SOP/silva_species_assignment_v128.fa.gz') inspecting the classification with: taxa_print <- taxa # removing sequence rownames for display only rownames(taxa_print) <- NULL head(taxa_print)","title":"8. Assign Taxonomy"},{"location":"metagenome/metabarcoding/#9-phylogenetic-tree","text":"creating a multiple alignment sequences <- getSequences(seq_table) names(sequences) <- sequences # this propagates to the tip labels of the tree alignment <- AlignSeqs(DNAStringSet(sequences), anchor=NA) build a neighbour-joining tree then fit a maximum likelihood tree using the neighbour-joining tree as a starting point phang_align <- phyDat(as(alignment, 'matrix'), type='DNA') dm <- dist.ml(phang_align) treeNJ <- NJ(dm) # note, tip order != sequence order fit = pml(treeNJ, data=phang_align) #changed negative lengths to 0 fitGTR <- update(fit, k=4, inv=0.2) fitGTR <- optim.pml(fitGTR, model='GTR', optInv=TRUE, optGamma=TRUE, rearrangement = 'stochastic', control = pml.control(trace = 0)) detach('package:phangorn', unload=TRUE)","title":"9. Phylogenetic Tree"},{"location":"metagenome/metabarcoding/#10-phyloseq","text":"we load now pre-prepared meta data for this tutorial its a normal tab table or data.frame starting with the sequence name sample_data <- read.table('https://hadrieng.github.io/tutorials/data/16S_metadata.txt', header=TRUE, row.names=\"sample_name\") constructing the phyloseq object using our output and the downloaded metadata we remove the mock sample, which is some kind of quality control of the whole process (it consists of 20 samples of known connetions) for more details about the mock sample look at the extensive tutorial physeq <- phyloseq(otu_table(seq_table_nochim, taxa_are_rows=FALSE), sample_data(sample_data), tax_table(taxa), phy_tree(fitGTR$tree)) # remove mock sample physeq <- prune_samples(sample_names(physeq) != 'Mock', physeq) physeq","title":"10. Phyloseq"},{"location":"metagenome/metabarcoding/#11-diversity-analysis-graphs","text":"","title":"11. Diversity analysis graphs"},{"location":"metagenome/metabarcoding/#diversity","text":"plot_richness is from phyloseq package . its just a \"wrapper\" so no calculations. We use the Shannon and Fisher wraper. plot_richness(physeq, x='day', measures=c('Shannon', 'Fisher'), color='when') + theme_minimal()","title":"\u03b1 diversity"},{"location":"metagenome/metabarcoding/#diversity_1","text":"1. Performed a MDS with euclidean distance (mathematically equivalent to a PCA) ord <- ordinate(physeq, 'MDS', 'euclidean') plot_ordination(physeq, ord, type='samples', color='when', title='PCA of the samples from the MiSeq SOP') + theme_minimal() 2. Performed with Bray-Curtis distance ord <- ordinate(physeq, 'NMDS', 'bray') plot_ordination(physeq, ord, type='samples', color='when', title='PCA of the samples from the MiSeq SOP') + theme_minimal() \u03b2 diversity (Bray-Courtis distance) looks like this: Distribution of the most abundant families top20 <- names(sort(taxa_sums(physeq), decreasing=TRUE))[1:20] physeq_top20 <- transform_sample_counts(physeq, function(OTU) OTU/sum(OTU)) physeq_top20 <- prune_taxa(top20, physeq_top20) plot_bar(physeq_top20, x='day', fill='Family') + facet_wrap(~when, scales='free_x') + theme_minimal() looks like this We can place them in a tree bacteroidetes <- subset_taxa(physeq, Phylum %in% c('Bacteroidetes')) plot_tree(bacteroidetes, ladderize='left', size='abundance', color='when', label.tips='Family') Looks like this:","title":"\u03b2 diversity"},{"location":"metagenome/metagenomics/","text":"Metagenomics Overview & Terminology collected genomic information of a sample unbiased targeting (no special marker like metabarcoding) Metabarcoding (16S) is used for large number of samples, i.e., multiple patients, longitudinal studies, etc. but offers limited taxonomical and functional resolution in comparision Whole Metagenome sequencing (WMS) , or shotgun metagenome sequencing, provides insight into community biodiversity and function in contrast to Metabarcoding, where only a specific region of the bacterial community (the 16s rRNA) is sequenced WMS aims at sequencing all the genomic material present in the environment WMS is more expensive but offers increased resolution, and allows the discovery of viruses as well as other mobile genetic elements Microbiota organism within a set environment mainly focused on bacteria * exluding viruses & mobile elements Microbiome includes all factors of an environment (organism, genetic elements, metabolites, proteins) Holobiont assemblages of different species that form a ecological unit divided into: virome, microbiome and macrobiological hosts Whole Metagenome Sequencing - practical comparing samples from the Pig Gut Microbiome against the Human Gut Microbiome Software used: - FastQC - Kraken - R - Pavian Data used: curl -O -J -L https://osf.io/h9x6e/download tar xvf subset_wms.tar.gz we ssh using additionally the -X flag to allow a graphical interface we used fastqc do check the quality of our data 1. Taxonomic read assignment with Kraken first we need the Kraken database #create a databases directory and download the database wget https://ccb.jhu.edu/software/kraken/dl/minikraken_20171019_4GB.tgz tar xzf minikraken_20171019_4GB.tgz running kraken on the reads produces a tab-delimited file with an assigned TaxID for each read # for the script you need to declare the KRAKEN_DB path KRAKEN_DB=\"/mnt/databases/minikraken_20171013_4GB\" for i in *_1.fastq do prefix=$(basename $i _1.fastq) # print which sample is being processed echo $prefix kraken --db $KRAKEN_DB --threads 2 --fastq-input ${prefix}_1_corr.fastq ${prefix}_2_corr.fastq > /home/student/wms/results/${prefix}.tab kraken-report --db $KRAKEN_DB /home/student/wms/results/${prefix}.tab > /home/student/wms/results/${prefix}_tax.txt done 2. Visualization with Pavian in R Installing and run Pavian in R with those commands: #one time only installations and setup options(repos = c(CRAN = \"http://cran.rstudio.com\")) if (!require(remotes)) { install.packages(\"remotes\") } remotes::install_github(\"fbreitwieser/pavian\") #Start up command pavian::runApp(port=5000) creates a new R-tab were you can open the kraken files creates a nice sankey chart of your organism within your sample its scalable, allows comparision between samples (as a table) and indepth analysis of samples (see figure) Example results in one sample as a sankey chart using pavian KRONA chart is also for visualization of metagenomic data good for explanation of your sample but PAVIAN is way better Metagenome assembly and binning - practical AIM: inspect and assemble metagenomic data and retrieve draft genomes using a mock community of 20 bacteria simulating Illumina HiSeq created with InSilicoSeq selected from the Tara Ocean study that recovered 957 distinct Metagenome-assembled-genomes (or MAGs) that were previsouly unknown 1. Correction and assembly of the illumina reads check the fastq data with fastqc do sickle and scythe as needed; see HTS assembly here with megahit sickle pe -f tara_reads_R1.fastq.gz -r tara_reads_R2.fastq.gz -t sanger -o tara_trimmed_R1.fastq -p tara_trimmed_R2.fastq -s /dev/null megahit -1 tara_trimmed_R1.fastq -2 tara_trimmed_R2.fastq -o tara_assembly 2. Binning First map the reads back against the assembly (for coverage information) bowtie2 is a ultra fast short read mapper/aligner ln -s tara_assembly/final.contigs.fa . #link contigs to current folder bowtie2-build final.contigs.fa final.contigs #index bowtie2 -x final.contigs -1 tara_trimmed_R1.fastq -2 tara_trimmed_R2.fastq | samtools view -bS -o tara_to_sort.bam #alignment and bam conversion samtools sort tara_to_sort.bam -o tara.bam samtools index tara.bam then we run metabat runMetaBat.sh -m 1500 final.contigs.fa tara.bam mv final.contigs.fa.metabat-bins1500 metabat 3. QC of the bins first time you run checkm you have to create the database sudo checkm data setRoot ~/.local/data/checkm checkm lineage_wf -x fa metabat checkm/ checkm bin_qa_plot -x fa checkm metabat plots 4. Get organism information and plot it #tells you which organism, saves under checkm/lineage needs folder checkm checkm qa checkm/lineage.ms checkm #plots its checkm bin_qa_plot -x fa checkm metabat plots you get something like this, which should represent 10 different species (theoretically): now you can annotate each bin (fasta files are in metabat dir) more informations can be found here and here 5. Barrnap BA sic R apid R ibosomal RNA P redictor isnt tsuitable for novel stuff since it has to be in the database, but seems still to work (see figure above of unknown marine bacteria) you can download it here","title":"Metagenomics"},{"location":"metagenome/metagenomics/#metagenomics","text":"","title":"Metagenomics"},{"location":"metagenome/metagenomics/#overview-terminology","text":"collected genomic information of a sample unbiased targeting (no special marker like metabarcoding) Metabarcoding (16S) is used for large number of samples, i.e., multiple patients, longitudinal studies, etc. but offers limited taxonomical and functional resolution in comparision Whole Metagenome sequencing (WMS) , or shotgun metagenome sequencing, provides insight into community biodiversity and function in contrast to Metabarcoding, where only a specific region of the bacterial community (the 16s rRNA) is sequenced WMS aims at sequencing all the genomic material present in the environment WMS is more expensive but offers increased resolution, and allows the discovery of viruses as well as other mobile genetic elements Microbiota organism within a set environment mainly focused on bacteria * exluding viruses & mobile elements Microbiome includes all factors of an environment (organism, genetic elements, metabolites, proteins) Holobiont assemblages of different species that form a ecological unit divided into: virome, microbiome and macrobiological hosts","title":"Overview &amp; Terminology"},{"location":"metagenome/metagenomics/#whole-metagenome-sequencing-practical","text":"comparing samples from the Pig Gut Microbiome against the Human Gut Microbiome Software used: - FastQC - Kraken - R - Pavian Data used: curl -O -J -L https://osf.io/h9x6e/download tar xvf subset_wms.tar.gz we ssh using additionally the -X flag to allow a graphical interface we used fastqc do check the quality of our data","title":"Whole Metagenome Sequencing - practical"},{"location":"metagenome/metagenomics/#1-taxonomic-read-assignment-with-kraken","text":"first we need the Kraken database #create a databases directory and download the database wget https://ccb.jhu.edu/software/kraken/dl/minikraken_20171019_4GB.tgz tar xzf minikraken_20171019_4GB.tgz running kraken on the reads produces a tab-delimited file with an assigned TaxID for each read # for the script you need to declare the KRAKEN_DB path KRAKEN_DB=\"/mnt/databases/minikraken_20171013_4GB\" for i in *_1.fastq do prefix=$(basename $i _1.fastq) # print which sample is being processed echo $prefix kraken --db $KRAKEN_DB --threads 2 --fastq-input ${prefix}_1_corr.fastq ${prefix}_2_corr.fastq > /home/student/wms/results/${prefix}.tab kraken-report --db $KRAKEN_DB /home/student/wms/results/${prefix}.tab > /home/student/wms/results/${prefix}_tax.txt done","title":"1. Taxonomic read assignment with Kraken"},{"location":"metagenome/metagenomics/#2-visualization-with-pavian-in-r","text":"Installing and run Pavian in R with those commands: #one time only installations and setup options(repos = c(CRAN = \"http://cran.rstudio.com\")) if (!require(remotes)) { install.packages(\"remotes\") } remotes::install_github(\"fbreitwieser/pavian\") #Start up command pavian::runApp(port=5000) creates a new R-tab were you can open the kraken files creates a nice sankey chart of your organism within your sample its scalable, allows comparision between samples (as a table) and indepth analysis of samples (see figure) Example results in one sample as a sankey chart using pavian KRONA chart is also for visualization of metagenomic data good for explanation of your sample but PAVIAN is way better","title":"2. Visualization with Pavian in R"},{"location":"metagenome/metagenomics/#metagenome-assembly-and-binning-practical","text":"AIM: inspect and assemble metagenomic data and retrieve draft genomes using a mock community of 20 bacteria simulating Illumina HiSeq created with InSilicoSeq selected from the Tara Ocean study that recovered 957 distinct Metagenome-assembled-genomes (or MAGs) that were previsouly unknown","title":"Metagenome assembly and binning - practical"},{"location":"metagenome/metagenomics/#1-correction-and-assembly-of-the-illumina-reads","text":"check the fastq data with fastqc do sickle and scythe as needed; see HTS assembly here with megahit sickle pe -f tara_reads_R1.fastq.gz -r tara_reads_R2.fastq.gz -t sanger -o tara_trimmed_R1.fastq -p tara_trimmed_R2.fastq -s /dev/null megahit -1 tara_trimmed_R1.fastq -2 tara_trimmed_R2.fastq -o tara_assembly","title":"1. Correction and assembly of the illumina reads"},{"location":"metagenome/metagenomics/#2-binning","text":"First map the reads back against the assembly (for coverage information) bowtie2 is a ultra fast short read mapper/aligner ln -s tara_assembly/final.contigs.fa . #link contigs to current folder bowtie2-build final.contigs.fa final.contigs #index bowtie2 -x final.contigs -1 tara_trimmed_R1.fastq -2 tara_trimmed_R2.fastq | samtools view -bS -o tara_to_sort.bam #alignment and bam conversion samtools sort tara_to_sort.bam -o tara.bam samtools index tara.bam then we run metabat runMetaBat.sh -m 1500 final.contigs.fa tara.bam mv final.contigs.fa.metabat-bins1500 metabat","title":"2. Binning"},{"location":"metagenome/metagenomics/#3-qc-of-the-bins","text":"first time you run checkm you have to create the database sudo checkm data setRoot ~/.local/data/checkm checkm lineage_wf -x fa metabat checkm/ checkm bin_qa_plot -x fa checkm metabat plots","title":"3. QC of the bins"},{"location":"metagenome/metagenomics/#4-get-organism-information-and-plot-it","text":"#tells you which organism, saves under checkm/lineage needs folder checkm checkm qa checkm/lineage.ms checkm #plots its checkm bin_qa_plot -x fa checkm metabat plots you get something like this, which should represent 10 different species (theoretically): now you can annotate each bin (fasta files are in metabat dir) more informations can be found here and here","title":"4. Get organism information and plot it"},{"location":"metagenome/metagenomics/#5-barrnap","text":"BA sic R apid R ibosomal RNA P redictor isnt tsuitable for novel stuff since it has to be in the database, but seems still to work (see figure above of unknown marine bacteria) you can download it here","title":"5. Barrnap"},{"location":"other_topics/RNA_seq/","text":"RNAseq Overview identifying novel transcripts and splicing variants Measuring gene expression (cellular identity, expression in response to stimuli, differences between health/sick, correlation to diseases, etc.) microRNA's, RNA editing low background noise, high technical reproducibility Typical RNAseq workflow Questions: Quantification : Expression rates Splicing variants : exon arrangements Novel transcripts RNA editing : single nucleotide variations (SNV) Practical: 1. poly-A RNA extraction (mRNA extraction Kit) 2. reverse transcription into c-DNA 3. shearing (for illumina, not for nanopore) 4. library prep (depends on which seq. technique) 5. sequencing and getting the reads (fastq) Bioinformatics: 1. Preprocessing fastq files using: Trimmomatic, cutadapt, trimgalore 1. adapter removal 2. trimming of bad quality 2. Mapping 1. no genome: de novo transcriptom construction 2. known genome: align reads against it 1. Quantification : accept only known/described transcripts 2. Splicing variants : allow alternate arrangements of known transcripts 3. Novel transcripts : allow alignment to exons and other regions 3. Mapping QC with RNA-SeQC , discard reads if: 1. cannot be uniquely mapped 2. alignment overlaps with several genes 3. bad alignment quality score 4. paired-end reads (illumina) do not map to the same genes 4. Expression Quantification (normalization of read counts) 1. correct read length bias (longer genes produce more reads) 2. correct Batch effects (labs and preparation time - different coverage between samples) 3. correct technical biases (use of different machines) Types of normalization You need to know: How was RNA extracted and the library build Which Sequencing platform single or basepaired ends? How many reads per sample? * QC reports? Normalization for: library size , gene length , RNA composition (comparison between samples) * Methods: * RPKM/FPKM (size and length) * not recommended if you compare between different samples * R eads p er k ilo base per m illion mapped reads (RPKM) * use only to report expression values NOT differential expression * TPM (size and length) * similar to RPKM but uses t ranscripts instead of reads * the presenter doesn't use this method * TMM and DESeq (size and composition) * TMM considers the tissue type expression variation * suitable for differential expression between sample groups Data visualization is done with MA-plot or vulcano plot (check the tutorial afterwards) Figure: Black is not significant, red are significant changes heatmaps is the most common representation for differential expression analysis (R/Bioconducter heatmap and ggplot package) Another way to see how samples cluster is with PCA plots Function enrichment analysis: GOSeq/topGO/GAGE (R package) DAVID Differential Expression - practical For this tutorial we used the test data from this paper . This is a highly comprehensive tutorial paper for RNAseq. Summary: We have two two commercially available RNA samples : Universal Human Reference (UHR) 10 cancer cell lines Human Brain Reference (HBR) isolated from brains of 23 Caucasians (m & f) mostly 60-80 years old a spike-in control was added to each sample (ERCC ExFold RNA). Spike-in consists of 92 transcripts that are present in known concentrations across a wide abundance range (from very few copies to many copies). This allows to test the degree to which the RNA-seq assay (including all laboratory and analysis steps) accurately reflects the relative abundance of transcript species within a sample two 'mixes' of spike-in control to allow assessment of differential expression output between samples 3 complete experimental replicates for each sample to assess the technical variability of our overall process of producing RNA-seq data in the lab UHR + ERCC Spike-In Mix1, Replicate 1 UHR + ERCC Spike-In Mix1, Replicate 2 UHR + ERCC Spike-In Mix1, Replicate 3 HBR + ERCC Spike-In Mix2, Replicate 1 HBR + ERCC Spike-In Mix2, Replicate 2 HBR + ERCC Spike-In Mix2, Replicate 3 For technical details, like Kits, rRNA removal, sample concentrations etc. read the S2 Data file from the paper. Data available at: curl -O -J -L https://osf.io/7zepj/download tar xzf toy_rna.tar.gz salmon for indexing and quantification of reads Salmon produces highly-accurate, transcript-level quantification estimates from RNA-seq data (you need another - splice aware program - if you are interested in new transcripts or splice sides. Quantify reads with salmon First Indexing: salmon index -t chr22_transcripts.fa -i chr22_index Now quantifying with this loop: simply goes through each sample and invokes salmon using basic options: -i index file from the command before --libType A tells salmon that it should automatically determine the library type of the sequencing reads (e.g. stranded vs. not stranded etc.) the -1 and -2 arguments tell salmon where to find the left and right reads for this sample (salmon accepts gzipped FASTQ files) -o output for i in *_R1.fastq.gz do prefix=$(basename $i _R1.fastq.gz) salmon quant -i chr22_index --libType A \\ -1 ${prefix}_R1.fastq.gz -2 ${prefix}_R2.fastq.gz -o quant/${prefix}; done in the output folder (quant folder) is the quant.sf file we use for Rstudio. R-studio analysis In R-studio set your working directory to the data (chr22_genes.gtf is visible) load the necessary modules library(tximport) library(GenomicFeatures) library(readr) Salmon did the quantification of the transcript level. We want to see which genes are differentially expressed, so we need to link the transcript names to the gene names. We can use our .gtf annotation for that, and the GenomicFeatures package: txdb <- makeTxDbFromGFF(\"chr22_genes.gtf\") k <- keys(txdb, keytype = \"TXNAME\") tx2gene <- select(txdb, keys = k, keytype = \"TXNAME\", columns = \"GENEID\") Now we can import the salmon quantification. # we imported a file were each sample name is stored under the column \"sample\" samples <- read.table(\"samples.txt\", header = TRUE) # we save the path to each quant.sf file using the name of each sample in the samples file files <- file.path(\"quant\", samples$sample, \"quant.sf\") # we renamed the columns name of each path we extracted names(files) <- paste0(samples$sample) # imports the quant files, txi.salmon is a datalist e.g. salmon$counts for the counts list-entry txi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene) Differential expression using DESeq2 + DESeqDataSet is a subclass of RangedSummarizedExperiment , used to store the input values, intermediate calculations and results of an analysis of differential expression . library(DESeq2) #loading module Instantiate the DESeqDataSet and generate result table. See ?DESeqDataSetFromTximport and ?DESeq for more information. dds <- DESeqDataSetFromTximport(txi.salmon, samples, ~condition) dds <- DESeq(dds) res <- results(dds) #summary(res) to see the results DESeq uses a negative binomial distribution. Such distributions have two parameters: mean and dispersion . So we can do a dispersion plot with the dispersion data: plotDispEsts(dds, main=\"Dispersion plot\") Explanations about dispersion and DESeq2 can be found in this very good tutorial here . Figure: The red line in the figure plots the estimate for the expected dispersion value for genes of a given expression strength . Each black dot is a gene with an associated mean expression level and maximum likelihood estimation (MLE) of the dispersion. Heatmap * For clustering and heatmaps, we need to log transform our data: rld <- rlogTransformation(dds) #log of data from \"dds\" #loading librarys for plot library(RColorBrewer) library(gplots) parameters of the heatmap: (mycols <- brewer.pal(8, \"Dark2\")[1:length(unique(samples$condition))]) sampleDists <- as.matrix(dist(t(assay(rld)))) heatmap.2(as.matrix(sampleDists), key=F, trace=\"none\", col=colorpanel(100, \"black\", \"white\"), ColSideColors=mycols[samples$condition], RowSideColors=mycols[samples$condition], margin=c(10, 10), main=\"Sample Distance Matrix\") Figure: Differential Expression Analysis: Comparison between each samples as a heatmap, black is identical, white not identical PCA Plot DESeq2::plotPCA(rld, intgroup=\"condition\") MA plot and the Volcano Plot we create, merge and sort data, so its readable for the plots table(res$padj<0.05) #extract data out of res res <- res[order(res$padj), ] #sorting the res variable #merging data so its ready for plots resdata <- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=\"row.names\", sort=FALSE) names(resdata)[1] <- \"Gene\" You can take a look at the p-values we sorted in the variable res , using these plots: hist(res$pvalue, breaks=50, col=\"grey\") #a hist plot DESeq2::plotMA(dds, ylim=c(-1,1), cex=1) #an MA plot now we do the actual vulcano plot # Volcano plot with(res, plot(log2FoldChange, -log10(pvalue), pch=20, main=\"Volcano plot\", xlim=c(-2.5,2))) # second one highlighting certain points with(subset(res, padj<.05 ), points(log2FoldChange, -log10(pvalue), pch=20, col=\"red\")) KEGG pathway analysis KEGG PATHWAY is a collection of manually drawn pathway maps representing our knowledge on the molecular interaction, reaction and relation networks for: * We need these library's: library(AnnotationDbi) library(org.Hs.eg.db) library(pathview) library(gage) library(gageData) Using the mapIds function to add more columns to the results the row.names of our results table has the Ensembl gene ID (our key), so we need to specify keytype=ENSEMBL the column argument tells the mapIds function which information we want the multiVals argument tells the function what to do if there are multiple possible values for a single input value (we take only the first entry if we have more than two) res$symbol <- mapIds(org.Hs.eg.db, keys=row.names(res), column=\"SYMBOL\", keytype=\"ENSEMBL\", multiVals=\"first\") res$entrez <- mapIds(org.Hs.eg.db, keys=row.names(res), column=\"ENTREZID\", keytype=\"ENSEMBL\", multiVals=\"first\") res$name <- mapIds(org.Hs.eg.db, keys=row.names(res), column=\"GENENAME\", keytype=\"ENSEMBL\", multiVals=\"first\") We\u2019re going to use the gage package for pathway analysis, and the pathview package to draw a pathway diagram. The gageData package has pre-compiled databases mapping genes to KEGG pathways and GO terms for common organisms: data(kegg.sets.hs) data(sigmet.idx.hs) kegg.sets.hs <- kegg.sets.hs[sigmet.idx.hs] head(kegg.sets.hs, 3) Run the pathway analysis foldchanges <- res$log2FoldChange names(foldchanges) <- res$entrez keggres <- gage(foldchanges, gsets=kegg.sets.hs, same.dir=TRUE) lapply(keggres, head) Now we need to identify which pathway we have and whats the ID of it: library(dplyr) # Get the pathways keggrespathways <- data.frame(id=rownames(keggres$greater), keggres$greater) %>% tbl_df() %>% filter(row_number()<=5) %>% .$id %>% as.character() keggrespathways # Get the IDs. keggresids <- substr(keggrespathways, start=1, stop=8) Finally, the pathview() function in the pathview package makes the plots. Let\u2019s write a function so we can loop through and draw plots for the top 5 pathways we created above. # Define plotting function for applying later plot_pathway <- function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\", new.signature=FALSE) # Unload dplyr since it conflicts with the next line detach(\"package:dplyr\", unload=T) # plot multiple pathways (plots saved to disk andreturns a throwaway list object) tmp <- sapply(keggresids, function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\")) The results are shown here (but only for 2 pathways and only the KEGG output): Another tutorial about this pathway stuff can be found here .","title":"RNA seq"},{"location":"other_topics/RNA_seq/#rnaseq","text":"","title":"RNAseq"},{"location":"other_topics/RNA_seq/#overview","text":"identifying novel transcripts and splicing variants Measuring gene expression (cellular identity, expression in response to stimuli, differences between health/sick, correlation to diseases, etc.) microRNA's, RNA editing low background noise, high technical reproducibility","title":"Overview"},{"location":"other_topics/RNA_seq/#typical-rnaseq-workflow","text":"Questions: Quantification : Expression rates Splicing variants : exon arrangements Novel transcripts RNA editing : single nucleotide variations (SNV) Practical: 1. poly-A RNA extraction (mRNA extraction Kit) 2. reverse transcription into c-DNA 3. shearing (for illumina, not for nanopore) 4. library prep (depends on which seq. technique) 5. sequencing and getting the reads (fastq) Bioinformatics: 1. Preprocessing fastq files using: Trimmomatic, cutadapt, trimgalore 1. adapter removal 2. trimming of bad quality 2. Mapping 1. no genome: de novo transcriptom construction 2. known genome: align reads against it 1. Quantification : accept only known/described transcripts 2. Splicing variants : allow alternate arrangements of known transcripts 3. Novel transcripts : allow alignment to exons and other regions 3. Mapping QC with RNA-SeQC , discard reads if: 1. cannot be uniquely mapped 2. alignment overlaps with several genes 3. bad alignment quality score 4. paired-end reads (illumina) do not map to the same genes 4. Expression Quantification (normalization of read counts) 1. correct read length bias (longer genes produce more reads) 2. correct Batch effects (labs and preparation time - different coverage between samples) 3. correct technical biases (use of different machines)","title":"Typical RNAseq workflow"},{"location":"other_topics/RNA_seq/#types-of-normalization","text":"You need to know: How was RNA extracted and the library build Which Sequencing platform single or basepaired ends? How many reads per sample? * QC reports? Normalization for: library size , gene length , RNA composition (comparison between samples) * Methods: * RPKM/FPKM (size and length) * not recommended if you compare between different samples * R eads p er k ilo base per m illion mapped reads (RPKM) * use only to report expression values NOT differential expression * TPM (size and length) * similar to RPKM but uses t ranscripts instead of reads * the presenter doesn't use this method * TMM and DESeq (size and composition) * TMM considers the tissue type expression variation * suitable for differential expression between sample groups Data visualization is done with MA-plot or vulcano plot (check the tutorial afterwards) Figure: Black is not significant, red are significant changes heatmaps is the most common representation for differential expression analysis (R/Bioconducter heatmap and ggplot package) Another way to see how samples cluster is with PCA plots Function enrichment analysis: GOSeq/topGO/GAGE (R package) DAVID","title":"Types of normalization"},{"location":"other_topics/RNA_seq/#differential-expression-practical","text":"For this tutorial we used the test data from this paper . This is a highly comprehensive tutorial paper for RNAseq. Summary: We have two two commercially available RNA samples : Universal Human Reference (UHR) 10 cancer cell lines Human Brain Reference (HBR) isolated from brains of 23 Caucasians (m & f) mostly 60-80 years old a spike-in control was added to each sample (ERCC ExFold RNA). Spike-in consists of 92 transcripts that are present in known concentrations across a wide abundance range (from very few copies to many copies). This allows to test the degree to which the RNA-seq assay (including all laboratory and analysis steps) accurately reflects the relative abundance of transcript species within a sample two 'mixes' of spike-in control to allow assessment of differential expression output between samples 3 complete experimental replicates for each sample to assess the technical variability of our overall process of producing RNA-seq data in the lab UHR + ERCC Spike-In Mix1, Replicate 1 UHR + ERCC Spike-In Mix1, Replicate 2 UHR + ERCC Spike-In Mix1, Replicate 3 HBR + ERCC Spike-In Mix2, Replicate 1 HBR + ERCC Spike-In Mix2, Replicate 2 HBR + ERCC Spike-In Mix2, Replicate 3 For technical details, like Kits, rRNA removal, sample concentrations etc. read the S2 Data file from the paper. Data available at: curl -O -J -L https://osf.io/7zepj/download tar xzf toy_rna.tar.gz","title":"Differential Expression - practical"},{"location":"other_topics/RNA_seq/#salmon-for-indexing-and-quantification-of-reads","text":"Salmon produces highly-accurate, transcript-level quantification estimates from RNA-seq data (you need another - splice aware program - if you are interested in new transcripts or splice sides. Quantify reads with salmon First Indexing: salmon index -t chr22_transcripts.fa -i chr22_index Now quantifying with this loop: simply goes through each sample and invokes salmon using basic options: -i index file from the command before --libType A tells salmon that it should automatically determine the library type of the sequencing reads (e.g. stranded vs. not stranded etc.) the -1 and -2 arguments tell salmon where to find the left and right reads for this sample (salmon accepts gzipped FASTQ files) -o output for i in *_R1.fastq.gz do prefix=$(basename $i _R1.fastq.gz) salmon quant -i chr22_index --libType A \\ -1 ${prefix}_R1.fastq.gz -2 ${prefix}_R2.fastq.gz -o quant/${prefix}; done in the output folder (quant folder) is the quant.sf file we use for Rstudio.","title":"salmon for indexing and quantification of reads"},{"location":"other_topics/RNA_seq/#r-studio-analysis","text":"In R-studio set your working directory to the data (chr22_genes.gtf is visible) load the necessary modules library(tximport) library(GenomicFeatures) library(readr) Salmon did the quantification of the transcript level. We want to see which genes are differentially expressed, so we need to link the transcript names to the gene names. We can use our .gtf annotation for that, and the GenomicFeatures package: txdb <- makeTxDbFromGFF(\"chr22_genes.gtf\") k <- keys(txdb, keytype = \"TXNAME\") tx2gene <- select(txdb, keys = k, keytype = \"TXNAME\", columns = \"GENEID\") Now we can import the salmon quantification. # we imported a file were each sample name is stored under the column \"sample\" samples <- read.table(\"samples.txt\", header = TRUE) # we save the path to each quant.sf file using the name of each sample in the samples file files <- file.path(\"quant\", samples$sample, \"quant.sf\") # we renamed the columns name of each path we extracted names(files) <- paste0(samples$sample) # imports the quant files, txi.salmon is a datalist e.g. salmon$counts for the counts list-entry txi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene) Differential expression using DESeq2 + DESeqDataSet is a subclass of RangedSummarizedExperiment , used to store the input values, intermediate calculations and results of an analysis of differential expression . library(DESeq2) #loading module Instantiate the DESeqDataSet and generate result table. See ?DESeqDataSetFromTximport and ?DESeq for more information. dds <- DESeqDataSetFromTximport(txi.salmon, samples, ~condition) dds <- DESeq(dds) res <- results(dds) #summary(res) to see the results DESeq uses a negative binomial distribution. Such distributions have two parameters: mean and dispersion . So we can do a dispersion plot with the dispersion data: plotDispEsts(dds, main=\"Dispersion plot\") Explanations about dispersion and DESeq2 can be found in this very good tutorial here . Figure: The red line in the figure plots the estimate for the expected dispersion value for genes of a given expression strength . Each black dot is a gene with an associated mean expression level and maximum likelihood estimation (MLE) of the dispersion. Heatmap * For clustering and heatmaps, we need to log transform our data: rld <- rlogTransformation(dds) #log of data from \"dds\" #loading librarys for plot library(RColorBrewer) library(gplots) parameters of the heatmap: (mycols <- brewer.pal(8, \"Dark2\")[1:length(unique(samples$condition))]) sampleDists <- as.matrix(dist(t(assay(rld)))) heatmap.2(as.matrix(sampleDists), key=F, trace=\"none\", col=colorpanel(100, \"black\", \"white\"), ColSideColors=mycols[samples$condition], RowSideColors=mycols[samples$condition], margin=c(10, 10), main=\"Sample Distance Matrix\") Figure: Differential Expression Analysis: Comparison between each samples as a heatmap, black is identical, white not identical PCA Plot DESeq2::plotPCA(rld, intgroup=\"condition\") MA plot and the Volcano Plot we create, merge and sort data, so its readable for the plots table(res$padj<0.05) #extract data out of res res <- res[order(res$padj), ] #sorting the res variable #merging data so its ready for plots resdata <- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=\"row.names\", sort=FALSE) names(resdata)[1] <- \"Gene\" You can take a look at the p-values we sorted in the variable res , using these plots: hist(res$pvalue, breaks=50, col=\"grey\") #a hist plot DESeq2::plotMA(dds, ylim=c(-1,1), cex=1) #an MA plot now we do the actual vulcano plot # Volcano plot with(res, plot(log2FoldChange, -log10(pvalue), pch=20, main=\"Volcano plot\", xlim=c(-2.5,2))) # second one highlighting certain points with(subset(res, padj<.05 ), points(log2FoldChange, -log10(pvalue), pch=20, col=\"red\"))","title":"R-studio analysis"},{"location":"other_topics/RNA_seq/#kegg-pathway-analysis","text":"KEGG PATHWAY is a collection of manually drawn pathway maps representing our knowledge on the molecular interaction, reaction and relation networks for: * We need these library's: library(AnnotationDbi) library(org.Hs.eg.db) library(pathview) library(gage) library(gageData) Using the mapIds function to add more columns to the results the row.names of our results table has the Ensembl gene ID (our key), so we need to specify keytype=ENSEMBL the column argument tells the mapIds function which information we want the multiVals argument tells the function what to do if there are multiple possible values for a single input value (we take only the first entry if we have more than two) res$symbol <- mapIds(org.Hs.eg.db, keys=row.names(res), column=\"SYMBOL\", keytype=\"ENSEMBL\", multiVals=\"first\") res$entrez <- mapIds(org.Hs.eg.db, keys=row.names(res), column=\"ENTREZID\", keytype=\"ENSEMBL\", multiVals=\"first\") res$name <- mapIds(org.Hs.eg.db, keys=row.names(res), column=\"GENENAME\", keytype=\"ENSEMBL\", multiVals=\"first\") We\u2019re going to use the gage package for pathway analysis, and the pathview package to draw a pathway diagram. The gageData package has pre-compiled databases mapping genes to KEGG pathways and GO terms for common organisms: data(kegg.sets.hs) data(sigmet.idx.hs) kegg.sets.hs <- kegg.sets.hs[sigmet.idx.hs] head(kegg.sets.hs, 3) Run the pathway analysis foldchanges <- res$log2FoldChange names(foldchanges) <- res$entrez keggres <- gage(foldchanges, gsets=kegg.sets.hs, same.dir=TRUE) lapply(keggres, head) Now we need to identify which pathway we have and whats the ID of it: library(dplyr) # Get the pathways keggrespathways <- data.frame(id=rownames(keggres$greater), keggres$greater) %>% tbl_df() %>% filter(row_number()<=5) %>% .$id %>% as.character() keggrespathways # Get the IDs. keggresids <- substr(keggrespathways, start=1, stop=8) Finally, the pathview() function in the pathview package makes the plots. Let\u2019s write a function so we can loop through and draw plots for the top 5 pathways we created above. # Define plotting function for applying later plot_pathway <- function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\", new.signature=FALSE) # Unload dplyr since it conflicts with the next line detach(\"package:dplyr\", unload=T) # plot multiple pathways (plots saved to disk andreturns a throwaway list object) tmp <- sapply(keggresids, function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\")) The results are shown here (but only for 2 pathways and only the KEGG output): Another tutorial about this pathway stuff can be found here .","title":"KEGG pathway analysis"},{"location":"other_topics/databases/","text":"Major Databases Nucleic ( ENA , Genbank , DDBJ ) ENA European Nucleotide Archive (go here to add DNA) they all share the same Accessionnumber Protein (SwissProt, RefSeq, TREMBL) all proteins go through Interpro Genomic (ENSEMBL) Older DB-Versions are always available only curated and publicized Genomes highly accurate Looking for a Species? go to ENSEMBL includes Cytogenic Banding (shows how stained Chromosomes have to look) EST shows were the gene is expressed (e. g. liver) Protein structure (PDB) DB for protein analysis Sequences from: Uniprot (Swissprot; trEMBL); Structure PDB (rcsb.org) Usually similar seq/structure = similar function Sequence motifs explained: {L} = everything except L; [LM] = M or L; x = everything [color=lightgreen] Use sequence profiles (e.g. motifs) to get more distant but related proteins, instead of just using BLAST Physiochemical properties (weight, electric load, hydrophili profile etc.) are taken into account for alignments Secondary structure prediction use a combination of methods, not only one try get a consensus \"prediction pattern\" of all prediction methods Best Site for this is \"NPSA-pbil\", its called consensus sec. structure prediction search here . other: http://expasy.org/ http://www.unoriprot.g/ http://www.ebiokit.eu/ http://blast.ncbi.nlm.nih.gov/Blast.cgi https://www.ebi.ac.uk/interpro http://www.rcsb.org/pdb Protein Visualization with: UGENE Suite , DeepView, VMD (good one), Pymol, Rasmol/Jmol Tridimensional structure prediction (homology modeling) You need to find a protein template (use BLAST or PSI-BLAST) with a similarity of at least 30 % and a structure Do a pairwaise alignment then Build a predicted structure with: Rigid body assembly, segment matching (SWISS - MODEL, SegMOD) Satisfaction of spatial restraints (distance and angles) => MODELLER, Geno3D Artificial evolution (template - based methods with ab initio - like energy minimization principles) => NEST Check quality afterwards (PROCHECK, PDBSum etc. to see if the model seems possible)","title":"Databases"},{"location":"other_topics/databases/#major-databases","text":"Nucleic ( ENA , Genbank , DDBJ ) ENA European Nucleotide Archive (go here to add DNA) they all share the same Accessionnumber Protein (SwissProt, RefSeq, TREMBL) all proteins go through Interpro Genomic (ENSEMBL) Older DB-Versions are always available only curated and publicized Genomes highly accurate Looking for a Species? go to ENSEMBL includes Cytogenic Banding (shows how stained Chromosomes have to look) EST shows were the gene is expressed (e. g. liver) Protein structure (PDB)","title":"Major Databases"},{"location":"other_topics/databases/#db-for-protein-analysis","text":"Sequences from: Uniprot (Swissprot; trEMBL); Structure PDB (rcsb.org) Usually similar seq/structure = similar function Sequence motifs explained: {L} = everything except L; [LM] = M or L; x = everything [color=lightgreen] Use sequence profiles (e.g. motifs) to get more distant but related proteins, instead of just using BLAST Physiochemical properties (weight, electric load, hydrophili profile etc.) are taken into account for alignments","title":"DB for protein analysis"},{"location":"other_topics/databases/#secondary-structure-prediction","text":"use a combination of methods, not only one try get a consensus \"prediction pattern\" of all prediction methods Best Site for this is \"NPSA-pbil\", its called consensus sec. structure prediction search here . other: http://expasy.org/ http://www.unoriprot.g/ http://www.ebiokit.eu/ http://blast.ncbi.nlm.nih.gov/Blast.cgi https://www.ebi.ac.uk/interpro http://www.rcsb.org/pdb Protein Visualization with: UGENE Suite , DeepView, VMD (good one), Pymol, Rasmol/Jmol","title":"Secondary structure prediction"},{"location":"other_topics/databases/#tridimensional-structure-prediction-homology-modeling","text":"You need to find a protein template (use BLAST or PSI-BLAST) with a similarity of at least 30 % and a structure Do a pairwaise alignment then Build a predicted structure with: Rigid body assembly, segment matching (SWISS - MODEL, SegMOD) Satisfaction of spatial restraints (distance and angles) => MODELLER, Geno3D Artificial evolution (template - based methods with ab initio - like energy minimization principles) => NEST Check quality afterwards (PROCHECK, PDBSum etc. to see if the model seems possible)","title":"Tridimensional structure prediction (homology modeling)"},{"location":"other_topics/gs/","text":"Genomic selection (GS) Overview (MAS versus GS) Marker assisted selection (MAS) : A small number of molelcular markers are used to tag genes-of-interest, but the overall impact on enhancing the efficiency of breeding is limited. MAS has been successfully used to incorporate major genes and/or QTLs. But, most traits of interest are not controlled by just a few large-effect genes, but by many genes of small effect and/or by a combination of major and minor genes. MAS is far less suitable for these types of trait genetic architectures. Epistatic interactions and the effects of genetic background make molecular breeding even more complicated. Traditional EBV (estimated breeding values): based on Mendelian Sampling * for MAS you use the program MAS-BLUP Genomic selection (GS) , introduced in 2001, presents a new alternative to traditional MAS that actually improve gain per selection in a breeding program per unit time, and thus breeding efficiency. In a GS breeding schema, genome-wide DNA markers are used to predict which individuals in a breeding population are most valuable as parents of the next generation of offspring. GC can get breeding values on newborns or embryos, therefore we can do selection way earlier than waiting for a specific trait to show. The generation intervall is faster and the genetic progress per year increases significantly. gEBV (genomic-based estimated breeding values): Selection on SNP effects, includes Mendelian Sampling [color=lightgreen] + for GS you use the program GBLUP A good overview of this can be found here. MAS Genomic Selection Find QTL or genes and select specifically for favourable alleles Estimate effects across all markers and select on the sum of effects Need strategy to combine with EBV Replaces EBV (gEBV) LE-MAS computationally intensive Can be very computationally intensive LE-MAS has major genotyping requirements Major genotyping requirements MAS - marker assisted selection 3 starting points of MAS GAS (gene assisted selection) Functional mutations - known genes QTL Genotype and effect Genotype selection candidates * For application in unrelated breeds, effect needs to be verified LD-MAS (linkage disequilibrium- marker assisted selection) Markers in pop.-wide LD with functional mutation As straightforward as GAS, more markers when using haplotype Monitor association over time * Application in other breeds my require confirmation study LE-MAS (linkage equilibrium- marker assisted selection) Markers in pop.-wide LE with functional mutation Only within family LD between marker and QTL (quantitative trait locus) IBD and QTL variance Requires extensive genotyping and statistical analysis Can be implemented directly after QTL detection Strategies for MAS and Uptake How to balance selection on markers and standard EBV? Tandem Selection * First select animals with favourable genotype * The best EBV within that group Index Selection * Select on weighted sum of EBV and Marker Score * Pre-selection * Select on markers in early life * Then EBV later in life Uptake: + Mainly for single gene defects (GAS or LD-MAS) + BLAD, CVM, RYR + Some candidate genes + IGF2, Myostatin, MC4R GS - genomic selection Overview Use genome-wide SNP information Rather than detecting QTL, estimate effects for all SNPs or SNP haplotypes Genomic breeding value (gEBV) is sum of all haplotype effects Principle/Concepts for GS You have a training population Population with known phenotypes (traits) thousands of genetic markers Used to create the gEBV You have a selection population, were you apply the gEBV so only the genotypes are known estimate the phenotype based on the gEBV Based on this you choose parents for the next \"generation\" of traits you want GBLUP BLUP means best linear unbiased prediction , and this method allows breeders of livestock to predict the breeding value of their herd animals. GBLUP is the genetic-based BLUP. Special Case: each genome region contributes equally to the trait * Rather that summing BLUP estimates across SNPs * Estimate BLUP at animal level using average marker based relationships. Same as traditional EBV but using a marker-derived relationship matrix rather than a pedigree ( Stammbaum ) derived A matrix. * No need for pedigree recording (all genome selected), so may be a solution for species or systems where pedigree recording is difficult Basic Formula for Genetic gain per year: some errors here since i need a formular generation $$ G = {r * i * \\sigma a \\over L_G} $$ G = Genetic gain per year i = Selection intensity e. g. i take the top 10 % of all animals with my trait \u03c3a = additative genetic standard derivation L~G~ = Generation intervall (average year of the parents when their offspring is born) r = its a normalization of D $$ r = \\sqrt{N_p h\u00b2\\over N_p h\u00b2+min(N_{QTL},M_e)} $$ N~P~ = Number of phenotypes h\u00b2 = heritability N~QTL~ = M~e~ is calculated with: $$ M_e = {2N_eL \\over ln(4N_eL) } $$ L = Genome size in Morgan (you need to google the value) N~e~ = Population size Software is available for these calculations e. g. SelAction for Windows; or use Excel Spreadsheet Rule of thumb: for an accuracy of around 0.99: * 10 * population size * L markers * 100 * population size * L phenotypes in training set Pros and Cons of GS Pro Contra Increases accuracy Genotyping not cheap Don\u2019t need records on all animals Need new evaluation tools May not need pedigree So far, mainly simulation results Can monitor inbreeding Long term effects unknown Only accounts for additive SNP effects Other information about breeding Pig and chicken Breeding are using pure lines for selection but crossbreds for the \"consumer products\" like meat and eggs Heterosis - crossbreading performes better then the mean of parents. (e.g. A\u00b3\u2070\u2070 + B\u2074\u2070\u2070 is not C\u00b3\u2075\u2070 its C\u00b3\u2075\u2070\u207b\u00b3\u2077\u2075) Aquaculture has mass spawning, and you need populations for breeding (e.g. 30 female and 30 male fish's) -> gene marker are really useful here","title":"GS"},{"location":"other_topics/gs/#genomic-selection-gs","text":"","title":"Genomic selection (GS)"},{"location":"other_topics/gs/#overview-mas-versus-gs","text":"Marker assisted selection (MAS) : A small number of molelcular markers are used to tag genes-of-interest, but the overall impact on enhancing the efficiency of breeding is limited. MAS has been successfully used to incorporate major genes and/or QTLs. But, most traits of interest are not controlled by just a few large-effect genes, but by many genes of small effect and/or by a combination of major and minor genes. MAS is far less suitable for these types of trait genetic architectures. Epistatic interactions and the effects of genetic background make molecular breeding even more complicated. Traditional EBV (estimated breeding values): based on Mendelian Sampling * for MAS you use the program MAS-BLUP Genomic selection (GS) , introduced in 2001, presents a new alternative to traditional MAS that actually improve gain per selection in a breeding program per unit time, and thus breeding efficiency. In a GS breeding schema, genome-wide DNA markers are used to predict which individuals in a breeding population are most valuable as parents of the next generation of offspring. GC can get breeding values on newborns or embryos, therefore we can do selection way earlier than waiting for a specific trait to show. The generation intervall is faster and the genetic progress per year increases significantly. gEBV (genomic-based estimated breeding values): Selection on SNP effects, includes Mendelian Sampling [color=lightgreen] + for GS you use the program GBLUP A good overview of this can be found here. MAS Genomic Selection Find QTL or genes and select specifically for favourable alleles Estimate effects across all markers and select on the sum of effects Need strategy to combine with EBV Replaces EBV (gEBV) LE-MAS computationally intensive Can be very computationally intensive LE-MAS has major genotyping requirements Major genotyping requirements","title":"Overview (MAS versus GS)"},{"location":"other_topics/gs/#mas-marker-assisted-selection","text":"","title":"MAS - marker assisted selection"},{"location":"other_topics/gs/#3-starting-points-of-mas","text":"GAS (gene assisted selection) Functional mutations - known genes QTL Genotype and effect Genotype selection candidates * For application in unrelated breeds, effect needs to be verified LD-MAS (linkage disequilibrium- marker assisted selection) Markers in pop.-wide LD with functional mutation As straightforward as GAS, more markers when using haplotype Monitor association over time * Application in other breeds my require confirmation study LE-MAS (linkage equilibrium- marker assisted selection) Markers in pop.-wide LE with functional mutation Only within family LD between marker and QTL (quantitative trait locus) IBD and QTL variance Requires extensive genotyping and statistical analysis Can be implemented directly after QTL detection","title":"3 starting points of MAS"},{"location":"other_topics/gs/#strategies-for-mas-and-uptake","text":"How to balance selection on markers and standard EBV? Tandem Selection * First select animals with favourable genotype * The best EBV within that group Index Selection * Select on weighted sum of EBV and Marker Score * Pre-selection * Select on markers in early life * Then EBV later in life Uptake: + Mainly for single gene defects (GAS or LD-MAS) + BLAD, CVM, RYR + Some candidate genes + IGF2, Myostatin, MC4R","title":"Strategies for MAS and Uptake"},{"location":"other_topics/gs/#gs-genomic-selection","text":"","title":"GS - genomic selection"},{"location":"other_topics/gs/#overview","text":"Use genome-wide SNP information Rather than detecting QTL, estimate effects for all SNPs or SNP haplotypes Genomic breeding value (gEBV) is sum of all haplotype effects","title":"Overview"},{"location":"other_topics/gs/#principleconcepts-for-gs","text":"You have a training population Population with known phenotypes (traits) thousands of genetic markers Used to create the gEBV You have a selection population, were you apply the gEBV so only the genotypes are known estimate the phenotype based on the gEBV Based on this you choose parents for the next \"generation\" of traits you want","title":"Principle/Concepts for GS"},{"location":"other_topics/gs/#gblup","text":"BLUP means best linear unbiased prediction , and this method allows breeders of livestock to predict the breeding value of their herd animals. GBLUP is the genetic-based BLUP. Special Case: each genome region contributes equally to the trait * Rather that summing BLUP estimates across SNPs * Estimate BLUP at animal level using average marker based relationships. Same as traditional EBV but using a marker-derived relationship matrix rather than a pedigree ( Stammbaum ) derived A matrix. * No need for pedigree recording (all genome selected), so may be a solution for species or systems where pedigree recording is difficult Basic Formula for Genetic gain per year: some errors here since i need a formular generation $$ G = {r * i * \\sigma a \\over L_G} $$ G = Genetic gain per year i = Selection intensity e. g. i take the top 10 % of all animals with my trait \u03c3a = additative genetic standard derivation L~G~ = Generation intervall (average year of the parents when their offspring is born) r = its a normalization of D $$ r = \\sqrt{N_p h\u00b2\\over N_p h\u00b2+min(N_{QTL},M_e)} $$ N~P~ = Number of phenotypes h\u00b2 = heritability N~QTL~ = M~e~ is calculated with: $$ M_e = {2N_eL \\over ln(4N_eL) } $$ L = Genome size in Morgan (you need to google the value) N~e~ = Population size Software is available for these calculations e. g. SelAction for Windows; or use Excel Spreadsheet Rule of thumb: for an accuracy of around 0.99: * 10 * population size * L markers * 100 * population size * L phenotypes in training set","title":"GBLUP"},{"location":"other_topics/gs/#pros-and-cons-of-gs","text":"Pro Contra Increases accuracy Genotyping not cheap Don\u2019t need records on all animals Need new evaluation tools May not need pedigree So far, mainly simulation results Can monitor inbreeding Long term effects unknown Only accounts for additive SNP effects","title":"Pros and Cons of GS"},{"location":"other_topics/gs/#other-information-about-breeding","text":"Pig and chicken Breeding are using pure lines for selection but crossbreds for the \"consumer products\" like meat and eggs Heterosis - crossbreading performes better then the mean of parents. (e.g. A\u00b3\u2070\u2070 + B\u2074\u2070\u2070 is not C\u00b3\u2075\u2070 its C\u00b3\u2075\u2070\u207b\u00b3\u2077\u2075) Aquaculture has mass spawning, and you need populations for breeding (e.g. 30 female and 30 male fish's) -> gene marker are really useful here","title":"Other information about breeding"},{"location":"other_topics/gwas/","text":"GWAS (Genome wide association study) QTL (quantitative trait locus) More than one gene is responsible for certain traits genes can interact between loci (epistatic effects) or within the loci (dominance effects) QTL is a section of DNA (locus) which correlates with a variation in a phenotype it is linked to, or contains, the genes which control that phenotype QTLs are mapped by identifying which SNPs correlate with an observed trait one QTL can affect multiple traits, and can be dominant or recessive How to identify QTLs 1. QTL/Linkage Mapping 2. GWAS (newer) study pop. is families or experimental crosses uses a population sample hundreds of DNA markers Tens of thousands DNA marker Current recombinations historical recombinations Linkage disequilibrium (LD) (non random association of allels between locis, LD \"decays\" - the faster the decay the higher the marker density has to be) Pro : can scan genome with fewer markers Cons : Can only detect alleles with large effect; limited resolution (identify broad region, not individual genes); requires data on multiple family members Association (GWAS) Pros : can detect subtle effects; very fine resolution Cons : requires 0.5 to 1 million markers to cover whole genome; requires large sample size 1. QTL/Linkage Mapping (inferior to GWAS) Determining the location of a gene in the genome Estimating the effects of the alleles and mode of action Individuals inherit different marker variants Individuals vary according to their marker genotype You have to know what you are looking for (e. g. certain diseases) Costly since you have to analyze each generation (good for disease reconstructions) The QTL has to be inferred from knowledge and prediction (e. g. color of the fur/hide) For QTL Linkage Mapping you need: Well recorded population with high quality DNA samples DNA Markers across the genome Map of Marker positions High quality genotype 2. GWAS (best method) In genetics, a genome-wide association study ( GWAS ), is an observational study of a genome-wide set of genetic variants in different individuals to see if ==any variant is associated with a trait== GWASs typically focus on associations between SNPs and traits (e.g. SNPs associated with milk production) a) Take a large (thousands), representative, sample of the population b) Characterised for a very large number of DNA variants c) Estimate a putative effect of every DNA variant on the trait of interest QC of SNP Genotyping For samples Blind duplicates genders unsuspected twinning or cryptic relatedness DNA degradation or fragmentation Call rate (> 80-90%) Heterozygosity: outliers, Plate/batch calling effects For SNPs Duplicate concordance (CEPH samples) Mendelian errors (typically < 1) Hardy-Weinberg errors (often > 10-5 ) Heterozygosity (outliers) Call rate (typically > 98%) Minor allele frequency (often > 1%) Validation of most critical results on independent genotyping platform Association analysis is often quickest way to find genotyping errors (PLINK) Signifikant SNP results and QC signifikant results in an manhatten plot or a Q-Q plot A significant marker associated with a QT does not imply a causative quantitative trait nucleotide (QTN) significant result indicates: The marker is in linkage disequilibrium with a QTN false positive result: by chance or due to Population stratification positive results have to be backed up with functional data and/or a replication study in a different population Problems that bias you results (confounding factors) SNPs can correlate with latent variables the variables can have an effect! SNP then gives false positive result Examples: Population structure Admixture breed or even family Geographical effects Batch effects Solutions: a) Genomic control - inflating factor to control p values b) Structure - estimate a population structure c) Principle components - genome wide IBS matrix (in Plink its the --cluster command) PLINK (Software) - practical whole genome wide association (GWAS) analysis tool Install by google PLINK and get the precompiled *.zip file. mv plink ~/.local/bin , done The workflow is usually: 1. Plan the study (choose design to target your hypothesis) 2. Collect data (based on design, number of markers) 3. Remove problematic data 4. Identify other pedigree related problems (population stratification) 5. Association analysis 6. Correction of results to minimize false positives 7. further validation of the region (e. g. identify possible genes) Correction and binary file creation Without phenotype file using .ped and .map files --ped wolf.ped and --map wolf.map .ped contains Family-, Individual-, Paternal-, Maternal-ID, Sex and phenotype .map contains chromosome, rs# or snp identifier, Genetic distance (morgans) and bp position we correct the individuals and SNPs with the flags: --geno 0.25 --maf 0.05 --mind 0.25 we use the --dog flag (or --cow ) --out specifies outputname, ==NEVER CHANGE IT== unless written and highlighted here we save the data also in binary using the --make-bed ; this creates 3 files: wolf.bed , wolf.fam and wolf.bin these files are now used with --bfile wolf so --ped, --map, --geno, --maf, --mind fall away plink --ped wolf.ped --map wolf.map --out wolf --geno 0.25 --maf 0.05 --mind 0.25 --dog --noweb --allow-no-sex --make-bed this creates a log file: wolf.log states amount of ==individuals== and how many excluded states how many ==SNPs== are included/excluded With phenotype file same command and usage as above, but we add the flag --no-pheno and the flag/file --pheno cow.phe to the command example with cow data: plink --ped cow.ped --map cow.map --out cow --geno 0.25 --maf 0.05 --mind 0.25 --cow --noweb --allow-no-sex --make-bed --no-pheno --pheno cow.phe Various analysises --freq - Allel frequency, creates a .frq file --missing - --hardy - --asso - * add a flag to this basic command: plink --bfile wolf --out wolf --dog --noweb --allow-no-sex Population stratification used to check if you may have more than one population (pedigree related problems), details here creating a 2D-MDS plot (more under 09 - \"\u03b2 diversity analysis\" ) plink --bfile wolf --out wolf --dog --noweb --allow-no-sex --cluster --mds-plot 2 Plotting in excel, we see 3 distinct populations: Basic reports, case/control phenotype for this to work you need the additional phenotype datafile .phe see also here first correcting the results (p-values) for multiple testing with 10,000 permutations which creates an .assoc and .assoc.mperm file: plink --bfile cow --allow-no-sex --cow --out cow --assoc --noweb --mperm 10000 --no-pheno --pheno cow.phe adjustment of results (e. g. Bonferroni adjusted significance value) change the --out to something else plink --bfile cow --allow-no-sex --cow --out cow2 --assoc --noweb --adjust use the new file cow2.qassoc for plotting Ploting the data Using manhatten plot, install with install.packages(\"qqman\") the sep=\"\" of read.table threats multiple space as one library(qqman) x <- read.table(\"cow2.qassoc\", sep=\"\", header = TRUE) manhattan(x, chr=\"CHR\", bp=\"BP\", snp=\"SNP\", p=\"P\") to highlight SNPs of interest add , highlight= y to the manhatten command, y is this: y <- c(\"rs3001\", \"SNP205\", \"rs3003\") search via excel the .qassoc for SNPs of interest (closer to 0 the more significant so smaller than 1e\u207b\u2077), see line above blue in the plot Results: * check this out for how to get GWAS into a plot or this figure","title":"GWAS"},{"location":"other_topics/gwas/#gwas-genome-wide-association-study","text":"","title":"GWAS (Genome wide association study)"},{"location":"other_topics/gwas/#qtl-quantitative-trait-locus","text":"More than one gene is responsible for certain traits genes can interact between loci (epistatic effects) or within the loci (dominance effects) QTL is a section of DNA (locus) which correlates with a variation in a phenotype it is linked to, or contains, the genes which control that phenotype QTLs are mapped by identifying which SNPs correlate with an observed trait one QTL can affect multiple traits, and can be dominant or recessive","title":"QTL (quantitative trait locus)"},{"location":"other_topics/gwas/#how-to-identify-qtls","text":"1. QTL/Linkage Mapping 2. GWAS (newer) study pop. is families or experimental crosses uses a population sample hundreds of DNA markers Tens of thousands DNA marker Current recombinations historical recombinations Linkage disequilibrium (LD) (non random association of allels between locis, LD \"decays\" - the faster the decay the higher the marker density has to be) Pro : can scan genome with fewer markers Cons : Can only detect alleles with large effect; limited resolution (identify broad region, not individual genes); requires data on multiple family members Association (GWAS) Pros : can detect subtle effects; very fine resolution Cons : requires 0.5 to 1 million markers to cover whole genome; requires large sample size","title":"How to identify QTLs"},{"location":"other_topics/gwas/#1-qtllinkage-mapping-inferior-to-gwas","text":"Determining the location of a gene in the genome Estimating the effects of the alleles and mode of action Individuals inherit different marker variants Individuals vary according to their marker genotype You have to know what you are looking for (e. g. certain diseases) Costly since you have to analyze each generation (good for disease reconstructions) The QTL has to be inferred from knowledge and prediction (e. g. color of the fur/hide) For QTL Linkage Mapping you need: Well recorded population with high quality DNA samples DNA Markers across the genome Map of Marker positions High quality genotype","title":"1. QTL/Linkage Mapping (inferior to GWAS)"},{"location":"other_topics/gwas/#2-gwas-best-method","text":"In genetics, a genome-wide association study ( GWAS ), is an observational study of a genome-wide set of genetic variants in different individuals to see if ==any variant is associated with a trait== GWASs typically focus on associations between SNPs and traits (e.g. SNPs associated with milk production) a) Take a large (thousands), representative, sample of the population b) Characterised for a very large number of DNA variants c) Estimate a putative effect of every DNA variant on the trait of interest","title":"2. GWAS (best method)"},{"location":"other_topics/gwas/#qc-of-snp-genotyping","text":"","title":"QC of SNP Genotyping"},{"location":"other_topics/gwas/#for-samples","text":"Blind duplicates genders unsuspected twinning or cryptic relatedness DNA degradation or fragmentation Call rate (> 80-90%) Heterozygosity: outliers, Plate/batch calling effects","title":"For samples"},{"location":"other_topics/gwas/#for-snps","text":"Duplicate concordance (CEPH samples) Mendelian errors (typically < 1) Hardy-Weinberg errors (often > 10-5 ) Heterozygosity (outliers) Call rate (typically > 98%) Minor allele frequency (often > 1%) Validation of most critical results on independent genotyping platform Association analysis is often quickest way to find genotyping errors (PLINK)","title":"For SNPs"},{"location":"other_topics/gwas/#signifikant-snp-results-and-qc","text":"signifikant results in an manhatten plot or a Q-Q plot A significant marker associated with a QT does not imply a causative quantitative trait nucleotide (QTN) significant result indicates: The marker is in linkage disequilibrium with a QTN false positive result: by chance or due to Population stratification positive results have to be backed up with functional data and/or a replication study in a different population","title":"Signifikant SNP results and QC"},{"location":"other_topics/gwas/#problems-that-bias-you-results-confounding-factors","text":"SNPs can correlate with latent variables the variables can have an effect! SNP then gives false positive result Examples: Population structure Admixture breed or even family Geographical effects Batch effects Solutions: a) Genomic control - inflating factor to control p values b) Structure - estimate a population structure c) Principle components - genome wide IBS matrix (in Plink its the --cluster command)","title":"Problems that bias you results (confounding factors)"},{"location":"other_topics/gwas/#plink-software-practical","text":"whole genome wide association (GWAS) analysis tool Install by google PLINK and get the precompiled *.zip file. mv plink ~/.local/bin , done The workflow is usually: 1. Plan the study (choose design to target your hypothesis) 2. Collect data (based on design, number of markers) 3. Remove problematic data 4. Identify other pedigree related problems (population stratification) 5. Association analysis 6. Correction of results to minimize false positives 7. further validation of the region (e. g. identify possible genes)","title":"PLINK (Software) - practical"},{"location":"other_topics/gwas/#correction-and-binary-file-creation","text":"","title":"Correction and binary file creation"},{"location":"other_topics/gwas/#without-phenotype-file","text":"using .ped and .map files --ped wolf.ped and --map wolf.map .ped contains Family-, Individual-, Paternal-, Maternal-ID, Sex and phenotype .map contains chromosome, rs# or snp identifier, Genetic distance (morgans) and bp position we correct the individuals and SNPs with the flags: --geno 0.25 --maf 0.05 --mind 0.25 we use the --dog flag (or --cow ) --out specifies outputname, ==NEVER CHANGE IT== unless written and highlighted here we save the data also in binary using the --make-bed ; this creates 3 files: wolf.bed , wolf.fam and wolf.bin these files are now used with --bfile wolf so --ped, --map, --geno, --maf, --mind fall away plink --ped wolf.ped --map wolf.map --out wolf --geno 0.25 --maf 0.05 --mind 0.25 --dog --noweb --allow-no-sex --make-bed this creates a log file: wolf.log states amount of ==individuals== and how many excluded states how many ==SNPs== are included/excluded","title":"Without phenotype file"},{"location":"other_topics/gwas/#with-phenotype-file","text":"same command and usage as above, but we add the flag --no-pheno and the flag/file --pheno cow.phe to the command example with cow data: plink --ped cow.ped --map cow.map --out cow --geno 0.25 --maf 0.05 --mind 0.25 --cow --noweb --allow-no-sex --make-bed --no-pheno --pheno cow.phe","title":"With phenotype file"},{"location":"other_topics/gwas/#various-analysises","text":"--freq - Allel frequency, creates a .frq file --missing - --hardy - --asso - * add a flag to this basic command: plink --bfile wolf --out wolf --dog --noweb --allow-no-sex","title":"Various analysises"},{"location":"other_topics/gwas/#population-stratification","text":"used to check if you may have more than one population (pedigree related problems), details here creating a 2D-MDS plot (more under 09 - \"\u03b2 diversity analysis\" ) plink --bfile wolf --out wolf --dog --noweb --allow-no-sex --cluster --mds-plot 2 Plotting in excel, we see 3 distinct populations:","title":"Population stratification"},{"location":"other_topics/gwas/#basic-reports-casecontrol-phenotype","text":"for this to work you need the additional phenotype datafile .phe see also here first correcting the results (p-values) for multiple testing with 10,000 permutations which creates an .assoc and .assoc.mperm file: plink --bfile cow --allow-no-sex --cow --out cow --assoc --noweb --mperm 10000 --no-pheno --pheno cow.phe adjustment of results (e. g. Bonferroni adjusted significance value) change the --out to something else plink --bfile cow --allow-no-sex --cow --out cow2 --assoc --noweb --adjust use the new file cow2.qassoc for plotting","title":"Basic reports, case/control phenotype"},{"location":"other_topics/gwas/#ploting-the-data","text":"Using manhatten plot, install with install.packages(\"qqman\") the sep=\"\" of read.table threats multiple space as one library(qqman) x <- read.table(\"cow2.qassoc\", sep=\"\", header = TRUE) manhattan(x, chr=\"CHR\", bp=\"BP\", snp=\"SNP\", p=\"P\") to highlight SNPs of interest add , highlight= y to the manhatten command, y is this: y <- c(\"rs3001\", \"SNP205\", \"rs3003\") search via excel the .qassoc for SNPs of interest (closer to 0 the more significant so smaller than 1e\u207b\u2077), see line above blue in the plot Results: * check this out for how to get GWAS into a plot or this figure","title":"Ploting the data"},{"location":"python/bioconda/","text":"Conda Package, dependency and environment management for any language\u2014Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN check here: https://conda.io/docs/user-guide/install/linux.html Installation miniconda should be enough, the others are \"bigger\" wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3*.sh add conda channels add bioconda to the channel https://bioconda.github.io/ conda config --add channels defaults #usually already installed conda config --add channels bioconda conda config --add channels conda-forge","title":"Bioconda"},{"location":"python/bioconda/#conda","text":"Package, dependency and environment management for any language\u2014Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN check here: https://conda.io/docs/user-guide/install/linux.html","title":"Conda"},{"location":"python/bioconda/#installation","text":"miniconda should be enough, the others are \"bigger\" wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3*.sh add conda channels add bioconda to the channel https://bioconda.github.io/ conda config --add channels defaults #usually already installed conda config --add channels bioconda conda config --add channels conda-forge","title":"Installation"},{"location":"python/python/","text":"Basics in shell do python3 to open the command line python3 skript.py to execute a .py skript Installing pip due to the version chaos that is pip pip3 python and python3, here an explanation stick with python 3 so if a site tells you to do pip install do pip3 install # installing the pip3 from python3.X (not pip from python 2.7) sudo apt-get install python3-pip ## use the apt-get for now # example of a pip install, DONT use sudo here pip3 install pylama pylama-pylint Special rules you can split a single line like this into multiple lines using \\ total = item_one + \\ item_two + \\ item_three Commands #!/usr/bin/python3 print(\"test\") #basically like echo, includes \\n input(\"\\n\\nPress the enter key to exit.\") #creates a user prompt, has two empty lines before ; # \";\" is used like in shell to get more commands in one line Variables in python3 Usually Class names start with an uppercase letter All other identifiers start with a lowercase letter Starting an identifier with a single leading underscore indicates that the identifier is private don't use words like for etc for it A few examples: Numeric Variables #!/usr/bin/python3 counter = 100 # An integer assignment miles = 1000.0 # A floating point name = \"John\" # A string cplx = 70.2-E12 # complex number # also possible to add one input to multiple variables a = b = c = 1 #or like an oneliner array (so a get 1 b 2 etc.) a, b, c = 1, 2, \"john\" # delete them with del counter, miles String Variables #!/usr/bin/python3 str = 'Hello World!' print (str) # Prints complete string, starts at 0 print (str[0]) # Prints first character of the string print (str[2:5]) # Prints characters starting from 3rd to 5th print (str[2:]) # Prints string starting from 3rd character print (str * 2) # Prints string two times print (str + \"TEST\") # works only on strings List Variables works a bit like in R can store different variable types, can be changed a list works only with [] a \"read only\" list is with () #!/usr/bin/python3 list = [ 'abcd', 786 , 2.23, 'john', 70.2 ] tinylist = [123, 'john'] print (list) # Prints complete list print (list[0]) # Prints first element of the list print (list[1:3]) # Prints elements starting from 2nd till 3rd print (list[2:]) # Prints elements starting from 3rd element print (tinylist * 2) # Prints list two times print (list + tinylist) # Prints concatenated lists tuple = ( 'abcd', 786 , 2.23, 'john', 70.2 ) # print works exactly the same here there is also a dictionary type of list with {} its basically unordered, so you can shove data into it check here Loops are without braces, spacing and lines depends if it works examples: for i in [1,2,3]: print(i) #only shows the variable print('%sbp' % i) #this is how to put variables and text together print(\"____\") print(\"loop is ended, wow\") #first line with : is called header, all of it is called suite if expression : suite elif expression : suite else : suite","title":"Basics"},{"location":"python/python/#basics","text":"in shell do python3 to open the command line python3 skript.py to execute a .py skript","title":"Basics"},{"location":"python/python/#installing-pip","text":"due to the version chaos that is pip pip3 python and python3, here an explanation stick with python 3 so if a site tells you to do pip install do pip3 install # installing the pip3 from python3.X (not pip from python 2.7) sudo apt-get install python3-pip ## use the apt-get for now # example of a pip install, DONT use sudo here pip3 install pylama pylama-pylint","title":"Installing pip"},{"location":"python/python/#special-rules","text":"you can split a single line like this into multiple lines using \\ total = item_one + \\ item_two + \\ item_three","title":"Special rules"},{"location":"python/python/#commands","text":"#!/usr/bin/python3 print(\"test\") #basically like echo, includes \\n input(\"\\n\\nPress the enter key to exit.\") #creates a user prompt, has two empty lines before ; # \";\" is used like in shell to get more commands in one line","title":"Commands"},{"location":"python/python/#variables-in-python3","text":"Usually Class names start with an uppercase letter All other identifiers start with a lowercase letter Starting an identifier with a single leading underscore indicates that the identifier is private don't use words like for etc for it A few examples:","title":"Variables in python3"},{"location":"python/python/#numeric-variables","text":"#!/usr/bin/python3 counter = 100 # An integer assignment miles = 1000.0 # A floating point name = \"John\" # A string cplx = 70.2-E12 # complex number # also possible to add one input to multiple variables a = b = c = 1 #or like an oneliner array (so a get 1 b 2 etc.) a, b, c = 1, 2, \"john\" # delete them with del counter, miles","title":"Numeric Variables"},{"location":"python/python/#string-variables","text":"#!/usr/bin/python3 str = 'Hello World!' print (str) # Prints complete string, starts at 0 print (str[0]) # Prints first character of the string print (str[2:5]) # Prints characters starting from 3rd to 5th print (str[2:]) # Prints string starting from 3rd character print (str * 2) # Prints string two times print (str + \"TEST\") # works only on strings","title":"String Variables"},{"location":"python/python/#list-variables","text":"works a bit like in R can store different variable types, can be changed a list works only with [] a \"read only\" list is with () #!/usr/bin/python3 list = [ 'abcd', 786 , 2.23, 'john', 70.2 ] tinylist = [123, 'john'] print (list) # Prints complete list print (list[0]) # Prints first element of the list print (list[1:3]) # Prints elements starting from 2nd till 3rd print (list[2:]) # Prints elements starting from 3rd element print (tinylist * 2) # Prints list two times print (list + tinylist) # Prints concatenated lists tuple = ( 'abcd', 786 , 2.23, 'john', 70.2 ) # print works exactly the same here there is also a dictionary type of list with {} its basically unordered, so you can shove data into it check here","title":"List Variables"},{"location":"python/python/#loops","text":"are without braces, spacing and lines depends if it works examples: for i in [1,2,3]: print(i) #only shows the variable print('%sbp' % i) #this is how to put variables and text together print(\"____\") print(\"loop is ended, wow\") #first line with : is called header, all of it is called suite if expression : suite elif expression : suite else : suite","title":"Loops"},{"location":"sequencing/annotation/","text":"Genome annotation Structural genome annotation Structural gene annotation - find out where the region of interest is Functional gene annotation - find out what the region do gff - genome feature file Main steps: QC assembly -> structural annotation -> manual curation -> functional annotation -> Submission or Downstream analysis QC of assembly is highly important Repeat Masking to improve the gene annotations most pipelines have this included, but check first before using a annotation program annotations are either based on proteins or transcripts if a protein is unknown then you won't annotate it RNA-seq tries to find all transcripts - should always be included in a annotation project Approaches for annotations: Similarity-based methods - these use similarity to annotated sequences like proteins, cDNAs, or ESTs Ab initio prediction - likelihood based methods that needs to be trained (give them 1000 known genes of a species) * annotations based on gene content (codon usage, GC content, exon/intron size, promotor, ORF, start codons, splice sites and more) * sensitivity and specifity has to be determined after the training * false positive results? overpredicting? * sensitivity and specifity on nucleotide level is more important than on gene level when evaluating the performance Hybrid approaches - ab initio tools with the ability to integrate external evidence/hints Comparative (homology) based gene finders - these align genomic sequences from different species and use the alignments to guide the gene predictions Chooser, combiner approaches - these combine gene predictions of other gene finders Pipelines - These combine multiple approaches * use a pipeline Popular tools : Supported by the maker tool: SNAP - Works ok, easy to train, not as good as others especially on longer intron genomes. Augustus - Works great, hard to train (but getting better) GeneMark-ES - Self training, no hints, buggy, not good for fragmented genomes or long introns (Best suited for Fungi). FGENESH - Works great, costs money even for training. GlimmerHMM (Eukaryote) GenScan Gnomon (NCBI) but you want to use those tools in a pipeline - combining various methodes since it improves highly the output quality PIPELINES : PASA Produces evidence-driven consensus gene models (-) minimalist pipeline (+) good for detecting isoforms (+) biologically relevant predictions using Ab initio tools and combined with EVM it does a pretty good job PASA + Ab initio + EVM is not automatized NCBI pipeline best one yet - but difficult to install NCBI staff can be asked an they help you Evidence + ab initio (Gnomon), repeat masking, gene naming, data formatting, miRNAs, tRNAs Ensembl Evidence based only ( comparative + homology ) MAKER2 Evidence based and/or ab initio developed as an easy-to-use alternative to other pipelines Easy to use and to configure Almost unlimited parallelism built-in (limited by data and hardware) Largely independent from the underlying system it is run on Everything is run through one command, no manual combining of data/outputs Follows common standards, produces GMOD compliant output Annotation Edit Distance (AED) metric for improved quality control Provides a mechanism to train and retrain ab initio gene predictors Annotations can be updated by re-launching Maker with new evidence Pipelines give good results, MAKER2 the most flexible, adjustable Most methods only build gene models, no functional inference Computational pipelines make mistakes Annotation requires manual curation As for assembly, an annotation is never finished, it can always be improved (e.g. Human) Genome annotation with augustus - practical we are using chromosome 4 of the fruit fly, Drosophila melanogaster QC of the assembly first: Fragmentation? (N50, N90, how many short contigs) Sanity of the fasta file (Presence of Ns, presence of ambiguous nucleotides, presence of lowercase nucleotides, single line sequences vs multiline sequences) completeness (using BUSCO) presence of organelles Others (GC content, How distant the investigated species is from the others annotated species available) 1. BUSCO for Assembly check BUSCO provides quantitative measures for the assessment of genome assembly, gene set, and transcriptome completeness, based on evolutionarily-informed expectations of gene content from near-universal single-copy orthologs selected from OrthoDB v9 . first get the best dataset from the busco website wget http://busco.ezlab.org/datasets/metazoa_odb9.tar.gz tar xzvf metazoa_odb9.tar.gz launching BUSCO, to create the folder 4_dmel_busco inside is a short_summary_4_dmel_busco.txt for statistics on how many genes/regions could be found in comparision to the reference genome the better the assembly the more will be found BUSCO.py -i ~/annotation_course/data/genome/4.fa -o 4_dmel_busco -m geno -c 8 -l metazoa_odb9 fasta_statisticsAndPlot.pl is used for additional statistics Part of this github rep called GAAS fasta_statisticsAndPlot.pl -f ~/annotation_course/data/genome/4.fa 2. augustus an ab initio gene finder if satisfied by the quality of the assembly we start the annotation we focus on the gene finder augustus these gene finders use likelihoods to find the most likely genes in the genome they are aware of start and stop codons and splice sites, and will only try to predict genes that follow these rules the most important factor here is that the gene finder needs to be trained on the organism you are running the program on, otherwise the probabilities for introns, exons, etc. will not be correct we have training files for Drosophila Augustus * call it with a genome/assembly and it saves the annotation as a gff3 file augustus --species=fly ~/annotation_course/data/genome/4.fa --gff3=yes > augustus_drosophila.gff #to get additional isoforms use this: augustus --species=fly ~/annotation_course/data/genome/4.fa --gff3=yes --alternatives-from-sampling=true > augustus_drosophila_isoform.gff gff3_sp_statistics.pl is also from GAAS for the created gff file from augustus gives an statistical overview of the gene annotations gff3_sp_statistics.pl --gff augustus_drosophila.gff 3. Visualisation opened the chromosome 4 file and the gff file in UGENE looks like this: An other good and popular software to explore genomes is IGV maker gene build pipeline - practical MAKER is a computational pipeline to automatically generate annotations from a range of input data. The Maker pipeline can work with any combination of the following data sets, which are put into the maker_opts.ctl : Proteins from the same species or related Swissport for high quality protein sequences Refseq sequence sets from the ftp-servers Ensembl using Biomart interface to download data for a specific region or a specific gene Proteins from more distantly related organisms Uniprot to include protein sequences from organisms closely related to your study organism EST sequences from the same species or very closely related species NCBI or EBI websites to retrieve such kind of data RNA-seq data from the same or very closely related species, in the form of splice sites or assembled transcripts normally genereted by yourself or are retrived on the Sequence Read Archive of NCBI (SRA) or the ENA of EBI ab initio predictions from one or more tools (supported are: Augustus, Snap, GeneMark, Fgenesh) 1. Start maker do maker -CTL to create the 3 files for the pipeline ( maker_opts.ctl, maker_bopts.ctl, maker_exe.ctl ) 2.1 Evidence-based annotation the best protein- and EST-alignments are chosen to build the most likely gene model, without an ab initio model We need to prepare various files and add their path to the maker_opts.ctl file (use , to seperate files): name of the genome sequence ( genome= ) name of the 'EST' set file(s) ( est= ) name of the 'Protein' set file(s) ( protein= ) name of the repeatmasker and repeatrunner files ( rm_gff= ) disabling ab initio with protein2genome=1 , est2genome=1 we deactivated the parameters model_org= and repeat_protein= to avoid the heavy work of repeatmasker (blank values) if the maker_opts.ctl is configured properly start the maker pipeline with mpiexec -n 8 maker # -n is core numbers # and compile the output with: maker_merge_outputs_from_datastore.pl --output maker_no_abinitio in the folder you find the annotation file maker.gff to get statistics use the gff3_sp_statistics.pl gff3_sp_statistics.pl --gff maker_no_abinitio/annotationByType/maker.gff 2.2 Run Maker with ab-initio predictions builds on the gff from step 2.1 and does a ab-initio prediction These files were present(repeatmasker.chr4.gff, repeatrunner.chr4.gff, 4.fa) These were created in Step 2.1 (est_gff_stringtie.gff, est2genome.gff, protein2genome.gff) so add them to the maker_opts.ctl change these now to protein2genome=0 , est2genome=0 , keep_preds=1 , augustus_species=fly With these settings, Maker will run augustus to predict gene loci, but inform these predictions with information from the protein and est alignments You can combine step 2.1 & 2.2 into one its basically step 2.1 with augustus and ab initio activated (the gffs are created during the pipeline) mpiexec -n 8 maker #compiling maker_merge_outputs_from_datastore.pl --output maker_with_abinitio #check statistics with this GAAS script gff3_sp_statistics.pl --gff maker_with_abinitio/annotationByType/maker.gff Summary : We created 2 maker.gff files. One in maker_with_abinto/ one in maker_noabinto directory/ . Doing Step 2.2 helped to make the gene predictions less fragmented. For best performance go all th way to step 2.2. For Intron / Exon manual curation look for the GT/AG in an Intron 5'-3' [EXON]++GT--intron--AG++[EXON] 3'-5' [EXON]++GA--intron--TG++[EXON] Functional genome annotation Functional gene annotation - find out what the region does You can do this experimentally (slow and expensive) or computationally Computationally: Sequence based - mainly done * based on similarity/motif/profile * orthology based on evolutionary relationship * Clustering with KOG/COG * Synteny: Satsuma + kraken + custom script * phylogeny based Structure based * global structure comparison * localized regions * active sites resides * Protein-Protein Interaction data blast based functional genome annotation you need Sequence, gff3 files Uniprot (exhaustive) or Swissprot (reliable) Use Annie to extract best hits from blast-hit list and the corresponding description from uniprot-headers Add the information to the annotation.gff using custom-script then you have a nice description of the blast hit in the gff file Database Information Comment KEGG Pathway Kyoto Encyclopedia of Genes and Genomes MetaCyc Pathway Curated database of experimentally elucidated metabolic pathways from all domains of life (NIH) Reactome Pathway Curated and peer reviewed pathway database UniPathway Pathway Manually curated resource of enzyme-catalyzed and spontaneous chemical reactions. GO Gene Ontology Three structured, controlled vocabularies (ontologies) : biological processes, cellular components and molecular functions Pfam Protein families Multiple sequence alignments and hidden Markov models Interpro P. fam., domains & functional sites Run separate search applications, and create a signature to search against Interpro. Tool Approach Comment Trinotate Best blast hit, protein domain identification (HMMER/PFAM), protein signal peptide and transmembrane domain prediction (signalP/tmHMM), and leveraging various annotation databases (eggNOG/GO/Kegg databases). Not automated Annocript Best blast hit Collects the best-hit and related annotations (proteins, domains, GO terms, Enzymes, pathways, short) Annot8r Best blast hits A tool for Gene Ontology, KEGG biochemical pathways and Enzyme Commission EC number annotation of nucleotide and peptide sequences. Sma3s Best blast hit, Best reciprocal blast hit, clusterisation 3 annotation levels afterParty BLAST, InterProScan web application Interproscan Separate search applications, HMMs, fingerprints, patterns of InterPro Created to unite secondary databases Blast2Go get best blast hits Retrieve only GO,Commercial ! Interproscan approach - practical we took a gff file and the corresponding fasta file we had a pre downloaded database we used this script to extract the protein sequences with an gff annotation ( AA.fa ) gff3_sp_extract_sequences.pl --gff maker_with_abinitio.gff -f 4.fa -p --cfs -o AA.fa we used the interproscan complete thing - incl. databases (48 GB) to get the information there is a way to analyse through the web but it has a limit on requests #takes 2-3 seconds per protein interproscan.sh -i AA.fa -t p -dp -pa -appl Pfam,ProDom-2006.1,SuperFamily-1.75 --goterms --iprlookup load the retrieved functional information in your annotation file writing your own script or use the maker script ipr_update_gff maker_with_abinitio.gff AA.fa.tsv > maker_with_abinitio_with_interpro.gff BLAST approach - practical similar to your resistance gene blasts blastp -db ~/annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa -query AA.fa -outfmt 6 -out blast.out -num_threads 8 we are using annie to process the blast output git clone https://github.com/genomeannotation/Annie.git # This is the annie.py script Annie/annie.py -b blast.out -db ~/annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa -g maker_with_abinitio.gff -o annotation_blast.annie Annie writes in a 3-column table format file, providing gene name and mRNA product information. The purpose of annie is relatively simple. It recovers the information in the sequence header of the uniprot fasta file, from the best sequence found by Blast (the lowest e-value). load the retrieved functional information in your annotation file writing your own script or use the maker script maker_gff3manager_JD_v8.pl -f maker_with_abinitio_with_interpro.gff -b annotation_blast.annie --ID FLY -o finalOutputDir we change the [product] tag to the [description] tag so Webapollo shows the actual protein name directly in the screen instead of just hovering over it /home/student/.local/GAAS/annotation/WebApollo/gff3_webApollo_compliant.pl --gff finalOutputDir/codingGeneFeatures.gff -o final_annotation.gff Subbmitting to EBI using a tool In order to submit to EBI , the use of a tool like EMBLmyGFF3 will be your best choice. Let's prepare your annotation to submit to ENA (EBI) You need to create an account and create a project asking a locus_tag for your annotation. You have to fill lot of metada information related to the assembly and so on. We will skip those tasks using fake information. First you need to download and install EMBLmyGFF3: pip install --user git+https://github.com/NBISweden/EMBLmyGFF3.git EMBLmyGFF3 finalOutputDir/codingGeneFeatures.gff 4.fa -o my_annotation_ready_to_submit.embl You now have a EMBL flat file ready to submit.","title":"Annotation"},{"location":"sequencing/annotation/#genome-annotation","text":"","title":"Genome annotation"},{"location":"sequencing/annotation/#structural-genome-annotation","text":"Structural gene annotation - find out where the region of interest is Functional gene annotation - find out what the region do gff - genome feature file Main steps: QC assembly -> structural annotation -> manual curation -> functional annotation -> Submission or Downstream analysis QC of assembly is highly important Repeat Masking to improve the gene annotations most pipelines have this included, but check first before using a annotation program annotations are either based on proteins or transcripts if a protein is unknown then you won't annotate it RNA-seq tries to find all transcripts - should always be included in a annotation project Approaches for annotations: Similarity-based methods - these use similarity to annotated sequences like proteins, cDNAs, or ESTs Ab initio prediction - likelihood based methods that needs to be trained (give them 1000 known genes of a species) * annotations based on gene content (codon usage, GC content, exon/intron size, promotor, ORF, start codons, splice sites and more) * sensitivity and specifity has to be determined after the training * false positive results? overpredicting? * sensitivity and specifity on nucleotide level is more important than on gene level when evaluating the performance Hybrid approaches - ab initio tools with the ability to integrate external evidence/hints Comparative (homology) based gene finders - these align genomic sequences from different species and use the alignments to guide the gene predictions Chooser, combiner approaches - these combine gene predictions of other gene finders Pipelines - These combine multiple approaches * use a pipeline Popular tools : Supported by the maker tool: SNAP - Works ok, easy to train, not as good as others especially on longer intron genomes. Augustus - Works great, hard to train (but getting better) GeneMark-ES - Self training, no hints, buggy, not good for fragmented genomes or long introns (Best suited for Fungi). FGENESH - Works great, costs money even for training. GlimmerHMM (Eukaryote) GenScan Gnomon (NCBI) but you want to use those tools in a pipeline - combining various methodes since it improves highly the output quality PIPELINES : PASA Produces evidence-driven consensus gene models (-) minimalist pipeline (+) good for detecting isoforms (+) biologically relevant predictions using Ab initio tools and combined with EVM it does a pretty good job PASA + Ab initio + EVM is not automatized NCBI pipeline best one yet - but difficult to install NCBI staff can be asked an they help you Evidence + ab initio (Gnomon), repeat masking, gene naming, data formatting, miRNAs, tRNAs Ensembl Evidence based only ( comparative + homology ) MAKER2 Evidence based and/or ab initio developed as an easy-to-use alternative to other pipelines Easy to use and to configure Almost unlimited parallelism built-in (limited by data and hardware) Largely independent from the underlying system it is run on Everything is run through one command, no manual combining of data/outputs Follows common standards, produces GMOD compliant output Annotation Edit Distance (AED) metric for improved quality control Provides a mechanism to train and retrain ab initio gene predictors Annotations can be updated by re-launching Maker with new evidence Pipelines give good results, MAKER2 the most flexible, adjustable Most methods only build gene models, no functional inference Computational pipelines make mistakes Annotation requires manual curation As for assembly, an annotation is never finished, it can always be improved (e.g. Human)","title":"Structural genome annotation"},{"location":"sequencing/annotation/#genome-annotation-with-augustus-practical","text":"we are using chromosome 4 of the fruit fly, Drosophila melanogaster QC of the assembly first: Fragmentation? (N50, N90, how many short contigs) Sanity of the fasta file (Presence of Ns, presence of ambiguous nucleotides, presence of lowercase nucleotides, single line sequences vs multiline sequences) completeness (using BUSCO) presence of organelles Others (GC content, How distant the investigated species is from the others annotated species available)","title":"Genome annotation with augustus - practical"},{"location":"sequencing/annotation/#1-busco-for-assembly-check","text":"BUSCO provides quantitative measures for the assessment of genome assembly, gene set, and transcriptome completeness, based on evolutionarily-informed expectations of gene content from near-universal single-copy orthologs selected from OrthoDB v9 . first get the best dataset from the busco website wget http://busco.ezlab.org/datasets/metazoa_odb9.tar.gz tar xzvf metazoa_odb9.tar.gz launching BUSCO, to create the folder 4_dmel_busco inside is a short_summary_4_dmel_busco.txt for statistics on how many genes/regions could be found in comparision to the reference genome the better the assembly the more will be found BUSCO.py -i ~/annotation_course/data/genome/4.fa -o 4_dmel_busco -m geno -c 8 -l metazoa_odb9 fasta_statisticsAndPlot.pl is used for additional statistics Part of this github rep called GAAS fasta_statisticsAndPlot.pl -f ~/annotation_course/data/genome/4.fa","title":"1. BUSCO for Assembly check"},{"location":"sequencing/annotation/#2-augustus-an-ab-initio-gene-finder","text":"if satisfied by the quality of the assembly we start the annotation we focus on the gene finder augustus these gene finders use likelihoods to find the most likely genes in the genome they are aware of start and stop codons and splice sites, and will only try to predict genes that follow these rules the most important factor here is that the gene finder needs to be trained on the organism you are running the program on, otherwise the probabilities for introns, exons, etc. will not be correct we have training files for Drosophila Augustus * call it with a genome/assembly and it saves the annotation as a gff3 file augustus --species=fly ~/annotation_course/data/genome/4.fa --gff3=yes > augustus_drosophila.gff #to get additional isoforms use this: augustus --species=fly ~/annotation_course/data/genome/4.fa --gff3=yes --alternatives-from-sampling=true > augustus_drosophila_isoform.gff gff3_sp_statistics.pl is also from GAAS for the created gff file from augustus gives an statistical overview of the gene annotations gff3_sp_statistics.pl --gff augustus_drosophila.gff","title":"2. augustus an ab initio gene finder"},{"location":"sequencing/annotation/#3-visualisation","text":"opened the chromosome 4 file and the gff file in UGENE looks like this: An other good and popular software to explore genomes is IGV","title":"3. Visualisation"},{"location":"sequencing/annotation/#maker-gene-build-pipeline-practical","text":"MAKER is a computational pipeline to automatically generate annotations from a range of input data. The Maker pipeline can work with any combination of the following data sets, which are put into the maker_opts.ctl : Proteins from the same species or related Swissport for high quality protein sequences Refseq sequence sets from the ftp-servers Ensembl using Biomart interface to download data for a specific region or a specific gene Proteins from more distantly related organisms Uniprot to include protein sequences from organisms closely related to your study organism EST sequences from the same species or very closely related species NCBI or EBI websites to retrieve such kind of data RNA-seq data from the same or very closely related species, in the form of splice sites or assembled transcripts normally genereted by yourself or are retrived on the Sequence Read Archive of NCBI (SRA) or the ENA of EBI ab initio predictions from one or more tools (supported are: Augustus, Snap, GeneMark, Fgenesh)","title":"maker gene build pipeline - practical"},{"location":"sequencing/annotation/#1-start-maker","text":"do maker -CTL to create the 3 files for the pipeline ( maker_opts.ctl, maker_bopts.ctl, maker_exe.ctl )","title":"1. Start maker"},{"location":"sequencing/annotation/#21-evidence-based-annotation","text":"the best protein- and EST-alignments are chosen to build the most likely gene model, without an ab initio model We need to prepare various files and add their path to the maker_opts.ctl file (use , to seperate files): name of the genome sequence ( genome= ) name of the 'EST' set file(s) ( est= ) name of the 'Protein' set file(s) ( protein= ) name of the repeatmasker and repeatrunner files ( rm_gff= ) disabling ab initio with protein2genome=1 , est2genome=1 we deactivated the parameters model_org= and repeat_protein= to avoid the heavy work of repeatmasker (blank values) if the maker_opts.ctl is configured properly start the maker pipeline with mpiexec -n 8 maker # -n is core numbers # and compile the output with: maker_merge_outputs_from_datastore.pl --output maker_no_abinitio in the folder you find the annotation file maker.gff to get statistics use the gff3_sp_statistics.pl gff3_sp_statistics.pl --gff maker_no_abinitio/annotationByType/maker.gff","title":"2.1 Evidence-based annotation"},{"location":"sequencing/annotation/#22-run-maker-with-ab-initio-predictions","text":"builds on the gff from step 2.1 and does a ab-initio prediction These files were present(repeatmasker.chr4.gff, repeatrunner.chr4.gff, 4.fa) These were created in Step 2.1 (est_gff_stringtie.gff, est2genome.gff, protein2genome.gff) so add them to the maker_opts.ctl change these now to protein2genome=0 , est2genome=0 , keep_preds=1 , augustus_species=fly With these settings, Maker will run augustus to predict gene loci, but inform these predictions with information from the protein and est alignments You can combine step 2.1 & 2.2 into one its basically step 2.1 with augustus and ab initio activated (the gffs are created during the pipeline) mpiexec -n 8 maker #compiling maker_merge_outputs_from_datastore.pl --output maker_with_abinitio #check statistics with this GAAS script gff3_sp_statistics.pl --gff maker_with_abinitio/annotationByType/maker.gff Summary : We created 2 maker.gff files. One in maker_with_abinto/ one in maker_noabinto directory/ . Doing Step 2.2 helped to make the gene predictions less fragmented. For best performance go all th way to step 2.2. For Intron / Exon manual curation look for the GT/AG in an Intron 5'-3' [EXON]++GT--intron--AG++[EXON] 3'-5' [EXON]++GA--intron--TG++[EXON]","title":"2.2 Run Maker with ab-initio predictions"},{"location":"sequencing/annotation/#functional-genome-annotation","text":"Functional gene annotation - find out what the region does You can do this experimentally (slow and expensive) or computationally Computationally: Sequence based - mainly done * based on similarity/motif/profile * orthology based on evolutionary relationship * Clustering with KOG/COG * Synteny: Satsuma + kraken + custom script * phylogeny based Structure based * global structure comparison * localized regions * active sites resides * Protein-Protein Interaction data","title":"Functional genome annotation"},{"location":"sequencing/annotation/#blast-based-functional-genome-annotation","text":"you need Sequence, gff3 files Uniprot (exhaustive) or Swissprot (reliable) Use Annie to extract best hits from blast-hit list and the corresponding description from uniprot-headers Add the information to the annotation.gff using custom-script then you have a nice description of the blast hit in the gff file Database Information Comment KEGG Pathway Kyoto Encyclopedia of Genes and Genomes MetaCyc Pathway Curated database of experimentally elucidated metabolic pathways from all domains of life (NIH) Reactome Pathway Curated and peer reviewed pathway database UniPathway Pathway Manually curated resource of enzyme-catalyzed and spontaneous chemical reactions. GO Gene Ontology Three structured, controlled vocabularies (ontologies) : biological processes, cellular components and molecular functions Pfam Protein families Multiple sequence alignments and hidden Markov models Interpro P. fam., domains & functional sites Run separate search applications, and create a signature to search against Interpro. Tool Approach Comment Trinotate Best blast hit, protein domain identification (HMMER/PFAM), protein signal peptide and transmembrane domain prediction (signalP/tmHMM), and leveraging various annotation databases (eggNOG/GO/Kegg databases). Not automated Annocript Best blast hit Collects the best-hit and related annotations (proteins, domains, GO terms, Enzymes, pathways, short) Annot8r Best blast hits A tool for Gene Ontology, KEGG biochemical pathways and Enzyme Commission EC number annotation of nucleotide and peptide sequences. Sma3s Best blast hit, Best reciprocal blast hit, clusterisation 3 annotation levels afterParty BLAST, InterProScan web application Interproscan Separate search applications, HMMs, fingerprints, patterns of InterPro Created to unite secondary databases Blast2Go get best blast hits Retrieve only GO,Commercial !","title":"blast based functional genome annotation"},{"location":"sequencing/annotation/#interproscan-approach-practical","text":"we took a gff file and the corresponding fasta file we had a pre downloaded database we used this script to extract the protein sequences with an gff annotation ( AA.fa ) gff3_sp_extract_sequences.pl --gff maker_with_abinitio.gff -f 4.fa -p --cfs -o AA.fa we used the interproscan complete thing - incl. databases (48 GB) to get the information there is a way to analyse through the web but it has a limit on requests #takes 2-3 seconds per protein interproscan.sh -i AA.fa -t p -dp -pa -appl Pfam,ProDom-2006.1,SuperFamily-1.75 --goterms --iprlookup load the retrieved functional information in your annotation file writing your own script or use the maker script ipr_update_gff maker_with_abinitio.gff AA.fa.tsv > maker_with_abinitio_with_interpro.gff","title":"Interproscan approach - practical"},{"location":"sequencing/annotation/#blast-approach-practical","text":"similar to your resistance gene blasts blastp -db ~/annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa -query AA.fa -outfmt 6 -out blast.out -num_threads 8 we are using annie to process the blast output git clone https://github.com/genomeannotation/Annie.git # This is the annie.py script Annie/annie.py -b blast.out -db ~/annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa -g maker_with_abinitio.gff -o annotation_blast.annie Annie writes in a 3-column table format file, providing gene name and mRNA product information. The purpose of annie is relatively simple. It recovers the information in the sequence header of the uniprot fasta file, from the best sequence found by Blast (the lowest e-value). load the retrieved functional information in your annotation file writing your own script or use the maker script maker_gff3manager_JD_v8.pl -f maker_with_abinitio_with_interpro.gff -b annotation_blast.annie --ID FLY -o finalOutputDir we change the [product] tag to the [description] tag so Webapollo shows the actual protein name directly in the screen instead of just hovering over it /home/student/.local/GAAS/annotation/WebApollo/gff3_webApollo_compliant.pl --gff finalOutputDir/codingGeneFeatures.gff -o final_annotation.gff","title":"BLAST approach - practical"},{"location":"sequencing/annotation/#subbmitting-to-ebi-using-a-tool","text":"In order to submit to EBI , the use of a tool like EMBLmyGFF3 will be your best choice. Let's prepare your annotation to submit to ENA (EBI) You need to create an account and create a project asking a locus_tag for your annotation. You have to fill lot of metada information related to the assembly and so on. We will skip those tasks using fake information. First you need to download and install EMBLmyGFF3: pip install --user git+https://github.com/NBISweden/EMBLmyGFF3.git EMBLmyGFF3 finalOutputDir/codingGeneFeatures.gff 4.fa -o my_annotation_ready_to_submit.embl You now have a EMBL flat file ready to submit.","title":"Subbmitting to EBI using a tool"},{"location":"sequencing/hybrid/","text":"Hybridassemblies the currently to go standard is nanopore and illumina reads combined this is easily done via unicycler github link What it does (briefly): spades Assembly with illumina reads only closing the gaps using only the nanopore Reads polishing via pilon and a few more corrections and stuff * So we only go with porechop for long reads and scythe sickle for short reads Unicycler unicycler -1 <fwd>_R1.fastq.gz -2 <rev>_R2.fastq.gz -l nanopore_reads.fastq -o outputfolder -1 and -2 are the illumina inputs -l is the long read input -o is the output Assembly visualization with Bandage Bandage is a GUI to interact with assembly graphs made by de novo assemblers such as Velvet, SPAdes, MEGAHIT and others. Figure: Connects Contigs; Repeats are region were many contigs connect; nanopore is one line","title":"Hybridassembly"},{"location":"sequencing/hybrid/#hybridassemblies","text":"the currently to go standard is nanopore and illumina reads combined this is easily done via unicycler github link What it does (briefly): spades Assembly with illumina reads only closing the gaps using only the nanopore Reads polishing via pilon and a few more corrections and stuff * So we only go with porechop for long reads and scythe sickle for short reads","title":"Hybridassemblies"},{"location":"sequencing/hybrid/#unicycler","text":"unicycler -1 <fwd>_R1.fastq.gz -2 <rev>_R2.fastq.gz -l nanopore_reads.fastq -o outputfolder -1 and -2 are the illumina inputs -l is the long read input -o is the output","title":"Unicycler"},{"location":"sequencing/hybrid/#assembly-visualization","text":"with Bandage Bandage is a GUI to interact with assembly graphs made by de novo assemblers such as Velvet, SPAdes, MEGAHIT and others. Figure: Connects Contigs; Repeats are region were many contigs connect; nanopore is one line","title":"Assembly visualization"},{"location":"sequencing/illumina/","text":"Illumina - practical example workflow with a few steps included part of the SLU bioinformatics course in general always check the --help for each program 1. Cleaning up Reads fastqc gives you an overview whats may be wrong with your reads N50 is defined as the minimum contig length needed to cover 50% of the genome. It means, half of the genome sequence is in contigs larger than or equal the N50 contig size PHRED quality score: gives the probability of an incorrect base call for each base. Score 10 = 1 error in 10 bases. Score 20 = 1 error in 100. Score 30 = 1 in 1000 and so on. It's encoded from 0 to 93 as ASCII symbols in the fastq format. scythe removes adapters and or barcodes, or remove the \"overrepresented sequence\" that fastqc is reporting * Download Illumina adapter files here sickle for removal of low quality sequences; -s /dev/null moves singleton files to trash * multiqc takes all fastqc reports in this directory and creates and combined html file fastqc <reads.fastq> scythe -a <adapters.fasta> <reads.fastq> sickle pe -f <fileinputfwd> -r <fileinputrev> -o <outputfwd> -p <outpudrev> -t sanger -s /dev/null -q 25 fastqc <trimmed_reads.fastq> multiqc . 2.a megahit assembly megahit -1 fwd.fastq.gz -2 rwd.fastq.gz -o m_genitalium 2.b or spades assembly spades.py -k 21,33,55,77 --careful --only-assembler -1 <fwd read>.fastq.gz -2 <rws reads>.fastq.gz -o spade_output -m 16 Quality control with quast quast.py m_genitalium.fasta -o m_genitalium_report 3. Index creation with bowtie and bowtie / samtool pipe bowtie2-build *ASSEMBLYFILE* *m_genitalium123* bowtie2 -x *m_genitalium123* -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz | samtools view -bS -o m_genitalium.bam -x is the foldername (created with the first bowtie2 command) 4. Sort with samtools and build a index samtools sort m_genitalium.bam -o m_genitalium.sorted.bam samtools index m_genitalium.sorted.bam 5. Starting pylon for missmatch correction pilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved 6. Quality control with busco.py needs a database ( wget from busco homepage) tells you how many conserved genes could be found in the assembly (QC) wget http://busco.ezlab.org/datasets/bacteria_odb9.tar.gz BUSCO.py -i m_genitalium_improved.fasta -l bacteria_odb9 -o busco_genitalium -m genome -i input = output of pylon -l is the database from wget -m genome = which type","title":"HTS (Illumina)"},{"location":"sequencing/illumina/#illumina-practical","text":"example workflow with a few steps included part of the SLU bioinformatics course in general always check the --help for each program","title":"Illumina - practical"},{"location":"sequencing/illumina/#1-cleaning-up-reads","text":"fastqc gives you an overview whats may be wrong with your reads N50 is defined as the minimum contig length needed to cover 50% of the genome. It means, half of the genome sequence is in contigs larger than or equal the N50 contig size PHRED quality score: gives the probability of an incorrect base call for each base. Score 10 = 1 error in 10 bases. Score 20 = 1 error in 100. Score 30 = 1 in 1000 and so on. It's encoded from 0 to 93 as ASCII symbols in the fastq format. scythe removes adapters and or barcodes, or remove the \"overrepresented sequence\" that fastqc is reporting * Download Illumina adapter files here sickle for removal of low quality sequences; -s /dev/null moves singleton files to trash * multiqc takes all fastqc reports in this directory and creates and combined html file fastqc <reads.fastq> scythe -a <adapters.fasta> <reads.fastq> sickle pe -f <fileinputfwd> -r <fileinputrev> -o <outputfwd> -p <outpudrev> -t sanger -s /dev/null -q 25 fastqc <trimmed_reads.fastq> multiqc .","title":"1. Cleaning up Reads"},{"location":"sequencing/illumina/#2a-megahit-assembly","text":"megahit -1 fwd.fastq.gz -2 rwd.fastq.gz -o m_genitalium","title":"2.a megahit assembly"},{"location":"sequencing/illumina/#2b-or-spades-assembly","text":"spades.py -k 21,33,55,77 --careful --only-assembler -1 <fwd read>.fastq.gz -2 <rws reads>.fastq.gz -o spade_output -m 16","title":"2.b or spades assembly"},{"location":"sequencing/illumina/#quality-control","text":"with quast quast.py m_genitalium.fasta -o m_genitalium_report","title":"Quality control"},{"location":"sequencing/illumina/#3-index-creation","text":"with bowtie and bowtie / samtool pipe bowtie2-build *ASSEMBLYFILE* *m_genitalium123* bowtie2 -x *m_genitalium123* -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz | samtools view -bS -o m_genitalium.bam -x is the foldername (created with the first bowtie2 command)","title":"3. Index creation"},{"location":"sequencing/illumina/#4-sort-with-samtools-and-build-a-index","text":"samtools sort m_genitalium.bam -o m_genitalium.sorted.bam samtools index m_genitalium.sorted.bam","title":"4. Sort with samtools and build a index"},{"location":"sequencing/illumina/#5-starting-pylon-for-missmatch-correction","text":"pilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved","title":"5. Starting pylon for missmatch correction"},{"location":"sequencing/illumina/#6-quality-control","text":"with busco.py needs a database ( wget from busco homepage) tells you how many conserved genes could be found in the assembly (QC) wget http://busco.ezlab.org/datasets/bacteria_odb9.tar.gz BUSCO.py -i m_genitalium_improved.fasta -l bacteria_odb9 -o busco_genitalium -m genome -i input = output of pylon -l is the database from wget -m genome = which type","title":"6. Quality control"},{"location":"sequencing/nanopore/","text":"Nanopore - practical de.NBI nanopore training course a few example workflows in general always check the --help for each program 0. Base calling using Albacore Comparison of Basecaller here # part of my script so variables are assigned to the options read_fast5_basecaller.py -i $currDir/$i -f $flowcell -t $CPU -q 100000 -o fastq -k $kittype -r -s $currDir/FASTQ/ 1. QC of reads Using the assembly-stats to get a summary of the reads: assembly-stats ERR1147227.fastq 2. Adapter removal & demultiplexing with porechop porechop needs seperate outputs depending if barcodes were present or not porechop -i <input>.fastq -b <output_foulder> You can redo assembly-stats to validate 3. Assembly with minimap2 and miniasm use canu for de novo assemblies minimap2 creates a files that miniasm needs for assembly minimap2 creates a first draft map. However its not a good minion only assembler , we use it here to combine it with illumina data minimap2 -x ava-ont ERR1147227_trimmed.fastq ERR1147227_trimmed.fastq | gzip -1 > ERR1147227.paf.gz miniasm -f ERR1147227_trimmed.fastq ERR1147227.paf.gz > ERR1147227.gfa canu example canu -d run_e.coli -p e.coli genomeSize=6m -nanopore-raw 7718_E.coli_sum.fastq 4. Polishing We do: index build, mapping reads to assembly, piping to samtools for bam conversion, sorting, indexing -p4 for 4 threads (CPU) as bowtie2 takes time bowtie2-build ERR1147227.fasta ERR1147227 bowtie2 -p4 -x ERR1147227 -1 ecoli_hiseq_R1.fastq.gz -2 ecoli_hiseq_R2.fastq.gz | samtools view -bS -o ERR1147227.bam samtools sort ERR1147227.bam -o ERR1147227.sorted.bam samtools index ERR1147227.sorted.bam Now we can do the actual polishing with pilon pilon --genome ERR1147227.fasta --frags ERR1147227.sorted.bam --output ERR1147227_improved","title":"Longreads (Nanopore)"},{"location":"sequencing/nanopore/#nanopore-practical","text":"de.NBI nanopore training course a few example workflows in general always check the --help for each program","title":"Nanopore - practical"},{"location":"sequencing/nanopore/#0-base-calling","text":"using Albacore Comparison of Basecaller here # part of my script so variables are assigned to the options read_fast5_basecaller.py -i $currDir/$i -f $flowcell -t $CPU -q 100000 -o fastq -k $kittype -r -s $currDir/FASTQ/","title":"0. Base calling"},{"location":"sequencing/nanopore/#1-qc-of-reads","text":"Using the assembly-stats to get a summary of the reads: assembly-stats ERR1147227.fastq","title":"1. QC of reads"},{"location":"sequencing/nanopore/#2-adapter-removal-demultiplexing","text":"with porechop porechop needs seperate outputs depending if barcodes were present or not porechop -i <input>.fastq -b <output_foulder> You can redo assembly-stats to validate","title":"2. Adapter removal &amp; demultiplexing"},{"location":"sequencing/nanopore/#3-assembly","text":"with minimap2 and miniasm use canu for de novo assemblies minimap2 creates a files that miniasm needs for assembly minimap2 creates a first draft map. However its not a good minion only assembler , we use it here to combine it with illumina data minimap2 -x ava-ont ERR1147227_trimmed.fastq ERR1147227_trimmed.fastq | gzip -1 > ERR1147227.paf.gz miniasm -f ERR1147227_trimmed.fastq ERR1147227.paf.gz > ERR1147227.gfa canu example canu -d run_e.coli -p e.coli genomeSize=6m -nanopore-raw 7718_E.coli_sum.fastq","title":"3. Assembly"},{"location":"sequencing/nanopore/#4-polishing","text":"We do: index build, mapping reads to assembly, piping to samtools for bam conversion, sorting, indexing -p4 for 4 threads (CPU) as bowtie2 takes time bowtie2-build ERR1147227.fasta ERR1147227 bowtie2 -p4 -x ERR1147227 -1 ecoli_hiseq_R1.fastq.gz -2 ecoli_hiseq_R2.fastq.gz | samtools view -bS -o ERR1147227.bam samtools sort ERR1147227.bam -o ERR1147227.sorted.bam samtools index ERR1147227.sorted.bam Now we can do the actual polishing with pilon pilon --genome ERR1147227.fasta --frags ERR1147227.sorted.bam --output ERR1147227_improved","title":"4. Polishing"},{"location":"sequencing/programs/","text":"Important Software here i include a few basic programs that are Important this list is not complete, because it changes usually quite fast SRA-toolkit link to sra website wget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz tar xf sratoolkit.current-ubuntu64.tar.gz cd sratoolkit.2.8.2-1-ubuntu64/bin mv * ~/.local/bin Assembly-stats for installation see github porechop Adapter removal and demultiplexing of nanopore data for installation see github minimap2 & miniasm fast but error-prone long read assembler Use this only with pilon and Illumina correction github link minimap2 github link miniasm # Install minimap and miniasm (requiring gcc and zlib) git clone https://github.com/lh3/minimap && (cd minimap && make) git clone https://github.com/lh3/miniasm && (cd miniasm && make) MUMmer github link wget https://github.com/mummer4/mummer/releases/download/v4.0.0beta2/mummer-4.0.0beta2.tar.gz tar xf mummer-4.0.0beta2.tar.gz cd mummer-4.0.0beta2/ ./configure LDFLAGS=-static make sudo make install Unicycler Hybrid assembler pipeline github link","title":"Programs"},{"location":"sequencing/programs/#important-software","text":"here i include a few basic programs that are Important this list is not complete, because it changes usually quite fast","title":"Important Software"},{"location":"sequencing/programs/#sra-toolkit","text":"link to sra website wget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz tar xf sratoolkit.current-ubuntu64.tar.gz cd sratoolkit.2.8.2-1-ubuntu64/bin mv * ~/.local/bin","title":"SRA-toolkit"},{"location":"sequencing/programs/#assembly-stats","text":"for installation see github","title":"Assembly-stats"},{"location":"sequencing/programs/#porechop","text":"Adapter removal and demultiplexing of nanopore data for installation see github","title":"porechop"},{"location":"sequencing/programs/#minimap2-miniasm","text":"fast but error-prone long read assembler Use this only with pilon and Illumina correction github link minimap2 github link miniasm # Install minimap and miniasm (requiring gcc and zlib) git clone https://github.com/lh3/minimap && (cd minimap && make) git clone https://github.com/lh3/miniasm && (cd miniasm && make)","title":"minimap2 &amp; miniasm"},{"location":"sequencing/programs/#mummer","text":"github link wget https://github.com/mummer4/mummer/releases/download/v4.0.0beta2/mummer-4.0.0beta2.tar.gz tar xf mummer-4.0.0beta2.tar.gz cd mummer-4.0.0beta2/ ./configure LDFLAGS=-static make sudo make install","title":"MUMmer"},{"location":"sequencing/programs/#unicycler","text":"Hybrid assembler pipeline github link","title":"Unicycler"},{"location":"shell/fasta/","text":"Manipulating files via bash 1. Awk & sed fastq/a manipulation 1.1 Convert .fastq to .fasta using awk, sed for file manipulation also includes creating fasta oneliners # converting fastq to fasta sed -n '1~4s/^@/>/p;2~4p' INFILE.fastq > OUTFILE.fasta 1.2 Converting .fasta to one liner One line is fasta header, one line is sequence it removes the \"sequence wraps\" perfect to extract sequences, e. g. grep \"blaCMY\" -A1 sequencelist.fasta # make fasta files to one liner sed ':a;N;/^>/M!s/\\n//;ta;P;D' Input.fasta > oneliner.fasta 1.3 Remove sequences by length # filter multi fasta by a seq length - in this case 1000 bp awk '/^>/ { getline seq } length(seq) >1000 { print $0 \"\\n\" seq }' oneliner.fasta > online_grt1000.fasta Summary as loops: # lazy way for x in *.fastq; do sed -n '1~4s/^@/>/p;2~4p' $x > ${x%.fastq}.fasta ; done for x in *.fasta; do sed ':a;N;/^>/M!s/\\n//;ta;P;D' $x > ${x%.fasta}_oneliner.fasta ; done for x in *_oneliner.fasta; do awk '/^>/ { getline seq } length(seq) >1000 { print $0 \"\\n\" seq }' $x > ${x%_onliner.fasta}_clean.fasta ; done # check all reads for x in *.fasta ; do echo grep -c \">\" $x 2. Convert .gfa to .fasta extracts the sequences out of a gfa file awk '/^S/{print \">\"$2\"\\n\"$3}' file_in.gfa | fold > file_out.fasta","title":"File Manipulation (fasta)"},{"location":"shell/fasta/#manipulating-files-via-bash","text":"","title":"Manipulating files via bash"},{"location":"shell/fasta/#1-awk-sed-fastqa-manipulation","text":"","title":"1. Awk &amp; sed fastq/a manipulation"},{"location":"shell/fasta/#11-convert-fastq-to-fasta","text":"using awk, sed for file manipulation also includes creating fasta oneliners # converting fastq to fasta sed -n '1~4s/^@/>/p;2~4p' INFILE.fastq > OUTFILE.fasta","title":"1.1 Convert .fastq to .fasta"},{"location":"shell/fasta/#12-converting-fasta-to-one-liner","text":"One line is fasta header, one line is sequence it removes the \"sequence wraps\" perfect to extract sequences, e. g. grep \"blaCMY\" -A1 sequencelist.fasta # make fasta files to one liner sed ':a;N;/^>/M!s/\\n//;ta;P;D' Input.fasta > oneliner.fasta","title":"1.2 Converting .fasta to one liner"},{"location":"shell/fasta/#13-remove-sequences-by-length","text":"# filter multi fasta by a seq length - in this case 1000 bp awk '/^>/ { getline seq } length(seq) >1000 { print $0 \"\\n\" seq }' oneliner.fasta > online_grt1000.fasta","title":"1.3 Remove sequences by length"},{"location":"shell/fasta/#summary","text":"as loops: # lazy way for x in *.fastq; do sed -n '1~4s/^@/>/p;2~4p' $x > ${x%.fastq}.fasta ; done for x in *.fasta; do sed ':a;N;/^>/M!s/\\n//;ta;P;D' $x > ${x%.fasta}_oneliner.fasta ; done for x in *_oneliner.fasta; do awk '/^>/ { getline seq } length(seq) >1000 { print $0 \"\\n\" seq }' $x > ${x%_onliner.fasta}_clean.fasta ; done # check all reads for x in *.fasta ; do echo grep -c \">\" $x","title":"Summary"},{"location":"shell/fasta/#2-convert-gfa-to-fasta","text":"extracts the sequences out of a gfa file awk '/^S/{print \">\"$2\"\\n\"$3}' file_in.gfa | fold > file_out.fasta","title":"2. Convert .gfa to .fasta"},{"location":"shell/git/","text":"Git via github.com 1. Github.com create a repository at github copy the link for cloning a repository locally type git clone <git_URL> get updates via git pull inside the git repository 1.1 Local Repository step by step use git init to create a git directory in any directory you want this is the master directory, so every sub directory is included at github , follow the description to link it to your local folder gitstatus always tells you what to do you add things you want to commit using git add <file> git commit -m \"message\" to commit your changes git push origin master pushes commited file to internet git repository git pull to get a update from github nano .gitignore write filenames in .gitignore that you don't want to be added or pushed 1.2 Installing Gits Download and compile new programs or git clones into ~/install/ then put the path to ~/.bashrc : b # do nano ~/.bashrc and add export PATH=\"/home/replik/install/canu/Linux-amd64/bin:$PATH\" Search the programs in github the make or configure steps are written there or in the README.md . Do a make clean if you get errors during make before doing a new make 1.3 Git sides this is basically a website hosted via github all files are maintained via the master can be easily created with mkdocs see this side Cheatsheet here is one, click me","title":"Git"},{"location":"shell/git/#git-via-githubcom","text":"","title":"Git via github.com"},{"location":"shell/git/#1-githubcom","text":"create a repository at github copy the link for cloning a repository locally type git clone <git_URL> get updates via git pull inside the git repository","title":"1. Github.com"},{"location":"shell/git/#11-local-repository","text":"step by step use git init to create a git directory in any directory you want this is the master directory, so every sub directory is included at github , follow the description to link it to your local folder gitstatus always tells you what to do you add things you want to commit using git add <file> git commit -m \"message\" to commit your changes git push origin master pushes commited file to internet git repository git pull to get a update from github nano .gitignore write filenames in .gitignore that you don't want to be added or pushed","title":"1.1 Local Repository"},{"location":"shell/git/#12-installing-gits","text":"Download and compile new programs or git clones into ~/install/ then put the path to ~/.bashrc : b # do nano ~/.bashrc and add export PATH=\"/home/replik/install/canu/Linux-amd64/bin:$PATH\" Search the programs in github the make or configure steps are written there or in the README.md . Do a make clean if you get errors during make before doing a new make","title":"1.2 Installing Gits"},{"location":"shell/git/#13-git-sides","text":"this is basically a website hosted via github all files are maintained via the master can be easily created with mkdocs see this side","title":"1.3 Git sides"},{"location":"shell/git/#cheatsheet","text":"here is one, click me","title":"Cheatsheet"},{"location":"shell/shell/","text":"Shell Commands Terminal shortcuts ctrl + C terminates a process ctrl + D logout from a ssh remote ctrl + Z pauses a process 1. Basic Commands cut -d -f3 # -f 3,4 also possible, delimiter tab is default for -d without input sort head -10 less uniq -c # a uniq that also counts using -c wc -l # wordcount or -l lines tail -n +2 # starts at line 2, reverse \"head\" seq 1 10 # sequence, prints 1 2 3 4 5 (...) 10 grep -c \">\" # -c counts the \">\" in all the files you give grep grep -w \"ls\" # exact match ls -lh # human readable ls command ln -s ~/data/* . # creates a link for all data in /data to current folder . zless # makes a \"less\" on an gz file tar -xvf mkdir -p # creates only folder if it doesnt exist chmod u=rw # [u g o a] [-+=] [rwx] user group other all(=user&group&other) htop # CPU and RAM information echo $PATH # shows all bin/ directories df -h # free disk space 1.1 Variables you can manipulate the output of variables directly write variables like this ${x} to make sure it uses the right variable name, if there comes no white space after e.g. varible x can't be found in $x_test since it adds the _test to the name, but ${x}_test would work # example for variable x echo \"$x\" # Forward_sequences.fasta # Remove the something from the end: echo \"${x%.fasta}\" # Forward_sequences # Remove something in front echo \"${x#Forward_}\" # sequences.fasta 1.2 Pipes # examples cut -d, -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c cut -d, -f4 data/survey_data.csv | tail -n +2 | sort | uniq -c | sort -n 1.3 Loops # example 1 for x in $(seq 1977 2002) do grep $x results/taxa_per_year.txt > results/years/$x-count.txt done # example 2 for x in *.fasta; do mkdir ${x%.fasta} #${x%.fasta} removes \".fasta\" from the variable $x mv $x ${x%.fasta} # so the folder would not be named Seq1.fastq/ done # example 3: Sickle loop with \"basename\" to extract part of filenames for i in *_1.fastq do prefix=$(basename $i _1.fastq) echo \"${prefix} is correcting with sickle\" sickle pe -f ${prefix}_1.fastq -r ${prefix}_2.fastq -o ${prefix}_1_corr.fastq -p ${prefix}_2_corr.fastq -t sanger -s /dev/null -q 2 done 2. Installing (apt) & Downloads apt search \"TERM\" # searches apt database apt show \"ncbi-blast+\" # described the package sudo apt install ncbi-blast+ # commands for download wget <URL> # sometimes better then wget, can handle redirections curl -O -J -L <URL> # to check the md5 sum of the file md5sum <file>","title":"Commands"},{"location":"shell/shell/#shell-commands","text":"Terminal shortcuts ctrl + C terminates a process ctrl + D logout from a ssh remote ctrl + Z pauses a process","title":"Shell Commands"},{"location":"shell/shell/#1-basic-commands","text":"cut -d -f3 # -f 3,4 also possible, delimiter tab is default for -d without input sort head -10 less uniq -c # a uniq that also counts using -c wc -l # wordcount or -l lines tail -n +2 # starts at line 2, reverse \"head\" seq 1 10 # sequence, prints 1 2 3 4 5 (...) 10 grep -c \">\" # -c counts the \">\" in all the files you give grep grep -w \"ls\" # exact match ls -lh # human readable ls command ln -s ~/data/* . # creates a link for all data in /data to current folder . zless # makes a \"less\" on an gz file tar -xvf mkdir -p # creates only folder if it doesnt exist chmod u=rw # [u g o a] [-+=] [rwx] user group other all(=user&group&other) htop # CPU and RAM information echo $PATH # shows all bin/ directories df -h # free disk space","title":"1. Basic Commands"},{"location":"shell/shell/#11-variables","text":"you can manipulate the output of variables directly write variables like this ${x} to make sure it uses the right variable name, if there comes no white space after e.g. varible x can't be found in $x_test since it adds the _test to the name, but ${x}_test would work # example for variable x echo \"$x\" # Forward_sequences.fasta # Remove the something from the end: echo \"${x%.fasta}\" # Forward_sequences # Remove something in front echo \"${x#Forward_}\" # sequences.fasta","title":"1.1 Variables"},{"location":"shell/shell/#12-pipes","text":"# examples cut -d, -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c cut -d, -f4 data/survey_data.csv | tail -n +2 | sort | uniq -c | sort -n","title":"1.2 Pipes"},{"location":"shell/shell/#13-loops","text":"# example 1 for x in $(seq 1977 2002) do grep $x results/taxa_per_year.txt > results/years/$x-count.txt done # example 2 for x in *.fasta; do mkdir ${x%.fasta} #${x%.fasta} removes \".fasta\" from the variable $x mv $x ${x%.fasta} # so the folder would not be named Seq1.fastq/ done # example 3: Sickle loop with \"basename\" to extract part of filenames for i in *_1.fastq do prefix=$(basename $i _1.fastq) echo \"${prefix} is correcting with sickle\" sickle pe -f ${prefix}_1.fastq -r ${prefix}_2.fastq -o ${prefix}_1_corr.fastq -p ${prefix}_2_corr.fastq -t sanger -s /dev/null -q 2 done","title":"1.3 Loops"},{"location":"shell/shell/#2-installing-apt-downloads","text":"apt search \"TERM\" # searches apt database apt show \"ncbi-blast+\" # described the package sudo apt install ncbi-blast+ # commands for download wget <URL> # sometimes better then wget, can handle redirections curl -O -J -L <URL> # to check the md5 sum of the file md5sum <file>","title":"2. Installing (apt) &amp; Downloads"},{"location":"shell/shell_adv/","text":"Advanced shell 1. Remote/cloud computing 1.1 ssh & keys tutorials for key generation and stuff here ssh , -i ~/.ssh/key (key location) and username@IP, use the -K flag to allow fastqc graphical interface # examples ssh -i ~/.ssh/replikation.pem ubuntu@ec2-11-218-160-32.us-east-2.compute.amazonaws.com ssh -i ~/.ssh/azure_rsa student@13.45.57.169 1.2 file transfer normally you use the scp command however use rsync for huge amount of files # examples rsync -avp -e \"ssh -i ~/.ssh/cloudgoogle\" * replik@35.231.111.99:~/fast5_files rsync -avpr -e \"ssh -i ~/.ssh/cloudgoogle\" <USER>@<IP>:~/auto_assembly/ . # example for synology with different rsync path # Username@IP is stored in $IP rsync --rsync-path=/bin/rsync -vr -e \"ssh -i ~/.ssh/id_rsa\" --remove-source-files --include \"*.fast5\" --include \"*/\" --exclude \"*\" /cygdrive/c/data/reads/ $IP:/volume1/sequencing_data/ example for an ongoing rsync to transfer fast5 files limits the speed (so sequencer can use still most of the SSD write speed) while true; do rsync -vr --ignore-existing --bwlimit=3M --include \"*.fast5\" --include \"*/\" --exclude \"*\" /mnt/e/data/reads/20181205_1008_02WW /mnt/g/02.Schweden_WW sleep 5 ; done 2. Screen virtual shell session in the cloud you want to do this if you work remotely than it wont matter if the terminal window is closed sinces its a virtual process running on the host Start a virtual shell in a cloud computer: screen -R <name> #name of the virtual machine # works with tab completion after you started a virtual screen screen -R <tab completion> A new empty command screen appears (this is the virtual screen) start the command in it and close the window to see all active screens type screen -list to get back to your screen type screen -R <name> or the number of screen -list (works better) so screen -R 37509 3. Advanced Scripting for deploying Software use fabric for software deploying scripts take a look at Hadriens github for examples makes sense to just create a simple sudo apt install script for all the important libraries 4. mounting devices get the name of the disk to mount via df # mount <name of disk> to <target> sudo mount /dev/nvme1n1p1 /mnt/ 5. Big file splitting pack files via tar split them into \"chunks\" transfer chunks via internet (e.g. scp or rsync ) put them together via cat and extract via tar example: # tar a folder (-v for terminal status) tar -cf archive.tar sequencing_read_folder/ # alternatively with compression tar -czf archive.tar.gz sequencing_read_folder/ # split it into 10 GB chunks, alternatively -b 500M (500MB) split -b 10G archive.tar \"archive.tar.part\" # joining them together cat archive.tar.part* > archive.tar # extract it tar -xf archive.tar sequencing_read_folder/ a combined tar zip and splitting tar -cvzf targetdir/ | split --bytes=10GB - archivname.tar.gz.","title":"Cloud Computing"},{"location":"shell/shell_adv/#advanced-shell","text":"","title":"Advanced shell"},{"location":"shell/shell_adv/#1-remotecloud-computing","text":"","title":"1. Remote/cloud computing"},{"location":"shell/shell_adv/#11-ssh-keys","text":"tutorials for key generation and stuff here ssh , -i ~/.ssh/key (key location) and username@IP, use the -K flag to allow fastqc graphical interface # examples ssh -i ~/.ssh/replikation.pem ubuntu@ec2-11-218-160-32.us-east-2.compute.amazonaws.com ssh -i ~/.ssh/azure_rsa student@13.45.57.169","title":"1.1 ssh &amp; keys"},{"location":"shell/shell_adv/#12-file-transfer","text":"normally you use the scp command however use rsync for huge amount of files # examples rsync -avp -e \"ssh -i ~/.ssh/cloudgoogle\" * replik@35.231.111.99:~/fast5_files rsync -avpr -e \"ssh -i ~/.ssh/cloudgoogle\" <USER>@<IP>:~/auto_assembly/ . # example for synology with different rsync path # Username@IP is stored in $IP rsync --rsync-path=/bin/rsync -vr -e \"ssh -i ~/.ssh/id_rsa\" --remove-source-files --include \"*.fast5\" --include \"*/\" --exclude \"*\" /cygdrive/c/data/reads/ $IP:/volume1/sequencing_data/ example for an ongoing rsync to transfer fast5 files limits the speed (so sequencer can use still most of the SSD write speed) while true; do rsync -vr --ignore-existing --bwlimit=3M --include \"*.fast5\" --include \"*/\" --exclude \"*\" /mnt/e/data/reads/20181205_1008_02WW /mnt/g/02.Schweden_WW sleep 5 ; done","title":"1.2 file transfer"},{"location":"shell/shell_adv/#2-screen","text":"virtual shell session in the cloud you want to do this if you work remotely than it wont matter if the terminal window is closed sinces its a virtual process running on the host Start a virtual shell in a cloud computer: screen -R <name> #name of the virtual machine # works with tab completion after you started a virtual screen screen -R <tab completion> A new empty command screen appears (this is the virtual screen) start the command in it and close the window to see all active screens type screen -list to get back to your screen type screen -R <name> or the number of screen -list (works better) so screen -R 37509","title":"2. Screen"},{"location":"shell/shell_adv/#3-advanced-scripting-for-deploying-software","text":"use fabric for software deploying scripts take a look at Hadriens github for examples makes sense to just create a simple sudo apt install script for all the important libraries","title":"3. Advanced Scripting for deploying Software"},{"location":"shell/shell_adv/#4-mounting-devices","text":"get the name of the disk to mount via df # mount <name of disk> to <target> sudo mount /dev/nvme1n1p1 /mnt/","title":"4. mounting devices"},{"location":"shell/shell_adv/#5-big-file-splitting","text":"pack files via tar split them into \"chunks\" transfer chunks via internet (e.g. scp or rsync ) put them together via cat and extract via tar example: # tar a folder (-v for terminal status) tar -cf archive.tar sequencing_read_folder/ # alternatively with compression tar -czf archive.tar.gz sequencing_read_folder/ # split it into 10 GB chunks, alternatively -b 500M (500MB) split -b 10G archive.tar \"archive.tar.part\" # joining them together cat archive.tar.part* > archive.tar # extract it tar -xf archive.tar sequencing_read_folder/ a combined tar zip and splitting tar -cvzf targetdir/ | split --bytes=10GB - archivname.tar.gz.","title":"5. Big file splitting"},{"location":"tools/UGENE/","text":"UGENE UGENE is free open-source cross-platform bioinformatics software It works perfectly on Windows, Mac OS and Linux Install commands for Linux: sudo add-apt-repository ppa:iefremov/ppa sudo apt-get update sudo apt-get install ugene sudo apt-get install ugene-non-free Download all dependencies for linux ready to go from here Online Tutorial can be found here Multiple sequence alignment Basic sequence operations part 1 Working with multiple sequence alignments Creating annotations Annotation qualifiers Querying remote database 3D Structure viewer , multiple structure view and image export Local sequence alignment with Smith-Waterman algorithm Restriction enzymes Large alignments Open Reading Frames Chromatograms viewer and editor Finding DNA repeats Searching for homologs of protein sequence with HMMMER3 Finding DNA tandem repeats Compare sequences with dotplots Alignment Editor Phylogenetic trees algorithms Transcription binding sites Profile-to-profile and profile-to-sequence MUSCLE alignments Workflow Designer","title":"UGENE"},{"location":"tools/UGENE/#ugene","text":"UGENE is free open-source cross-platform bioinformatics software It works perfectly on Windows, Mac OS and Linux Install commands for Linux: sudo add-apt-repository ppa:iefremov/ppa sudo apt-get update sudo apt-get install ugene sudo apt-get install ugene-non-free Download all dependencies for linux ready to go from here Online Tutorial can be found here Multiple sequence alignment Basic sequence operations part 1 Working with multiple sequence alignments Creating annotations Annotation qualifiers Querying remote database 3D Structure viewer , multiple structure view and image export Local sequence alignment with Smith-Waterman algorithm Restriction enzymes Large alignments Open Reading Frames Chromatograms viewer and editor Finding DNA repeats Searching for homologs of protein sequence with HMMMER3 Finding DNA tandem repeats Compare sequences with dotplots Alignment Editor Phylogenetic trees algorithms Transcription binding sites Profile-to-profile and profile-to-sequence MUSCLE alignments Workflow Designer","title":"UGENE"},{"location":"tools/atom/","text":"Atom steps here are described using win10 and a WSL with ubuntu Important Packages markdown-pdf (better protocol prints) minimap (shows a script minimap on the right side) linter (corrects scripts) and linter-shellcheck need also shellcheck in linux or powershell and linter-python needs pylama installed in python 3 and linter-lintr don't install the auto-dependency on win10, go with \"atom-language-r\" add \"C:\\Program Files\\R\\R-3.5.1\\bin\\R.exe\" to path of this package install via R install.packages(\"lintr\") remote-atom (edit a remotely located script on your client via atom) you need to start the atom server in atom Packages -> Remote Atom -> Start Server you need to install rmate on the host access host via ssh -R 52698:localhost:52698 name@IP start a remote script like rmate skript.sh you can rename rmate to ratom also for ratom skript.sh Installing shellcheck you need the linter package and the linter-shellcheck package in atom then you install shellcheck via scoop into windows directly via powershell check the scoop site for scoop installation # In windows power shell type this: Set-ExecutionPolicy RemoteSigned -scope CurrentUser iex (new-object net.webclient).downloadstring('https://get.scoop.sh')` scoop install shellcheck in atom go to the shellcheck package and enter the following path to it C:\\Users\\<USERNAME>\\scoop\\apps\\shellcheck\\0.5.0\\shellcheck.exe Installing pylama (corrects python scripts) you need to install the linter and linter-python package in atom also you need to install pylama in python3 # installing the pip3 from python3.X (not pip from python 2.7) sudo apt-get install python3-pip ## use the apt-get for now pip3 install pylama pylama-pylint ## dont use sudo here add this path to the settings of linter-python: C:\\Users\\<USERNAME>\\Documents\\Atom_linter_shortcuts\\python.bat in python.bat write the following line wsl python3 -m pylama","title":"Atom Editor"},{"location":"tools/atom/#atom","text":"steps here are described using win10 and a WSL with ubuntu","title":"Atom"},{"location":"tools/atom/#important-packages","text":"markdown-pdf (better protocol prints) minimap (shows a script minimap on the right side) linter (corrects scripts) and linter-shellcheck need also shellcheck in linux or powershell and linter-python needs pylama installed in python 3 and linter-lintr don't install the auto-dependency on win10, go with \"atom-language-r\" add \"C:\\Program Files\\R\\R-3.5.1\\bin\\R.exe\" to path of this package install via R install.packages(\"lintr\") remote-atom (edit a remotely located script on your client via atom) you need to start the atom server in atom Packages -> Remote Atom -> Start Server you need to install rmate on the host access host via ssh -R 52698:localhost:52698 name@IP start a remote script like rmate skript.sh you can rename rmate to ratom also for ratom skript.sh","title":"Important Packages"},{"location":"tools/atom/#installing-shellcheck","text":"you need the linter package and the linter-shellcheck package in atom then you install shellcheck via scoop into windows directly via powershell check the scoop site for scoop installation # In windows power shell type this: Set-ExecutionPolicy RemoteSigned -scope CurrentUser iex (new-object net.webclient).downloadstring('https://get.scoop.sh')` scoop install shellcheck in atom go to the shellcheck package and enter the following path to it C:\\Users\\<USERNAME>\\scoop\\apps\\shellcheck\\0.5.0\\shellcheck.exe","title":"Installing shellcheck"},{"location":"tools/atom/#installing-pylama-corrects-python-scripts","text":"you need to install the linter and linter-python package in atom also you need to install pylama in python3 # installing the pip3 from python3.X (not pip from python 2.7) sudo apt-get install python3-pip ## use the apt-get for now pip3 install pylama pylama-pylint ## dont use sudo here add this path to the settings of linter-python: C:\\Users\\<USERNAME>\\Documents\\Atom_linter_shortcuts\\python.bat in python.bat write the following line wsl python3 -m pylama","title":"Installing pylama (corrects python scripts)"},{"location":"tools/emboss/","text":"EMBOSS Open Software Suite wossname is your first to go command to search for all the tools wossname \"sequence alignment\" total of around 100 tools, including data retrivel from web ( fastq-dump ) install EMBOSS: sudo apt-get install emboss You may need libx11-dev module (visualisation of graphs) sudo apt-get install libx11-dev Web based Version Tools included (selection): prophet gapped alignment water smith waterman local alignment infoseq get information of a sequence showfeat get features of a seq, coils, bindingsites etc. eprimer3 or primer3, primers and hybridization oligos, best one water is highly accurate alignment but very slow seqret fetches sequences from databases getorf finds open reading frames transseq translate nucleotides seq to proteins dottup dot plot of pairwise alignment good image for publication needle global alignment prettyplot multiple seq. alignment and graphics equicktandem microsatellites, palindroms, repeats wEMBOSS you need a EMBOSS server (own or a other available somewhere (like a workstation)) you access this with wEMBOSS all results can be created in html wEMBOSS needs to know if DNA or Protein sequence, then it shows associated programs GUI is user friendly but less flexible program shows you the associated \"command line","title":"EMBOSS"},{"location":"tools/emboss/#emboss-open-software-suite","text":"wossname is your first to go command to search for all the tools wossname \"sequence alignment\" total of around 100 tools, including data retrivel from web ( fastq-dump ) install EMBOSS: sudo apt-get install emboss You may need libx11-dev module (visualisation of graphs) sudo apt-get install libx11-dev Web based Version Tools included (selection): prophet gapped alignment water smith waterman local alignment infoseq get information of a sequence showfeat get features of a seq, coils, bindingsites etc. eprimer3 or primer3, primers and hybridization oligos, best one water is highly accurate alignment but very slow seqret fetches sequences from databases getorf finds open reading frames transseq translate nucleotides seq to proteins dottup dot plot of pairwise alignment good image for publication needle global alignment prettyplot multiple seq. alignment and graphics equicktandem microsatellites, palindroms, repeats","title":"EMBOSS Open Software Suite"},{"location":"tools/emboss/#wemboss","text":"you need a EMBOSS server (own or a other available somewhere (like a workstation)) you access this with wEMBOSS all results can be created in html wEMBOSS needs to know if DNA or Protein sequence, then it shows associated programs GUI is user friendly but less flexible program shows you the associated \"command line","title":"wEMBOSS"},{"location":"tools/mkdocs/","text":"mkdocs 1. Overview Use the mkdocs tutorial for site creation click me Used the site style template frome here: click me Installation: # installing mkdocs sudo apt install mkdocs # installing styles pip3 install mkdocs-bootswatch don't change anything on the git repository, it should automatically do every thing after the first mkdocs gh-deploy !! it uses the branch pushed via mkdocs gh-deploy for the website 2. Practical steps 2.1 First creation # create repository via github.vom # git clone it to the local machine and do on the repository folder: mkdocs new <repositoryname> cd <repositoryname> # change a few things in the mkdocs.yml # create markdowns und docs/ # now build the site with: mkdocs build --clean # push all changes to your master git add & git commit & git push # and so on # push the site mkdocs gh-deploy 2.2 Working with it, daily # change files # now build the site with: mkdocs build --clean # clean is important if a style change was done # push all changes to your master git add & git commit & git push .... # push the site mkdocs gh-deploy 3. Themes and Navigation you need to pip3 install themes and add their name to the theme: line mkdocs build --clean will remove old stuff and gather stuff for the new theme Example mkdocs.yml: site_name: Bioinformatics # your site name theme: yeti # enter the theme name here nav: # nav bar at the top of site - Shell: #name of drop down menu - Basics: # second drop down menu - Commands: shell/shell.md # content under Shell/Basics/ - Cloud Computing: shell/shell_adv.md # content under Shell/Basics/ - File Manipulation (fasta): shell/fasta.md # content under Shell/ - Git: shell/git.md - Python: - Basics: python.md - R: - Basics: R.md","title":"mkdocs"},{"location":"tools/mkdocs/#mkdocs","text":"","title":"mkdocs"},{"location":"tools/mkdocs/#1-overview","text":"Use the mkdocs tutorial for site creation click me Used the site style template frome here: click me Installation: # installing mkdocs sudo apt install mkdocs # installing styles pip3 install mkdocs-bootswatch don't change anything on the git repository, it should automatically do every thing after the first mkdocs gh-deploy !! it uses the branch pushed via mkdocs gh-deploy for the website","title":"1. Overview"},{"location":"tools/mkdocs/#2-practical-steps","text":"","title":"2. Practical steps"},{"location":"tools/mkdocs/#21-first-creation","text":"# create repository via github.vom # git clone it to the local machine and do on the repository folder: mkdocs new <repositoryname> cd <repositoryname> # change a few things in the mkdocs.yml # create markdowns und docs/ # now build the site with: mkdocs build --clean # push all changes to your master git add & git commit & git push # and so on # push the site mkdocs gh-deploy","title":"2.1 First creation"},{"location":"tools/mkdocs/#22-working-with-it-daily","text":"# change files # now build the site with: mkdocs build --clean # clean is important if a style change was done # push all changes to your master git add & git commit & git push .... # push the site mkdocs gh-deploy","title":"2.2 Working with it, daily"},{"location":"tools/mkdocs/#3-themes-and-navigation","text":"you need to pip3 install themes and add their name to the theme: line mkdocs build --clean will remove old stuff and gather stuff for the new theme Example mkdocs.yml: site_name: Bioinformatics # your site name theme: yeti # enter the theme name here nav: # nav bar at the top of site - Shell: #name of drop down menu - Basics: # second drop down menu - Commands: shell/shell.md # content under Shell/Basics/ - Cloud Computing: shell/shell_adv.md # content under Shell/Basics/ - File Manipulation (fasta): shell/fasta.md # content under Shell/ - Git: shell/git.md - Python: - Basics: python.md - R: - Basics: R.md","title":"3. Themes and Navigation"}]}